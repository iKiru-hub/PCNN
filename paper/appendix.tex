
\section{Appendix}

\subsection{Spatial representations}
\subsection{Grid cell module}

Unlike other approaches to generate grid fields \cite{dabaghianGridCellsBorder, schoyenHexagonsAllWay2025}, we defined a correspondence between the global environment in which the agent moves, a two-dimensional Euclidean space $\mathbf{R}^{2}$, and a bounded local space of a grid module, corresponding to the torus.

The global velocity $\mathbf{v}=\{x,y\}$ is then mapped to a local velocity, scaled by a speed scalar $s^{\text{gc}}_{l}$ specific to the grid cell module $l$, which determines its periodicity in space. This approach has been used in previous works \cite{liModelingPlaceCells2019}.
% The starting position on the torus is randomly chosen when the model is initialized since what matters is the sequence of displacements without any reference to a meaningful origin.

The choice of a toroidal space is motivated by consolidated experimental evidence of the neural space of grid cells, which are organized in modules of different sizes spanning the animal's environment. However, the shape of their firing pattern is known to be hexagonal, which corresponds to the optimal tiling of a two-dimensional plane, giving rise to a neural space lying on a twisted torus.
In this work, for simplicity, we consider a square tiling and thus a square torus, without much loss of generality except for the slight increase of grid cells required for a sufficiently cover.

A grid cell module $l$ of size $N^{\text{gc}}$ is identified by a set of positions defined over a square centered on the origin and size of $2$, such that $\{(x_{i}, y_{i})\;\vert\; i \in N^{\text{gc}}\;\land\;x_{i},y_{i}\in (-1, 1)\}$.
This local square space has boundary conditions for each dimension, such that, for instance, when $x_{t} + s^{\text{gc}}_{l} \cdot v_{x} > 2$ the position update is taken to the other side $x_{t+1} = x_{t} + s^{\text{gc}}_{l} \cdot v_{x} - 2$, where $s^{\text{gc}}_{l}$ is the scale of the velocity in the local space of the module $l$ with respect to the real global agent velocity $v=\{v_{x}, v_{y}\}$.
When the module is initialized, the starting positions of its cells are uniformly distributed over the square forming a lattice.
When the agent is reset in a new position at the beginning of new trial, a displacement vector is applied to the last cells positions such that the mapping between the module local space and the global environment is preserved.

The firing rate vector of each cell is determined with respect to a 2D Gaussian tuning curve centered at the origin at $(0,0)$, and it is calculated as \\ $r_{i} = \exp\left(-\frac{x^{2}_{i} + y^{2}_{i}}{\sigma^{\text{gc}}_{l}}\right)$, where $\sigma^{\text{gc}}_{l}$ is the width of the tuning curve for module $l$.

The final population vector of the grid cell network GC is the concatenated and flattened firing rate vector of all modules $\textbf{u}^{\text{GC}}$.

\hfill \break
In our model, each grid cell had a tuning width of 0.04. They were defined as 8 modules of size 36, and the relative speed scales were \\ $\{1., 0.8, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1, 0.07\}$.

\subsection{Place cells}

\paragraph{Tuning formation}
% formation | forward + competition
The tuning of a new place cell is simply defined as the current GC population vector $\textbf{u}^{\text{GC}}_{t}$, and its index is that of the first silent cell, which is added to the forward weight matrix $\textbf{W}_{i}^{\text{GC}to \text{PC}}\rightarrow \textbf{u}^{\text{GC}}$.

In order to avoid overlapping of place fields, the tuning process is aborted in case the cosine similarity of the new pattern and the old ones is greater than a threshold $\theta^{\text{PC}}_{\text{rep}}$.

Each cell represents a position in the GC activity space, which can be considered a node within a graph of place cells (PC).
Although it is totally possible to only use the $N^{\text{GC}}$-dimensional tuning patterns and be agnostic about the dimensionality of the space in which the agent lives, to simplify the calculations, we mapped each pattern to 2D positions in a vector space.
Then, the PC recurrent connectivity matrix is calculated with a nearest neighbors algorithm, which instead of a fixed number $K$ of neighbors uses a distance threshold $\theta^{\text{PC}}_{\text{rec}}$.

\hfill \break
\paragraph{Activity} The current firing rate of the PC population is determined by the cosine similarity between the GC input and the forward weight matrix, then passed through a generalized sigmoid $\phi(z)=\left[1 + \exp(-\beta(z-\alpha))\right]^{-1}$.
The parameter $\alpha$ represents the activation threshold, or horizontal offset, while $\beta$ the gain, or steepness.
It is also defined as an activity trace, which has an upper value of 1 and decays exponentially:

\begin{equation}
    m_{i} = - m_{i} / \tau^{\text{PC}} + u_{i}
\end{equation}

\noindent It is used as a proxy for a memory trace.

\hfill \break
In the model, a PC population is defined by its average place field size, determining the granularity of the representation of the place.

Below, a table of the parameters specific to the two PC populations is reported.

\begin{table}[h]
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        & $\beta$ & $\alpha$ & $\theta_{\text{rep}}^{\text{PC}}$ & $\theta_{\text{rec}}^{\text{PC}}$ & $\tau^{\text{PC}}$ \\
        \midrule
        pc   & 33.0  & 1.0  & 0.86  & 43  & 140 \\
        \bottomrule
    \end{tabular}
    \caption{\textsc{PC layer parameters}}
\end{table}

\subsection{Modulation}

Neuromodulation is defined in terms of a leaky variable $v$ whose state is perturbed by an external input $x$, whose qualitative meaning differs for each neuromodulator $k$.

\begin{equation}
\begin{aligned}
    v_{k} &= - v_{k} / \tau_{k} + x_{k} \\
    v_{k} &= \text{max}(v_{k}, 0)
\end{aligned}
\end{equation}

% plasticity + prediction error

\paragraph{Learning rule}
The connection weights $\textbf{W}^{k}$ are updated according to a plasticity rule composed of an Hebbian term, involving the leaky variable and the place cells above a certain threshold $\theta^{k}$, and a prediction error.

A prediction $\textbf{p}$ is calculated before each time-step and signifies the expected neuromodulation activation for a given place cell $p_{i} = \textbf{W}^{k}_{i} \hat{u}^{\text{PC}}$, where $\hat{u}^{\text{PC}}$ is the PC population vector obtained by simulating the planned action.
A prediction error is computed as the difference between the prediction and the current modulated place activation.

The full connection update then becomes:

\begin{equation}
    \Delta \textbf{W}^{k}_{i} = \eta^{k} v^{k} u^{\text{PC}}_{i} - \eta^{k}_{\text{pred}} (p_{i} - v^{k} u^{\text{PC}})
\end{equation}

\noindent where $\eta^{k},\,\eta^{k}_{\text{pred}}$ are the learning rates, interpretable as the weight contribution of the Hebbian coupling and prediction accuracy respectively.
Additionally, connections values are kept non-negative and, in order to speed up the extinction of past place-reward associations, if a non-zero prediction error is below a 0.5 then it is set to 0.5.

\paragraph{Active neuronal modulation}
Neuromodulation acts on the neuronal profile of the place cells by affecting the value of the activation gain and relocate the center of their tuning.

Gain modulation is implemented using the activity traces and a constant reference gain value $\bar{\beta}$:
\begin{equation}
    \beta_{i} = c^{k}_{a} \bar{\beta} m_{i} + (1 - m_{i}) \bar{\beta}
\end{equation}

\noindent where $c^{k}_{a}$ is a scaling gain parameter, and if it is 1 then no modulation takes place.

% remapping
Concerning center relocation, it is applied to recently active neurons with non-zero trace $m_{i}$.
For a place cell $i$ with position $\textbf{x}_{i}$ (in the vector space), it is calculated a displacement vector $q_{i}$ with respect to the current position $\textbf{x}_{j}$, identified as the most active place cell $j$.

\begin{equation}
    q_{i} = c^{k}_{b} v^{k} \exp\left(-\frac{\|\textbf{x}_{i}-\textbf{x}_{j}\|}{\sigma^{k}}\right)
\end{equation}


\noindent where $c^{k}_{b}$ is a scaling relocation parameter, while $\sigma^{k}$ the width of the Gaussian distance.
This displacemented is used to move in GC activity space and get the new GC population vector to use as tuning pattern.

Also in this case, it is ensured that the new place field center is at a minimum distance $\theta^{PC}_{\text{min}}$ from the others; here Euclidean distance is used.


\subsection{Decision making}

\subsubsection{Path-planning algorithm}
The planning of a new route is implemented as a modified Dijkstra algorithm over the coarse-grained place cell graph, provided as connectivity matrix $C$.
Its particularity is the use of a weighting $\widetilde{\textbf{W}}$ of the nodes according to the neuromodulation map.

\begin{algorithm}
\caption{Modified Dijkstra algorithm}
\begin{algorithmic}[1]
    \Require Connectivity matrix $\mathbf{C} \in \{0,1\}^{N^{\text{PC}} \times N^{\text{PC}}}$,
        Node coordinates $\mathbf{x} \in \mathbb{R}^{N^{\text{PC}} \times 2}$,
        Node weights $\widetilde{\textbf{W}} \in \mathbb{R}^{N^{\text{PC}}}$,
        Start node $s$, End node $t$
    \Ensure Shortest path from $t_{0}$ to $T$

\State $distances \gets [\infty, \infty, \ldots, \infty]$ \Comment{Initialize distances}
\State $distances[t_{0}] \gets 0$
\State $parent \gets [-1, -1, \ldots, -1]$ \Comment{Parent pointers}
\State $finalized \gets [false, false, \ldots, false]$ \Comment{Set of finalized nodes}
\State $PQ \gets \emptyset$ \Comment{Priority queue}
\State $PQ.push((0, t_{0}))$ \Comment{Insert start node with priority 0}

\While{$PQ \neq \emptyset$}
    \State $(dist, j) \gets PQ.extractMin()$
    \If{$finalized[j]$ or $dist > distances[j]$}
        \State \textbf{continue}
    \EndIf
    \State $finalized[j] \gets true$
    \If{$j = T$}
        \State \textbf{break} \Comment{Destination reached}
    \EndIf

    \For{each node $i$ where $\mathbf{C}_{i,j} = 1$ and not $finalized[i]$}
        \If{$\widetilde{\mathbf{W}}[i] < -1000$}
            \State \textbf{continue} \Comment{Skip nodes with high negative weights}
        \EndIf

        \State $\Delta x \gets \mathbf{x}_{j,0} - \mathbf{x}_{i,0}$
        \State $\Delta y \gets \mathbf{x}_{j,1} - \mathbf{x}_{i,1}$
        \State $edge\_dist \gets \sqrt{\Delta x^2 + \Delta y^2}$ \Comment{Euclidean distance}
        \State $new\_dist \gets d[j] + edge\_dist$

        \If{$new\_dist < d[i]$}
            \State $d[i] \gets new\_dist$
            \State $parent[i] \gets u$
            \State $PQ.push((new\_dist, i))$
        \EndIf
    \EndFor
\EndWhile

\State $path \gets []$
\If{$d[T] = \infty$}
    \State \Return $\emptyset$ \Comment{No path exists}
\EndIf

\State $curr \gets t$
\While{$curr \neq -1$}
    \State $path.append(curr)$
    \State $curr \gets parent[curr]$
\EndWhile
\State $path.reverse()$

\If{$path$ is empty or $path[0] \neq s$}
    \State \Return $\emptyset$
\EndIf

\Return $path$
\end{algorithmic}
\end{algorithm}
% policy algorithm


% settings

% result figure
