
\section{Appendix}

\subsection{Grid cell module}

Unlike other approaches to generate grid fields \cite{dabaghianGridCellsBorder, schoyenHexagonsAllWay2025}, we defined a correspondence between the global environment in which the agent moves, a two-dimensional Euclidean space $\mathbf{R}^{2}$, and a bounded local space of a grid module, corresponding to the torus.

The global velocity $\mathbf{v}=\{x,y\}$ is then mapped to a local velocity, scaled by a speed scalar $s^{\text{gc}}_{l}$ specific to the grid cell module $l$, which determines its periodicity in space. This approach has been used in previous works \cite{liModelingPlaceCells2019}.
% The starting position on the torus is randomly chosen when the model is initialized since what matters is the sequence of displacements without any reference to a meaningful origin.


The choice of a toroidal space is motivated by consolidated experimental evidence of the neural space of grid cells, which are organized in modules of different sizes spanning the animal's environment. However, the shape of their firing pattern is known to be hexagonal, which corresponds to the optimal tiling of a two-dimensional plane, giving rise to a neural space lying on a twisted torus.
In this work, for simplicity, we consider a square tiling and thus a square torus, without much loss of generality except for the slight increase of grid cells required for a sufficiently cover.

A grid cell module $l$ of size $N^{\text{gc}}$ is identified by a set of positions defined over a square centered on the origin and size of $2$, such that $\{(x_{i}, y_{i})\;\vert\; i \in N^{\text{gc}}\;\land\;x_{i},y_{i}\in (-1, 1)\}$.
This local square space has boundary conditions for each dimension, such that, for instance, when $x_{t} + s^{\text{gc}}_{l} \cdot v_{x} > 2$ the position update is taken to the other side $x_{t+1} = x_{t} + s^{\text{gc}}_{l} \cdot v_{x} - 2$, where $s^{\text{gc}}_{l}$ is the scale of the velocity in the local space of the module $l$ with respect to the real global agent velocity $v=\{v_{x}, v_{y}\}$.
When the module is initialized, the starting positions of its cells are uniformly distributed over the square forming a lattice.
When the agent is reset in a new position at the beginning of new trial, a displacement vector is applied to the last cells positions such that the mapping between the module local space and the global environment is preserved.

The firing rate vector of each cell is determined with respect to a 2D Gaussian tuning curve centered at the origin at $(0,0)$, and it is calculated as \\ $r_{i} = \exp\left(-\frac{x^{2}_{i} + y^{2}_{i}}{\sigma^{\text{gc}}_{l}}\right)$, where $\sigma^{\text{gc}}_{l}$ is the width of the tuning curve for module $l$.

The final population vector of the grid cell network GC is the concatenated and flattened firing rate vector of all modules $\textbf{u}^{\text{GC}}$.

\hfill \break
In our model, each grid cell had a tuning width of 0.04. They were defined as 8 modules of size 36, and the relative speed scales were \\ $\{1., 0.8, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1, 0.07\}$.

\subsection{Place cells}

\paragraph{Tuning formation}
% formation | forward + competition
The tuning of a new place cell is simply defined as the current GC population vector $\textbf{u}^{\text{GC}}_{t}$, and its index is that of the first silent cell, which is added to the forward weight matrix $\textbf{W}_{i}^{\text{GC}to \text{PC}}\leftarrow \textbf{u}^{\text{GC}}$.

In order to avoid overlapping of place fields, the tuning process is aborted in case the cosine similarity of the new pattern and the old ones is greater than a threshold $\theta^{\text{PC}}_{\text{rep}}$.

Each cell represents a position in the GC activity space, which can be considered a node within a graph of place cells (PC).
Although it is totally possible to only use the $N^{\text{GC}}$-dimensional tuning patterns and be agnostic about the dimensionality of the space in which the agent lives, to simplify the calculations, we mapped each pattern to 2D positions in a vector space.
Then, the PC recurrent connectivity matrix is calculated with a nearest neighbors algorithm, which instead of a fixed number $K$ of neighbors uses a distance threshold $\theta^{\text{PC}}_{\text{rec}}$.

\paragraph{Activity} The current firing rate of the PC population is determined by the cosine similarity between the GC input and the forward weight matrix, then passed through a generalized sigmoid $\phi(z)=\left[1 + \exp(-\beta(z-\alpha))\right]^{-1}$.
The parameter $\alpha$ represents the activation threshold, or horizontal offset, while $\beta$ the gain, or steepness.

\[
    u^{\text{PC}}_{i}=\phi\left(\cos\left(\textbf{u}^{\text{GC}},\textbf{W}^{\text{GC},\text{PC}}_{i}\right)\right)
\]

It is also defined as an activity trace, which has an upper value of 1 and decays exponentially:

\[
    m_{i} = - m_{i} / \tau^{\text{PC}} + u_{i}
\]

\noindent It is used as a proxy for a memory trace.

\hfill \break
In the model, a PC population is defined by its average place field size, determining the granularity of the representation of the place.

Below, a table of the parameters specific to the two PC populations is reported.

\begin{table}[h]
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        & $\beta$ & $\alpha$ & $\theta_{\text{rep}}^{\text{PC}}$ & $\theta_{\text{rec}}^{\text{PC}}$ & $\tau^{\text{PC}}$ \\
        \midrule
        pc   & 33.0  & 1.0  & 0.86  & 43  & 140 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{PC layer parameters}}
\end{table}

\subsection{Modulation}

Neuromodulation is defined in terms of a leaky variable $v$ whose state is perturbed by an external input $x$, whose qualitative meaning differs for each neuromodulator $k$.

\begin{equation}
\begin{aligned}
    v_{k} &= - v_{k} / \tau_{k} + x_{k} \\
    v_{k} &= \text{max}(v_{k}, 0)
\end{aligned}
\end{equation}

% plasticity + prediction error

\paragraph{Learning rule}
The connection weights $\textbf{W}^{k}$ are updated according to a plasticity rule composed of an Hebbian term, involving the leaky variable and the place cells above a certain threshold $\theta^{k}$, and a prediction error.

A prediction $\textbf{p}$ is calculated before each time-step and signifies the expected neuromodulation activation for a given place cell $\hat{v}^{k}_{i} = \textbf{W}^{k}_{i} \hat{u}^{\text{PC}}_{i}$, where $\hat{u}^{\text{PC}}_{i}$ is the PC population vector obtained by simulating the planned action.
A prediction error is computed as the difference between the prediction and the current modulated place activation.

The full connection update then becomes:

\[
    \Delta \textbf{W}^{k}_{i} = \eta^{k} v^{k} u^{\text{PC}}_{i} - \eta^{k}_{\text{pred}} (\hat{v}^{k}_{i} - v^{k}_{i}) u^{\text{PC}}
\]

\noindent where $\eta^{k},\,\eta^{k}_{\text{pred}}$ are the learning rates, interpretable as the weight contribution of the Hebbian coupling and prediction accuracy respectively.
Additionally, connections values are kept non-negative and, in order to speed up the extinction of past place-reward associations, if a non-zero prediction error is below a 0.5 then it is set to 0.5.

\paragraph{Active neuronal modulation}
Neuromodulation acts on the neuronal profile of the place cells by affecting the value of the activation gain and relocate the center of their tuning.

Gain modulation is implemented using the activity traces and a constant reference gain value $\bar{\beta}$:
\[
    \beta_{i} = c^{k}_{a} m_{i} \bar{\beta} + (1 - m_{i}) \bar{\beta}
\]

\noindent where $c^{k}_{a}$ is a scaling gain parameter, and if it is 1 then no modulation takes place.

% remapping
Concerning center relocation, it is applied to recently active neurons with non-zero trace $m_{i}$.
For a place cell $i$ with position $\textbf{x}_{i}$ (in the vector space), it is calculated a displacement vector $q_{i}$ with respect to the current position $\textbf{x}_{j}$, identified as the most active place cell $j$.

\[
    q_{i} = c^{k}_{b} v^{k} \exp\left(-\frac{\|\textbf{x}_{i}-\textbf{x}_{j}\|}{\sigma^{k}}\right)
\]


\noindent where $c^{k}_{b}$ is a scaling relocation parameter, while $\sigma^{k}$ the width of the Gaussian distance.
This displacemented is used to move in GC activity space and get the new GC population vector to use as tuning pattern.

Also in this case, it is ensured that the new place field center is at a minimum distance $\theta^{PC}_{\text{min}}$ from the others; here Euclidean distance is used.


\subsection{Decision making}

\paragraph{Behaviour selection logic}
\hfill \break
The possible behaviours are \textit{exploration} and \textit{exploitation}, and an action is defined as a 2D velocity vector.
For exploration, an action can be generated either as random navigation, using a polar vector of fixed magnitude (the speed) and angle sampled from a uniform distribution, or as a step within a goal-directed navigation plan to reach a random destination, which corresponds to a randomly sampled existing place cells.
In the goal-directed navigation the magnitude of the velocity vector is less or equal than a fixed speed value, depending on the distance from the next target position in the plan
Instead, for the exploitation behaviour, the action is a step within a goal-directed navigation towards the reward location.
The behaviour selection process depends on the experience of collision, the presence of a plan, and the success in the navigation planning. A diagram of this logic is reported in Figure \ref{fig:dm_logic}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/pcnn_dm_logic.png}
    \caption{\textbf{Diagram of the behaviour selection process}}
    \label{fig:dm_logic}
\end{figure}


\paragraph{Path-planning algorithm}
\hfill \break

\begin{algorithm}[H]
\caption{Modified Dijkstra algorithm}
\begin{algorithmic}[1]
    \Require Connectivity matrix $\mathbf{C} \in \{0,1\}^{N^{\text{PC}} \times N^{\text{PC}}}$,
        Node coordinates $\mathbf{x} \in \mathbb{R}^{N^{\text{PC}} \times 2}$,
        Node weights $\widetilde{\textbf{W}} \in \mathbb{R}^{N^{\text{PC}}}$,
        Start node $s$, End node $t$
    \Ensure Shortest path from $t_{0}$ to $T$

\State $distances \gets [\infty, \infty, \ldots, \infty]$ \Comment{Initialize distances}
\State $distances[t_{0}] \gets 0$
\State $parent \gets [-1, -1, \ldots, -1]$ \Comment{Parent pointers}
\State $finalized \gets [false, false, \ldots, false]$ \Comment{Set of finalized nodes}
\State $PQ \gets \emptyset$ \Comment{Priority queue}
\State $PQ.push((0, t_{0}))$ \Comment{Insert start node with priority 0}

\While{$PQ \neq \emptyset$}
    \State $(dist, j) \gets PQ.extractMin()$
    \If{$finalized[j]$ or $dist > distances[j]$}
        \State \textbf{continue}
    \EndIf
    \State $finalized[j] \gets true$
    \If{$j = T$}
        \State \textbf{break} \Comment{Destination reached}
    \EndIf

    \For{each node $i$ where $\mathbf{C}_{i,j} = 1$ and not $finalized[i]$}
        \If{$\widetilde{\mathbf{W}}[i] < -1000$}
            \State \textbf{continue} \Comment{Skip nodes with high negative weights}
        \EndIf

        \State $\Delta x \gets \mathbf{x}_{j,0} - \mathbf{x}_{i,0}$
        \State $\Delta y \gets \mathbf{x}_{j,1} - \mathbf{x}_{i,1}$
        \State $edge\_dist \gets \sqrt{\Delta x^2 + \Delta y^2}$ \Comment{Euclidean distance}
        \State $new\_dist \gets d[j] + edge\_dist$

        \If{$new\_dist < d[i]$}
            \State $d[i] \gets new\_dist$
            \State $parent[i] \gets u$
            \State $PQ.push((new\_dist, i))$
        \EndIf
    \EndFor
\EndWhile

\State $path \gets []$
\If{$d[T] = \infty$}
    \State \Return $\emptyset$ \Comment{No path exists}
\EndIf

\State $curr \gets t$
\While{$curr \neq -1$}
    \State $path.append(curr)$
    \State $curr \gets parent[curr]$
\EndWhile
\State $path.reverse()$

\If{$path$ is empty or $path[0] \neq s$}
    \State \Return $\emptyset$
\EndIf

\Return $path$
\end{algorithmic}
\end{algorithm}\label{alg:dijk}
% policy algorithm

% result figure
The planning of a new route is implemented as a modified Dijkstra algorithm over the place cell graph, provided as connectivity matrix $C$.
Its particularity is the use of a weighting $\widetilde{\textbf{W}}$ of the nodes according to the neuromodulation map. A description is reported in algorithm \ref{alg:dijk}.


\subsection{Environments}

The game in which test the model has been developed with the python library Pygame, used under license GNU LGPL version 2.1 and available at \url{https://github.com/pygame/pygame}.
The environment layout consisting in a customizable arrangement of vertical and horizontal hard walls with variable length and fixed width. Below in Figure \ref{fig:rooms} some samples are shown.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/envs.png}
    \caption{\textbf{Sample of generated environments}}
    \label{fig:rooms}
\end{figure}

The reward object is defined as a circle with size the 5\% of the total environment area.
When the agent position is within its boundary, it is provided a binary signal $R\sim \mathcal{B}(p_{r})$ drawn from a Bernoulli with probability $p_{r}=0.6$.
The duration of the reward fetching is set to 2 time steps.

The agent object is defined as a square with size the 3.3\% of the total environment area.

The testing protocol was inspired by the behavior of animals that venture into new territories in search of food. It was divided into two parts:
\begin{itemize}
    \item \textbf{exploration phase}: the agent was placed in a random location within the environment for 10'000 time steps.
        In this phase the reward is not present.
        Further, in order to force greater exploration of the environment, every 3'000 steps it was teleported to another random location. This external intervention was meant to mitigate the randomness in the exploratory behavioural strategy of the agent.

    \item \textbf{reward phase}: a reward is insert in a random location, and it is available to be discovered. When it is encountered, the agent is teleported to a random location within the environement, and after a fixed about \~100 time step it is enabled its reward-seeking behaviour, in
        the form of goal-directed navigation.
        The total duration of this phase is set to 20'000 time steps.

\end{itemize}

An episode is defined as a continuous trajectory during the reward phase, namely a set of time steps starting from when the agent is place in a position until either it finds the reward or the simulation ends.

\paragraph{Detour experiment}
The protocol is modified such that after a fixed number of episodes the layout of the environment is changed, \textit{e.g.} a wall is inserted.
This experiment is meant to test the ability to reach the reward location by using the same cognitive map, and possibly update it with the new sensory information, such as the detection of the new boundaries. 

\paragraph{Changing reward experiment}
During the reward phase, the reward location is changed after a fixed number of fetches.


