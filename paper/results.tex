\section{Results}

% benchmarks used
\subsection{Naturalistic task}
The evaluation of the model ability to construct and utilize a cognitive map was defined as the total rewards collected during several rewards trials in different environments.
The protocol was inspired by the behaviour of animals who are end up in a new territory in search for food in various locations.

The testing environments varied in layout, determined by the position and number of walls.

First there was the exploration phase, in which the agent was placed in a random location and let roam through a pseudo-random walk for 5000 steps. Depending on environment layout, the formed map was more or less matching the the actual topology.
Then the exploitation phase began, in which a circle the 5\% of the total are was designated to provide a binary reward $R\sim \mathcal{B}(p_{r})$ drawn from a Bernoully with probability $p_{r}$.
When the reward was successfully fetched, the agent was teleported to a random location in the environment.
Further, in order to mimic the depletion of a food source the reward area was modeled with as leaky variable $\dot{v}_{r}=(E_{r}-v_{r})/\tau_{r} + R$, and its location moved whenever its level went below a threshold $v_{r}<\theta_{r}$.

\hfill \break
The optimization of the parameters of the model was carried out using the Covariance-Matrix Adaptation evolutionary strategy (CMA-ES) \cite{igelCovarianceMatrixAdaptation2007} with a population of 256 individuals for 100 generations.


\subsection{Performance in wayfinding}
Our primary aim was to evaluate the formation of the cognitive map through neuromodulation in terms of performance of goal-navigation in different environments.
The best model resulting from evolution reached solid navigation and adaptation skills.
The agent was able to visit a significant portion of the environment during exploration, and use neuromodulation to produce useful spatial representations.

In figure \ref{fig:main_results}, three emerging cognitive maps are shown different environments.
The left panel of plot \ref{fig:main_results}-\textbf{a} displays fine-grained place cells associated with collisions and reward events, signaling boundaries (in blue) and reward (in green) locations.
The overlapping of these two representations and the coarse-grained place cells (in pink) is what we refer to as a cognitive map, since these are the main source of spatial and contextual information used during planned navigation, whose path is depicted as a grey line.
The right panel instead portraits the actual enviroment with walls (black), reward location (green), and multiple trajectories (red).
During exploration, the main areas were visited, until the reward position was located, and goal-directed navigation dominated, as highlighted by the density of path lines.
Considering the position of the walls and corners, the layout of this environment does not make target locations always visible, as it is a non-convex area, and can be thus be classified as wayfinding \cite{meilingerQualitativeDifferencesMemory2016}.
The challenge of not being able to use straight lines is overcome by the graph approach using local data and the switching between the differently grained place cells layers, allowing the agent to successfully going around obstructions and avoiding being stuck.
Further, the plans were also minimizing path length by construction, within the part of the space covered by the cognitive map.

However, it is worth nothing that not all simulations resulted in reward being found in the first place, due to the randomness of the exploratory process; this was more pronounced in complex enviroment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/main_results.png}
    \caption{\textsc{Cognitive maps and performance results} - \textbf{a}: \textit{the plot on the left represent a cognitive map over a space, together with the plan (grey line) to reach a target location from a starting position. The  plot on the right is a view of the same enviroment but with highlighted walls (black thick lines) the reward (green circle), trajectory (red line), agent position (black square).} -
    \textbf{b}: \textit{trajectories for multiple trials with the agent starting at the same position (black square) but with the reward location (green circles) periodically moving} -
    \textbf{c}: \textit{performance results over a common environment settigs for five different models: chance (plasticity disabled) being the lowest all all, no modulation of place fields for DA and BND being meaningfully lower than any variant with modulation of either kind enabled.} -
\textbf{d}: \textit{cognitive map but in terms of the activity of the place cells over multiple trajectories with a fixed reward location. Further, it is marked with high opacity the activity of the DA-modulated coarse-grained place cells, with their place fields enlarged near the reward.}}
    \label{fig:main_results}
\end{figure}


\subsection{Adaptive goal representation through prediction error}
Then, we tested the adaptability to environmental changes. In this scenario, the reward object was moved after it was fetched a fixed amount of times.
Here, the difficulty lied in the unlearning of past locations and the discovery of the new ones, in a protocol similar to \cite{brzoskoNeuromodulationSpikeTimingDependentPlasticity2019}.
In plot \ref{fig:main_results}-\textbf{b} it is reported the set of trajectories over many trials with the reward displaced in three possible locations. The agent was capable of planning behaviour, as earlier, but also to explore and find the new rewards, as shown by the density of lines.
Whenever a goal path resulted in failed prediction, the DA-based sensory error weakened the association between the place cells and reward signal, leading to an extinction of its representation at that location.

\subsection{Modulation of spatial resolution affects performance}
Lastly, we investigated the effect of modulating the place cells density and field size.
The goal position was fixed, but the agent was randomly relocated after fetching; performance was defined as total number of reward count within a time window.
Our working hypothesis is these experience-driven neuronal changes would improve the quality of the cognitive map, and be reflected in the navigation abilities.
The assessment of this claim was conducted by comparing variants of the model, obtained by progressively ablating the reward (DA) and collision (BND) modulation of density and field size, but a cognitive map was still possible as their representations of goals and boundaries were preserved.
We also defined a chance level by instead blocking all modulation-based plasticity and allowing only place cells formation.
All models were ran in two different environments differing by number of internal walls, in total 2048 simulations were done for each case. However, due to the issue of the reward not being found, only the top 1024 were considered; the number of faulty runs affected all models in equal measure, since it depended solely on exploration noise.

The results are shown in plot \ref{fig:main_results}-\textbf{c}. The top reports the scores in the setting without internal walls. All models performed above chance, but the main finding is confirmation of the importance of DA modulation, supported by the statistical difference between those endowed with it and not.
Indeed, in a convex region such as this one, once the reward has been located boundary information has a limited utility.

A different situation was when more walls are present, as revealed by the results at the bottom. In this scenario, the boundary modulation turned into the statistically more dominant, although than reward one still remains better than no modulation at all.
In this environment, scores are lower as navigation became more difficult, and adequately tuning place cells fields and density improves wall representation.




