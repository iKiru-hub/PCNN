@article{abrahamMetaplasticityPlasticitySynaptic1996,
  title = {Metaplasticity: The Plasticity of Synaptic Plasticity},
  shorttitle = {Metaplasticity},
  author = {Abraham, W. C. and Bear, M. F.},
  year = {1996},
  month = apr,
  journal = {Trends in Neurosciences},
  volume = {19},
  number = {4},
  pages = {126--130},
  issn = {0166-2236},
  doi = {10.1016/s0166-2236(96)80018-x},
  abstract = {In this paper, we review experimental evidence for a novel form of persistent synaptic plasticity we call metaplasticity. Metaplasticity is induced by synaptic or cellular activity, but it is not necessarily expressed as a change in the efficacy of normal synaptic transmission. Instead, it is manifest as a change in the ability to induce subsequent synaptic plasticity, such as long-term potentiation or depression. Thus, metaplasticity is a higher-order form of synaptic plasticity. Metaplasticity might involve alterations in NMDA-receptor function in some cases, but there are many other candidate mechanisms. The induction of metaplasticity complicates the interpretation of many commonly studied aspects of synaptic plasticity, such as saturation and biochemical correlates.},
  langid = {english},
  pmid = {8658594},
  keywords = {Animals,Humans,Neuronal Plasticity,Synaptic Transmission}
}

@article{abualigahAntLionOptimizer2021,
  title = {Ant {{Lion Optimizer}}: {{A Comprehensive Survey}} of {{Its Variants}} and {{Applications}}},
  shorttitle = {Ant {{Lion Optimizer}}},
  author = {Abualigah, Laith and Shehab, Mohammad and Alshinwan, Mohammad and Mirjalili, Seyedali and Elaziz, Mohamed Abd},
  year = {2021},
  month = may,
  journal = {Archives of Computational Methods in Engineering},
  volume = {28},
  number = {3},
  pages = {1397--1416},
  issn = {1886-1784},
  doi = {10.1007/s11831-020-09420-6},
  urldate = {2023-02-16},
  abstract = {This paper introduces a comprehensive overview of the Ant Lion Optimizer (ALO). ALO is a novel metaheuristic swarm-based approach introduced by Mirjalili in 2015 to emulate the hunting behavior of ant lions in nature life. The review is highlighted the applications that are utilized ALO algorithm to solve various optimization problems. In ALO, the best solution is determined to enhance the performance of the functional and efficient during the optimization process by finding the minimum or maximum values to solve a certain problem. Metaheuristic algorithms have become the focus of research due to introduce of decision-making and asses the benefits in solving various optimization problems. Also, a review of ALO variants is presented in this paper such as binary, modification, hybridization, enhanced, and others. The classifications of the ALO's applications include the benchmark functions, machine learning applications, networks applications, engineering applications, software engineering, and Image processing. Finally, According to the reviewed papers published in the literature, the ALO algorithm is mostly utilized in solving various optimization problems. Presenting an overview and reviewing the ALO applications are the main aims of this review paper.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/XGYWWHF4/Abualigah et al. - 2021 - Ant Lion Optimizer A Comprehensive Survey of Its .pdf}
}

@misc{AcceptTermsConditions,
  title = {Accept {{Terms}} and {{Conditions}} on {{JSTOR}}},
  urldate = {2018-10-16},
  howpublished = {https://www-jstor-org.ezproxy.is.ed.ac.uk/tc/accept?origin=\%2Fstable\%2Fpdf\%2F25662808.pdf\%3Frefreqid\%3Dexcelsior\%253Aceea4f9e9edaeac53421430dfeb65bc0},
  file = {/Users/daniekru/Zotero/storage/WAR6Z9BE/accept.html}
}

@misc{achiamSurpriseBasedIntrinsicMotivation2017,
  title = {Surprise-{{Based Intrinsic Motivation}} for {{Deep Reinforcement Learning}}},
  author = {Achiam, Joshua and Sastry, Shankar},
  year = {2017},
  month = mar,
  number = {arXiv:1703.01732},
  eprint = {1703.01732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.01732},
  urldate = {2022-09-20},
  abstract = {Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as \${\textbackslash}epsilon\$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the \$k\$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/CFYUGJZ4/Achiam and Sastry - 2017 - Surprise-Based Intrinsic Motivation for Deep Reinf.pdf;/Users/daniekru/Zotero/storage/JUT3AI26/1703.html}
}

@inproceedings{agrawalAnalysisThompsonSampling2012,
  title = {Analysis of {{Thompson Sampling}} for the {{Multi-armed Bandit Problem}}},
  booktitle = {Proceedings of the 25th {{Annual Conference}} on {{Learning Theory}}},
  author = {Agrawal, Shipra and Goyal, Navin},
  year = {2012},
  month = jun,
  pages = {39.1-39.26},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2024-03-25},
  abstract = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the stochastic multi-armed bandit problem. More precisely, for the stochastic two-armed bandit problem, the expected regret in time T is O({\textbackslash}frac{\textbackslash}ln T∆ + {\textbackslash}frac1∆{\textasciicircum}3). And, for the stochastic N-armed bandit problem, the expected regret in time T is O({\textbackslash}left[{\textbackslash}left({\textbackslash}sum\_i=2{\textasciicircum}N {\textbackslash}frac1{\textbackslash}Delta\_i{\textasciicircum}2{\textbackslash}right){\textasciicircum}2{\textbackslash}right] {\textbackslash}ln T). Our bounds are optimal but for the dependence on {\textbackslash}Delta\_i and the constant factors in big-Oh.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/FIASC57Q/Agrawal and Goyal - 2012 - Analysis of Thompson Sampling for the Multi-armed .pdf}
}

@inproceedings{aitkenArchitectureLearningBehave1994,
  title = {An Architecture for Learning to Behave},
  booktitle = {Proceedings of 1994 {{IEEE International Conference}} on {{Neural Networks}} ({{ICNN}}'94)},
  author = {Aitken, A.M.},
  year = {1994},
  month = jun,
  volume = {2},
  pages = {828-831 vol.2},
  doi = {10.1109/ICNN.1994.374286},
  abstract = {The SAM architecture is a novel neural network architecture, based on the cerebral neocortex, for combining unsupervised learning modules. When used as part of the control system for an agent, the architecture enables the agent to learn the functional semantics of its motor outputs and sensory inputs, and to acquire behavioral sequences by imitating other agents (learning by 'watching'). This involves attempting to recreate the sensory sequences the agent has been exposed to. The architecture scales well to multiple motor and sensory modalities, and to more complex behavioral requirements. The SAM architecture may also hint at an explanation of several features of the operation of the cerebral neocortex.{$<>$}},
  keywords = {agent,Animals,Artificial intelligence,Australia,behavioral requirements,behavioral sequences,Biological neural networks,brain models,cerebral neocortex,Computer architecture,Computer science,control system,Control systems,functional semantics,Laboratories,learning by watching,learning to behave,motor outputs,multiple motor,neural net architecture,neural nets,neural network architecture,SAM architecture,sensory inputs,sensory modalities,sensory sequences,software agents,Supervised learning,unsupervised learning,Unsupervised learning},
  file = {/Users/daniekru/Zotero/storage/7HR7VL28/374286.html}
}

@misc{aliPredictiveCodingConsequence2021,
  title = {Predictive Coding Is a Consequence of Energy Efficiency in Recurrent Neural Networks},
  author = {Ali, Abdullahi and Ahmad, Nasir and de Groot, Elgar and van Gerven, Marcel A. J. and Kietzmann, Tim C.},
  year = {2021},
  month = nov,
  pages = {2021.02.16.430904},
  institution = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2021.02.16.430904},
  urldate = {2021-12-03},
  abstract = {Predictive coding represents a promising framework for understanding brain function. It postulates that the brain continuously inhibits predictable sensory input, ensuring a preferential processing of surprising elements. A central aspect of this view is its hierarchical connectivity, involving recurrent message passing between excitatory bottom-up signals and inhibitory top-down feedback. Here we use computational modelling to demonstrate that such architectural hard-wiring is not necessary. Rather, predictive coding is shown to emerge as a consequence of energy efficiency. When training recurrent neural networks to minimise their energy consumption while operating in predictive environments, the networks self-organise into prediction and error units with appropriate inhibitory and excitatory interconnections, and learn to inhibit predictable sensory input. Moving beyond the view of purely top-down driven predictions, we furthermore demonstrate, via virtual lesioning experiments, that networks perform predictions on two timescales: fast lateral predictions among sensory units, and slower prediction cycles that integrate evidence over time.},
  chapter = {New Results},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/UY22DSX3/Ali et al. - 2021 - Predictive coding is a consequence of energy effic.pdf;/Users/daniekru/Zotero/storage/IGHHFP7A/2021.02.16.html}
}

@incollection{allenAnimalConsciousness2017,
  title = {Animal {{Consciousness}}},
  booktitle = {The {{Blackwell Companion}} to {{Consciousness}}},
  author = {Allen, Colin and Trestman, Michael},
  year = {2017},
  pages = {63--76},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781119132363.ch5},
  urldate = {2023-06-21},
  abstract = {This article surveys philosophical and scientific issues arising from questions about animal consciousness. These questions include: which animals have consciousness and what (if anything) that consciousness might be like. Just what sort(s) of science can bear on these questions is a live issue, but investigations of the behavior and neurophysiology of a wide taxonomic range of animals, as well as the phylogenetic relationships among taxa are relevant. Such questions are also deeply philosophical, with epistemological, metaphysical, and phenomenological dimensions. Progress will ultimately require interdisciplinary work by philosophers willing to engage with the empirical details of animal biology, as well as scientists who are sensitive to the philosophical complexities of the issue. The article discusses motivations for studying animal consciousness, and the common framing of such issues by analogy to the human case. Various concepts of consciousness, their historical background, and the related epistemological and metaphysical issues are described. Theories about the structure and function of consciousness are identified and assessed in the context of ideas about the evolution and distribution of consciousness, and comparative approaches to specific cognitive capacities that have often been taken to indicate that humans are not the only conscious animals.},
  chapter = {5},
  isbn = {978-1-119-13236-3},
  langid = {english},
  keywords = {animal consciousness,comparative cognition,emotions,episodic memory,evolution,learning,metacognition,mirror self-recognition,pain,to study},
  file = {/Users/daniekru/Zotero/storage/EL64EW7H/Allen and Trestman - 2017 - Animal Consciousness.pdf;/Users/daniekru/Zotero/storage/Z8RNRWM9/9781119132363.html}
}

@article{allenNonspatialSequenceCoding2016,
  title = {Nonspatial {{Sequence Coding}} in {{CA1 Neurons}}},
  author = {Allen, Timothy A. and Salz, Daniel M. and McKenzie, Sam and Fortin, Norbert J.},
  year = {2016},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {36},
  number = {5},
  pages = {1547--1563},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2874-15.2016},
  urldate = {2023-01-23},
  abstract = {The hippocampus is critical to the memory for sequences of events, a defining feature of episodic memory. However, the fundamental neuronal mechanisms underlying this capacity remain elusive. While considerable research indicates hippocampal neurons can represent sequences of locations, direct evidence of coding for the memory of sequential relationships among nonspatial events remains lacking. To address this important issue, we recorded neural activity in CA1 as rats performed a hippocampus-dependent sequence-memory task. Briefly, the task involves the presentation of repeated sequences of odors at a single port and requires rats to identify each item as ``in sequence'' or ``out of sequence''. We report that, while the animals' location and behavior remained constant, hippocampal activity differed depending on the temporal context of items---in this case, whether they were presented in or out of sequence. Some neurons showed this effect across items or sequence positions (general sequence cells), while others exhibited selectivity for specific conjunctions of item and sequence position information (conjunctive sequence cells) or for specific probe types (probe-specific sequence cells). We also found that the temporal context of individual trials could be accurately decoded from the activity of neuronal ensembles, that sequence coding at the single-cell and ensemble level was linked to sequence memory performance, and that slow-gamma oscillations (20--40 Hz) were more strongly modulated by temporal context and performance than theta oscillations (4--12 Hz). These findings provide compelling evidence that sequence coding extends beyond the domain of spatial trajectories and is thus a fundamental function of the hippocampus. SIGNIFICANCE STATEMENT The ability to remember the order of life events depends on the hippocampus, but the underlying neural mechanisms remain poorly understood. Here we addressed this issue by recording neural activity in hippocampal region CA1 while rats performed a nonspatial sequence memory task. We found that hippocampal neurons code for the temporal context of items (whether odors were presented in the correct or incorrect sequential position) and that this activity is linked with memory performance. The discovery of this novel form of temporal coding in hippocampal neurons advances our fundamental understanding of the neurobiology of episodic memory and will serve as a foundation for our cross-species, multitechnique approach aimed at elucidating the neural mechanisms underlying memory impairments in aging and dementia.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2016 Allen et al.. This article is freely available online through the J Neurosci Author Open Choice option.},
  langid = {english},
  pmid = {26843637},
  keywords = {electrophysiology,episodic memory,hippocampus,rats,sequence memory,temporal context},
  file = {/Users/daniekru/Zotero/storage/2LIARBLU/Allen et al. - 2016 - Nonspatial Sequence Coding in CA1 Neurons.pdf}
}

@article{almePlaceCellsHippocampus2014,
  title = {Place Cells in the Hippocampus: {{Eleven}} Maps for Eleven Rooms},
  shorttitle = {Place Cells in the Hippocampus},
  author = {Alme, Charlotte B. and Miao, Chenglin and Jezek, Karel and Treves, Alessandro and Moser, Edvard I. and Moser, May-Britt},
  year = {2014},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {52},
  pages = {18428--18435},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1421056111},
  urldate = {2023-03-06},
  abstract = {The contribution of hippocampal circuits to high-capacity episodic memory is often attributed to the large number of orthogonal activity patterns that may be stored in these networks. Evidence for high-capacity storage in the hippocampus is missing, however. When animals are tested in pairs of environments, different combinations of place cells are recruited, consistent with the notion of independent representations. However, the extent to which representations remain independent across larger numbers of environments has not been determined. To investigate whether spatial firing patterns recur when animals are exposed to multiple environments, we tested rats in 11 recording boxes, each in a different room, allowing for 55 comparisons of place maps in each animal. In each environment, activity was recorded from neuronal ensembles in hippocampal area CA3, with an average of 30 active cells per animal. Representations were highly correlated between repeated tests in the same room but remained orthogonal across all combinations of different rooms, with minimal overlap in the active cell samples from each environment. A low proportion of cells had activity in many rooms but the firing locations of these cells were completely uncorrelated. Taken together, the results suggest that the number of independent spatial representations stored in hippocampal area CA3 is large, with minimal recurrence of spatial firing patterns across environments.},
  keywords = {optional},
  file = {/Users/daniekru/Zotero/storage/WM8DZEN6/Alme et al. - 2014 - Place cells in the hippocampus Eleven maps for el.pdf}
}

@article{alonBiologicalNetworksTinkerer2003,
  title = {Biological {{Networks}}: {{The Tinkerer}} as an {{Engineer}}},
  shorttitle = {Biological {{Networks}}},
  author = {Alon, U.},
  year = {2003},
  month = sep,
  journal = {Science},
  volume = {301},
  number = {5641},
  pages = {1866--1867},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1089072},
  urldate = {2022-11-04},
  abstract = {This viewpoint comments on recent advances in understanding the design principles of biological networks. It highlights the surprising discovery of ``good-engineering'' principles in biochemical circuitry that evolved by random tinkering.},
  file = {/Users/daniekru/Zotero/storage/KYM6FIB3/Alon - 2003 - Biological Networks The Tinkerer as an Engineer.pdf}
}

@article{alonNetworkMotifsTheory2007,
  title = {Network Motifs: Theory and Experimental Approaches},
  shorttitle = {Network Motifs},
  author = {Alon, Uri},
  year = {2007},
  month = jun,
  journal = {Nature Reviews Genetics},
  volume = {8},
  number = {6},
  pages = {450--461},
  publisher = {Nature Publishing Group},
  issn = {1471-0064},
  doi = {10.1038/nrg2102},
  urldate = {2022-11-04},
  abstract = {Transcription regulation networks seem to be built of a few regulatory patterns called network motifs.Each network motif can carry out defined information-processing functions. These functions have been experimentally studied in selected systems, mostly Escherichia coli.Negative autoregulation can speed up responses and reduce fluctuations, whereas positive autoregulation slows responses and increases variations.Coherent feedforward loops can show persistence detection, whereas incoherent feedforward loops show pulse generation and response acceleration.Single-input modules can generate temporal programmes of expression.Dense overlapping regulons can act as arrays of gates for combinatorial decision making.Developmental networks display these network motifs, and additional motifs, such as two-point positive-feedforward loops for decision making and memory, and cascades for regulating slow multi-step processes.Network motifs in systems that have been studied experimentally so far seem to be wired together in a 'modular' way that allows us to understand the dynamics of each individual motif, even when it is connected to the rest of the network.Evolution seems to have converged on the same motifs in different systems and different organisms, suggesting that they are selected for again and again on the basis of their biological functions.Other biological networks, such as signalling and neuronal networks, also show network motifs, some of which are similar to the motifs that are found in transcription networks.},
  copyright = {2007 Nature Publishing Group},
  langid = {english},
  keywords = {Agriculture,Animal Genetics and Genomics,Biomedicine,Cancer Research,Gene Function,general,Human Genetics},
  file = {/Users/daniekru/Zotero/storage/EDMDVG4Y/Alon - 2007 - Network motifs theory and experimental approaches.pdf;/Users/daniekru/Zotero/storage/9HN2928Z/nrg2102.html}
}

@article{andersonHeterogeneousModulationPlace2003,
  title = {Heterogeneous {{Modulation}} of {{Place Cell Firing}} by {{Changes}} in {{Context}}},
  author = {Anderson, Michael I. and Jeffery, Kathryn J.},
  year = {2003},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {23},
  number = {26},
  pages = {8827--8835},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.23-26-08827.2003},
  urldate = {2025-02-24},
  abstract = {Hippocampal place cells show spatially localized activity that can be modulated by both geometric information (e.g., the distances and directions of features in the environment) and nongeometric information (e.g., colors, odors, and possibly behaviors). Nongeometric information may allow the discrimination of different spatial contexts. Understanding how nongeometric (or contextual) information affects hippocampal activity is important in light of proposals that the hippocampus may play a role in constructing a representation of spatial context. We investigated the contextual modulation of place cell activity by recording hippocampal place cells while rats foraged in compound contexts comprising black or white color paired with lemon or vanilla odor. Some cells responded to the color or odor changes alone, but most responded to varying combinations of both. Thus, we demonstrate, for the first time, that there is a heterogeneous input by contextual inputs into the hippocampus. We propose a model of contextual remapping of place cells in which the geometric inputs are selectively activated by subsets of contextual stimuli. Because it appears that different place cells are affected by different subsets of contextual stimuli, the representation of the entire context would require activity at the population level, supporting a role for the hippocampus in constructing a representation of spatial context.},
  chapter = {Behavioral/Systems/Cognitive},
  copyright = {Copyright {\copyright} 2003 Society for Neuroscience 0270-6474/03/238827-09.00/0},
  langid = {english},
  pmid = {14523083},
  keywords = {context,hippocampus,place cells,remapping,single-unit,spatial representation},
  file = {/Users/daniekru/Zotero/storage/P5PMI83X/Anderson and Jeffery - 2003 - Heterogeneous Modulation of Place Cell Firing by Changes in Context.pdf}
}

@article{aokiGABAAGABABReceptors2018,
  title = {{{GABA-A}} and {{GABA-B Receptors}} in {{Filial Imprinting Linked With Opening}} and {{Closing}} of the {{Sensitive Period}} in {{Domestic Chicks}} ({{Gallus}} Gallus Domesticus)},
  author = {Aoki, Naoya and Yamaguchi, Shinji and Fujita, Toshiyuki and Mori, Chihiro and Fujita, Eiko and Matsushima, Toshiya and Homma, Koichi J.},
  year = {2018},
  journal = {Frontiers in Physiology},
  volume = {9},
  pages = {1837},
  issn = {1664-042X},
  doi = {10.3389/fphys.2018.01837},
  abstract = {Filial imprinting of domestic chicks has a well-defined sensitive (critical) period lasting in the laboratory from hatching to day 3. It is a typical model to investigate the molecular mechanisms underlying memory formation in early learning. We recently found that thyroid hormone 3,5,3{$\prime$}-triiodothyronine (T\textsubscript{3}) is a determinant of the sensitive period. Rapid increases in cerebral T\textsubscript{3} levels are induced by imprinting training, rendering chicks imprintable. Furthermore, the administration of exogenous T\textsubscript{3} makes chicks imprintable on days 4 or 6 even after the sensitive period has ended. However, how T\textsubscript{3} affects neural transmission to enable imprinting remains mostly unknown. In this study, we demonstrate opposing roles for gamma-aminobutyric acid (GABA)-A and GABA-B receptors in imprinting downstream of T\textsubscript{3}. Quantitative reverse transcription polymerase chain reaction and immunoblotting showed that the GABA-A receptor expression increases gradually from days 1 to 5, whereas the GABA-B receptor expression gradually decreases. We examined whether neurons in the intermediate medial mesopallium (IMM), the brain region responsible for imprinting, express both types of GABA receptors. Immunostaining showed that morphologically identified putative projection neurons express both GABA-A and GABA-B receptors, suggesting that those GABA receptors interact with each other in these cells to modulate the IMM outputs. The roles of GABA-A and GABA-B receptors were investigated using various agonists and antagonists. Our results show that GABA-B receptor antagonists suppressed imprinting on day 1, while its agonists made day 4 chicks imprintable without administration of exogenous T\textsubscript{3}. By contrast, GABA-A receptor agonists suppressed imprinting on day 1, while its antagonists induced imprintability on day 4 without exogenous T\textsubscript{3}. Furthermore, both GABA-A receptor agonists and GABA-B receptor antagonists suppressed T\textsubscript{3}-induced imprintability on day 4 after the sensitive period has ended. Our data from these pharmacological experiments indicate that GABA-B receptors facilitate imprinting downstream of T\textsubscript{3} by initiating the sensitive period, while the GABA-A receptor contributes to the termination of the sensitive period. In conclusion, we propose that opposing roles of GABA-A and GABA-B receptors in the brain during development determine the induction and termination of the sensitive period.}
}

@article{apicellaSurveyModernTrainable2021,
  title = {A Survey on Modern Trainable Activation Functions},
  author = {Apicella, Andrea and Donnarumma, Francesco and Isgr{\`o}, Francesco and Prevete, Roberto},
  year = {2021},
  month = jun,
  journal = {Neural Networks},
  volume = {138},
  pages = {14--32},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.01.026},
  urldate = {2024-12-12},
  abstract = {In neural networks literature, there is a strong interest in identifying and defining activation functions which can improve neural network performance. In recent years there has been a renovated interest in the scientific community in investigating activation functions which can be trained during the learning process, usually referred to as trainable, learnable or adaptable activation functions. They appear to lead to better network performance. Diverse and heterogeneous models of trainable activation function have been proposed in the literature. In this paper, we present a survey of these models. Starting from a discussion on the use of the term ``activation function'' in literature, we propose a taxonomy of trainable activation functions, highlight common and distinctive proprieties of recent and past models, and discuss main advantages and limitations of this type of approach. We show that many of the proposed approaches are equivalent to adding neuron layers which use fixed (non-trainable) activation functions and some simple local rule that constrains the corresponding weight layers.},
  keywords = {Activation functions,Learnable activation functions,Machine learning,Neural networks,Trainable activation functions},
  file = {/Users/daniekru/Zotero/storage/NTJ38X85/Apicella et al. - 2021 - A survey on modern trainable activation functions.pdf;/Users/daniekru/Zotero/storage/7TM4A8BM/S0893608021000344.html}
}

@article{araujoParasitismDiversityLife2003,
  title = {Parasitism, the Diversity of Life, and Paleoparasitology},
  author = {Ara{\'u}jo, Adauto and Jansen, Ana Maria and Bouchet, Fran{\c c}oise and Reinhard, Karl and Ferreira, Luiz Fernando},
  year = {2003},
  month = jan,
  journal = {Mem{\'o}rias do Instituto Oswaldo Cruz},
  volume = {98},
  pages = {5--11},
  publisher = {Instituto Oswaldo Cruz, Minist{\'e}rio da Sa{\'u}de},
  issn = {0074-0276, 1678-8060},
  doi = {10.1590/S0074-02762003000900003},
  urldate = {2023-06-27},
  abstract = {The parasite-host-environment system is dynamic, with several points of equilibrium. This makes it difficult to trace the thresholds between benefit and damage, and therefore, the definitions of commensalism, mutualism, and symbiosis become worthless. Therefore, the same concept of parasitism may encompass commensalism, mutualism, and symbiosis. Parasitism is essential for life. Life emerged as a consequence of parasitism at the molecular level, and intracellular parasitism created evolutive events that allowed species to diversify. An ecological and evolutive approach to the study of parasitism is presented here. Studies of the origin and evolution of parasitism have new perspectives with the development of molecular paleoparasitology, by which ancient parasite and host genomes can be recovered from disappeared populations. Molecular paleoparasitology points to host-parasite co-evolutive mechanisms of evolution traceable through genome retrospective studies.},
  langid = {english},
  keywords = {ancient DNA,evolution,infectious diseases,origin of parasitism,paleoparasitology,parasitism},
  file = {/Users/daniekru/Zotero/storage/SU6H8PZS/Araújo et al. - 2003 - Parasitism, the diversity of life, and paleoparasi.pdf}
}

@article{arielIntrinsicVariabilityPv2012,
  title = {Intrinsic Variability in {{Pv}}, {{RRP}} Size, {{Ca}}(2+) Channel Repertoire, and Presynaptic Potentiation in Individual Synaptic Boutons},
  author = {Ariel, Pablo and Hoppa, Michael B. and Ryan, Timothy A.},
  year = {2012},
  journal = {Frontiers in Synaptic Neuroscience},
  volume = {4},
  pages = {9},
  issn = {1663-3563},
  doi = {10.3389/fnsyn.2012.00009},
  abstract = {The strength of individual synaptic contacts is considered a key modulator of information flow across circuits. Presynaptically the strength can be parsed into two key parameters: the size of the readily releasable pool (RRP) and the probability that a vesicle in that pool will undergo exocytosis when an action potential fires (Pv). How these variables are controlled and the degree to which they vary across individual nerve terminals is crucial to understand synaptic plasticity within neural circuits. Here we report robust measurements of these parameters in rat hippocampal neurons and their variability across populations of individual synapses. We explore the diversity of presynaptic Ca(2+) channel repertoires and evaluate their effect on synaptic strength at single boutons. Finally, we study the degree to which synapses can be differentially modified by a known potentiator of presynaptic function, forskolin. Our experiments revealed that both Pv and RRP spanned a large range, even for synapses made by the same axon, demonstrating that presynaptic efficacy is governed locally at the single synapse level. Synapses varied greatly in their dependence on N or P/Q type Ca(2+) channels for neurotransmission, but there was no association between specific channel repertoires and synaptic efficacy. Increasing cAMP concentration using forskolin enhanced synaptic transmission in a Ca(2+)-independent manner that was inversely related with a synapse's initial Pv, and independent of its RRP size. We propose a simple model based on the relationship between Pv and calcium entry that can account for the variable potentiation of synapses based on initial probability of vesicle fusion.},
  langid = {english},
  pmcid = {PMC3542534},
  pmid = {23335896},
  keywords = {exocytosis,imaging,pHluorin,readily releasable pool,release probability,synapse},
  file = {/Users/daniekru/Zotero/storage/PM4UDV8H/Ariel et al. - 2012 - Intrinsic variability in Pv, RRP size, Ca(2+) channel repertoire, and presynaptic potentiation in in.pdf}
}

@article{artzy-randrupCommentNetworkMotifs2004,
  title = {Comment on "{{Network Motifs}}: {{Simple Building Blocks}} of {{Complex Networks}}" and "{{Superfamilies}} of {{Evolved}} and {{Designed Networks}}"},
  shorttitle = {Comment on "{{Network Motifs}}},
  author = {{Artzy-Randrup}, Yael and Fleishman, Sarel J. and {Ben-Tal}, Nir and Stone, Lewi},
  year = {2004},
  month = aug,
  journal = {Science},
  volume = {305},
  number = {5687},
  pages = {1107--1107},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1099334},
  urldate = {2022-11-04},
  file = {/Users/daniekru/Zotero/storage/IC7Z27AP/Artzy-Randrup et al. - 2004 - Comment on Network Motifs Simple Building Blocks.pdf}
}

@article{asaadBackPropagationNeural2019,
  title = {Back {{Propagation Neural Network}}({{BPNN}}) and {{Sigmoid Activation Function}} in {{Multi-Layer Networks}}},
  author = {Asaad, Renas and Ali, Rasan},
  year = {2019},
  month = nov,
  journal = {Academic Journal of Nawroz University},
  volume = {8},
  pages = {216},
  doi = {10.25007/ajnu.v8n4a464},
  abstract = {Back propagation neural network are known for computing the problems that cannot easily be computed (huge datasets analysis or training) in artificial neural networks. The main idea of this paper is to implement XOR logic gate by ANNs using back propagation neural network for back propagation of errors, and sigmoid activation function. This neural network to map non-linear threshold gate. The non-linear used to classify binary inputs (x1, x2) and passing it through hidden layer for computing coefficient\_errors and gradient\_errors (Cerrors, Gerrors), after computing errors by (ei = Output\_desired- Output\_actual) the weights and thetas ({$\Delta$}Wji = ({$\alpha$})(Xj)(gi), {$\Delta\upvarTheta$}j = ({$\alpha$})(-1)(gi)) are changing according to errors. Sigmoid activation function is = sig(x)=1/(1+e-x) and Derivation of sigmoid is = dsig(x) = sig(x)(1-sig(x)). The sig(x) and Dsig(x) is between 1 to 0.}
}

@article{auerFinitetimeAnalysisMultiarmed2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo},
  year = {2002},
  journal = {Machine Learning},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/N9AC4Q7C/Auer and Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf}
}

@article{averbeckTheoryChoiceBandit2015,
  title = {Theory of {{Choice}} in {{Bandit}}, {{Information Sampling}} and {{Foraging Tasks}}},
  author = {Averbeck, Bruno B.},
  year = {2015},
  month = mar,
  journal = {PLoS Computational Biology},
  volume = {11},
  number = {3},
  pages = {e1004164},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1004164},
  urldate = {2024-03-25},
  abstract = {Decision making has been studied with a wide array of tasks. Here we examine the theoretical structure of bandit, information sampling and foraging tasks. These tasks move beyond tasks where the choice in the current trial does not affect future expected rewards. We have modeled these tasks using Markov decision processes (MDPs). MDPs provide a general framework for modeling tasks in which decisions affect the information on which future choices will be made. Under the assumption that agents are maximizing expected rewards, MDPs provide normative solutions. We find that all three classes of tasks pose choices among actions which trade-off immediate and future expected rewards. The tasks drive these trade-offs in unique ways, however. For bandit and information sampling tasks, increasing uncertainty or the time horizon shifts value to actions that pay-off in the future. Correspondingly, decreasing uncertainty increases the relative value of actions that pay-off immediately. For foraging tasks the time-horizon plays the dominant role, as choices do not affect future uncertainty in these tasks., Numerous choice tasks have been used to study decision processes. Some of these choice tasks, specifically n-armed bandit, information sampling and foraging tasks, pose choices that trade-off immediate and future reward. Specifically, the best choice may not be the choice that pays off the highest reward immediately, and exploration of unknown options vs. exploiting known options can be a normatively useful strategy. We characterized the optimal choice strategies across these tasks using Markov Decision Processes (MDPs). The MDP framework can characterize optimal choice strategies when choices are affected by the value of future rewards. We found that uncertainty and time horizon have important effects on the choice strategies in these tasks. Specifically, in bandit and information sampling tasks, increasing uncertainty increases the value of exploring choice options that tend to pay off in the future, while decreasing uncertainty increases the value of choice options that pay off immediately. These effects are increased when time horizons are longer. Foraging tasks differ in that uncertainty plays a minimal role. However, time horizon is still important in foraging. Specifically, for long time horizons, travel delays to rewards become less relevant.},
  pmcid = {PMC4376795},
  pmid = {25815510},
  file = {/Users/daniekru/Zotero/storage/SIVS27TJ/Averbeck - 2015 - Theory of Choice in Bandit, Information Sampling a.pdf}
}

@article{backmanEffectsWorkingMemoryTraining2011,
  title = {Effects of {{Working-Memory Training}} on {{Striatal Dopamine Release}}},
  author = {B{\"a}ckman, Lars and Nyberg, Lars and Soveri, Anna and Johansson, Jarkko and Andersson, Micael and Dahlin, Erika and Neely, Anna S. and Virta, Jere and Laine, Matti and Rinne, Juha O.},
  year = {2011},
  month = aug,
  journal = {Science},
  volume = {333},
  number = {6043},
  pages = {718--718},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1204978},
  urldate = {2024-04-26},
  abstract = {Updating of working memory has been associated with striato-frontal brain regions and phasic dopaminergic neurotransmission. We assessed raclopride binding to striatal dopamine (DA) D2 receptors during a letter-updating task and a control condition before and after 5 weeks of updating training. Results showed that updating affected DA activity before training and that training further increased striatal DA release during updating. These findings highlight the pivotal role of transient neural processes associated with D2 receptor activity in working memory.},
  file = {/Users/daniekru/Zotero/storage/CM3EC9SP/Bäckman et al. - 2011 - Effects of Working-Memory Training on Striatal Dop.pdf}
}

@incollection{baddeleyWorkingMemory1974,
  title = {Working {{Memory}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Baddeley, Alan D. and Hitch, Graham},
  year = {1974},
  volume = {8},
  pages = {47--89},
  publisher = {Elsevier},
  doi = {10.1016/S0079-7421(08)60452-1},
  urldate = {2024-05-13},
  isbn = {978-0-12-543308-2},
  langid = {english}
}

@article{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  journal = {arXiv:1409.0473 [cs, stat]},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  urldate = {2021-12-02},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/VAQB5VLW/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;/Users/daniekru/Zotero/storage/ZKN9DDND/1409.html}
}

@article{bakerEMERGENTTOOLUSE,
  title = {{{EMERGENT TOOL USE FROM MULTI-AGENT AUTOCURRICULA}}},
  author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi},
  pages = {28},
  abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a selfsupervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/7EMUZDAD/Baker et al. - EMERGENT TOOL USE FROM MULTI-AGENT AUTOCURRICULA.pdf}
}

@misc{balestrieroCookbookSelfSupervisedLearning2023,
  title = {A {{Cookbook}} of {{Self-Supervised Learning}}},
  author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
  year = {2023},
  month = apr,
  number = {arXiv:2304.12210},
  eprint = {2304.12210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.12210},
  urldate = {2023-04-26},
  abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,to study},
  file = {/Users/daniekru/Zotero/storage/BQRXNMJ2/Balestriero et al. - 2023 - A Cookbook of Self-Supervised Learning.pdf;/Users/daniekru/Zotero/storage/IZYESVCL/2304.html}
}

@article{balewskiValueDynamicsAffect2023,
  title = {Value Dynamics Affect Choice Preparation during Decision-Making},
  author = {Balewski, Zuzanna Z. and Elston, Thomas W. and Knudsen, Eric B. and Wallis, Joni D.},
  year = {2023},
  month = sep,
  journal = {Nature neuroscience},
  volume = {26},
  number = {9},
  pages = {1575--1583},
  issn = {1097-6256},
  doi = {10.1038/s41593-023-01407-3},
  urldate = {2024-11-27},
  abstract = {During decision-making, neurons in the orbitofrontal cortex (OFC) sequentially represent the value of each option in turn, but it is unclear how these dynamics are translated into a choice response. One brain region that may be implicated in this process is the anterior cingulate cortex (ACC), which strongly connects with OFC and contains many neurons that encode the choice response. We investigated how OFC value signals interacted with ACC neurons encoding the choice response by performing simultaneous high-channel count recordings from the two areas in nonhuman primates. ACC neurons encoding the choice response steadily increased their firing rate throughout the decision-making process, peaking shortly before the time of the choice response. Furthermore, the value dynamics in OFC affected ACC ramping---when OFC represented the more valuable option, ACC ramping accelerated. Because OFC tended to represent the more valuable option more frequently and for a longer duration, this interaction could explain how ACC selects the more valuable response.},
  pmcid = {PMC10576429},
  pmid = {37563295},
  file = {/Users/daniekru/Zotero/storage/484SD2K3/Balewski et al. - 2023 - Value dynamics affect choice preparation during decision-making.pdf}
}

@article{baninoVectorbasedNavigationUsing2018,
  title = {Vector-Based Navigation Using Grid-like Representations in Artificial Agents},
  author = {Banino, Andrea and Barry, Caswell and Uria, Benigno and Blundell, Charles and Lillicrap, Timothy and Mirowski, Piotr and Pritzel, Alexander and Chadwick, Martin J. and Degris, Thomas and Modayil, Joseph and Wayne, Greg and Soyer, Hubert and Viola, Fabio and Zhang, Brian and Goroshin, Ross and Rabinowitz, Neil and Pascanu, Razvan and Beattie, Charlie and Petersen, Stig and Sadik, Amir and Gaffney, Stephen and King, Helen and Kavukcuoglu, Koray and Hassabis, Demis and Hadsell, Raia and Kumaran, Dharshan},
  year = {2018},
  month = may,
  journal = {Nature},
  volume = {557},
  number = {7705},
  pages = {429--433},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0102-6},
  urldate = {2023-10-27},
  abstract = {Deep neural networks have achieved impressive successes in fields ranging from object recognition to complex games such as Go1,2. Navigation, however, remains a substantial challenge for artificial agents, with deep neural networks trained by reinforcement learning3--5 failing to rival the proficiency of mammalian spatial behaviour, which is underpinned by grid cells in the entorhinal cortex6. Grid cells are thought to provide a multi-scale periodic representation that functions as a metric for coding space7,8 and is critical for integrating self-motion (path integration)6,7,9 and planning direct trajectories to goals (vector-based navigation)7,10,11. Here we set out to leverage the computational functions of grid cells to develop a deep reinforcement learning agent with mammal-like navigational abilities. We first trained a recurrent network to perform path integration, leading to the emergence of representations resembling grid cells, as well as other entorhinal cell types12. We then showed that this representation provided an effective basis for an agent to locate goals in challenging, unfamiliar, and changeable environments---optimizing the primary objective of navigation through deep reinforcement learning. The performance of agents endowed with grid-like representations surpassed that of an expert human and comparison agents, with the metric quantities necessary for vector-based navigation derived from grid-like units within the network. Furthermore, grid-like representations enabled agents to conduct shortcut behaviours reminiscent of those performed by mammals. Our findings show that emergent grid-like representations furnish agents with a Euclidean spatial metric and associated vector operations, providing a foundation for proficient navigation. As such, our results support neuroscientific theories that see grid cells as critical for vector-based navigation7,10,11, demonstrating that the latter can be combined with path-based strategies to support navigation in challenging environments.},
  copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
  langid = {english},
  keywords = {Computer science,Learning algorithms},
  file = {/Users/daniekru/Zotero/storage/D4B8FQ72/Bonino_2018_supp.pdf;/Users/daniekru/Zotero/storage/QQRAQWB4/Banino et al. - 2018 - Vector-based navigation using grid-like representa.pdf}
}

@misc{banMultifacetContextualBandits2021,
  title = {Multi-Facet {{Contextual Bandits}}: {{A Neural Network Perspective}}},
  shorttitle = {Multi-Facet {{Contextual Bandits}}},
  author = {Ban, Yikun and He, Jingrui and Cook, Curtiss B.},
  year = {2021},
  month = jun,
  number = {arXiv:2106.03039},
  eprint = {2106.03039},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-03},
  abstract = {Contextual multi-armed bandit has shown to be an effective tool in recommender systems. In this paper, we study a novel problem of multi-facet bandits involving a group of bandits, each characterizing the users' needs from one unique aspect. In each round, for the given user, we need to select one arm from each bandit, such that the combination of all arms maximizes the final reward. This problem can find immediate applications in E-commerce, healthcare, etc. To address this problem, we propose a novel algorithm, named MuFasa, which utilizes an assembled neural network to jointly learn the underlying reward functions of multiple bandits. It estimates an Upper Confidence Bound (UCB) linked with the expected reward to balance between exploitation and exploration. Under mild assumptions, we provide the regret analysis of MuFasa. It can achieve the near-optimal \${\textbackslash}widetilde\{ {\textbackslash}mathcal\{O\}\}((K+1){\textbackslash}sqrt\{T\})\$ regret bound where \$K\$ is the number of bandits and \$T\$ is the number of played rounds. Furthermore, we conduct extensive experiments to show that MuFasa outperforms strong baselines on real-world data sets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/3LTK9QB4/Ban et al. - 2021 - Multi-facet Contextual Bandits A Neural Network P.pdf}
}

@article{barackTwoViewsCognitive2021,
  title = {Two Views on the Cognitive Brain},
  author = {Barack, David L. and Krakauer, John W.},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  volume = {22},
  number = {6},
  pages = {359--371},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/s41583-021-00448-6},
  urldate = {2023-07-13},
  abstract = {Cognition can be defined as computation over meaningful representations in the brain to produce adaptive behaviour. There are two views on the relationship between cognition and the brain that are largely implicit in the literature. The Sherringtonian view seeks to explain cognition as the result of operations on signals performed at nodes in a network and passed between them that are implemented by specific neurons and their connections in circuits in the brain. The contrasting Hopfieldian view explains cognition as the result of transformations between or movement within representational spaces that are implemented by neural populations. Thus, the Hopfieldian view relegates details regarding the identity of and connections between specific neurons to the status of secondary explainers. Only the Hopfieldian approach has the representational and computational resources needed to develop novel neurofunctional objects that can serve as primary explainers of cognition.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Intelligence,Philosophy,to study},
  file = {/Users/daniekru/Zotero/storage/A8SUCMIV/Barack and Krakauer - 2021 - Two views on the cognitive brain.pdf}
}

@article{barakNeuronalPopulationCoding2010,
  title = {Neuronal {{Population Coding}} of {{Parametric Working Memory}}},
  author = {Barak, Omri and Tsodyks, Misha and Romo, Ranulfo},
  year = {2010},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {28},
  pages = {9424--9430},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1875-10.2010},
  urldate = {2024-05-13},
  abstract = {Comparing two sequentially presented stimuli is a widely used experimental paradigm for studying working memory. The delay activity of many single neurons in the prefrontal cortex (PFC) of monkeys was found to be stimulus-specific, however, population dynamics of stimulus representation has not been elucidated. We analyzed the population state of a large number of PFC neurons during a somatosensory discrimination task. Using the tuning curves of the neurons, we derived a compact characterization of the population state. Stimulus representation by the population was found to degrade after stimulus termination, and emerge in a different form toward the end of the delay. Specifically, the tuning properties of neurons were found to change during the task. We suggest a mechanism whereby information about the stimulus is contained in activity-dependent synaptic facilitation of recurrent connections.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2010 the authors 0270-6474/10/309424-07\$15.00/0},
  langid = {english},
  pmid = {20631171},
  file = {/Users/daniekru/Zotero/storage/ZLIXIUS4/Barak et al. - 2010 - Neuronal Population Coding of Parametric Working M.pdf}
}

@article{barakWorkingModelsWorking2014,
  title = {Working Models of Working Memory},
  author = {Barak, Omri and Tsodyks, Misha},
  year = {2014},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  series = {Theoretical and Computational Neuroscience},
  volume = {25},
  pages = {20--24},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2013.10.008},
  urldate = {2024-05-13},
  abstract = {Working memory is a system that maintains and manipulates information for several seconds during the planning and execution of many cognitive tasks. Traditionally, it was believed that the neuronal underpinning of working memory is stationary persistent firing of selective neuronal populations. Recent advances introduced new ideas regarding possible mechanisms of working memory, such as short-term synaptic facilitation, precise tuning of recurrent excitation and inhibition, and intrinsic network dynamics. These ideas are motivated by computational considerations and careful analysis of experimental data. Taken together, they may indicate the plethora of different processes underlying working memory in the brain.},
  file = {/Users/daniekru/Zotero/storage/KMXSSXRR/S0959438813002158.html}
}

@article{bariDynamicDecisionMaking2021,
  title = {Dynamic Decision Making and Value Computations in Medial Frontal Cortex},
  author = {Bari, Bilal A. and Cohen, Jeremiah Y.},
  year = {2021},
  journal = {International review of neurobiology},
  volume = {158},
  pages = {83--113},
  issn = {0074-7742},
  doi = {10.1016/bs.irn.2020.12.001},
  urldate = {2024-05-07},
  pmcid = {PMC8162729},
  pmid = {33785157},
  file = {/Users/daniekru/Zotero/storage/38ANPK5Z/Bari and Cohen - 2021 - Dynamic decision making and value computations in .pdf}
}

@article{bariDynamicDecisionMaking2021a,
  title = {Dynamic Decision Making and Value Computations in Medial Frontal Cortex},
  author = {Bari, Bilal A. and Cohen, Jeremiah Y.},
  year = {2021},
  journal = {International review of neurobiology},
  volume = {158},
  pages = {83--113},
  issn = {0074-7742},
  doi = {10.1016/bs.irn.2020.12.001},
  urldate = {2024-05-07},
  pmcid = {PMC8162729},
  pmid = {33785157},
  file = {/Users/daniekru/Zotero/storage/8FCF9538/Bari and Cohen - 2021 - Dynamic decision making and value computations in .pdf}
}

@misc{bartolHippocampalSpineHead2015,
  title = {Hippocampal {{Spine Head Sizes Are Highly Precise}}},
  author = {Bartol, Thomas M. and Bromer, Cailey and Kinney, Justin and Chirillo, Michael A. and Bourne, Jennifer N. and Harris, Kristen M. and Sejnowski, Terrence J.},
  year = {2015},
  month = mar,
  primaryclass = {New Results},
  pages = {016329},
  publisher = {bioRxiv},
  doi = {10.1101/016329},
  urldate = {2024-12-12},
  abstract = {Hippocampal synaptic activity is probabilistic and because synaptic plasticity depends on its history, the amount of information that can be stored at a synapse is limited. The strong correlation between the size and efficacy of a synapse allowed us to estimate the precision of synaptic plasticity. In an electron microscopic reconstruction of hippocampal neuropil we found single axons making two or more synaptic contacts onto the same dendrites which would have shared histories of presynaptic and postsynaptic activity. The postsynaptic spine heads, but not the spine necks, of these pairs were nearly identical in size. The precision is much greater than previous estimates and requires postsynaptic averaging over a time window many seconds to minutes in duration depending on the rate of input spikes and probability of release. One Sentence Summary Spine heads on the same dendrite that receive input from the same axon are the same size.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2015, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/IDEYXNTM/Bartol et al. - 2015 - Hippocampal Spine Head Sizes Are Highly Precise.pdf}
}

@article{bartoloPrefrontalCortexPredicts2020,
  title = {Prefrontal {{Cortex Predicts State Switches}} during {{Reversal Learning}}},
  author = {Bartolo, Ramon and Averbeck, Bruno B.},
  year = {2020},
  month = jun,
  journal = {Neuron},
  volume = {106},
  number = {6},
  pages = {1044-1054.e4},
  publisher = {Elsevier},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2020.03.024},
  urldate = {2024-05-14},
  langid = {english},
  pmid = {32315603},
  keywords = {Bayesian update,large-scale recordings,macaques,model-based,neural ensemble,prefrontal cortex,reversal learning,state inference},
  file = {/Users/daniekru/Zotero/storage/DDZ2TWRK/Bartolo and Averbeck - 2020 - Prefrontal Cortex Predicts State Switches during R.pdf}
}

@article{bastonBiologicallyInspiredComputational2015,
  title = {A {{Biologically Inspired Computational Model}} of {{Basal Ganglia}} in {{Action Selection}}},
  author = {Baston, Chiara and Ursino, Mauro},
  year = {2015},
  month = nov,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2015},
  pages = {e187417},
  publisher = {Hindawi},
  issn = {1687-5265},
  doi = {10.1155/2015/187417},
  urldate = {2024-05-14},
  abstract = {The basal ganglia (BG) are a subcortical structure implicated in action selection. The aim of this work is to present a new cognitive neuroscience model of the BG, which aspires to represent a parsimonious balance between simplicity and completeness. The model includes the 3 main pathways operating in the BG circuitry, that is, the direct (Go), indirect (NoGo), and hyperdirect pathways. The main original aspects, compared with previous models, are the use of a two-term Hebb rule to train synapses in the striatum, based exclusively on neuronal activity changes caused by dopamine peaks or dips, and the role of the cholinergic interneurons (affected by dopamine themselves) during learning. Some examples are displayed, concerning a few paradigmatic cases: action selection in basal conditions, action selection in the presence of a strong conflict (where the role of the hyperdirect pathway emerges), synapse changes induced by phasic dopamine, and learning new actions based on a previous history of rewards and punishments. Finally, some simulations show model working in conditions of altered dopamine levels, to illustrate pathological cases (dopamine depletion in parkinsonian subjects or dopamine hypermedication). Due to its parsimonious approach, the model may represent a straightforward tool to analyze BG functionality in behavioral experiments.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/D3SNG68W/Baston and Ursino - 2015 - A Biologically Inspired Computational Model of Bas.pdf}
}

@misc{beaulieuLearningContinuallyLearn2020,
  title = {Learning to {{Continually Learn}}},
  author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},
  year = {2020},
  month = mar,
  number = {arXiv:2002.09571},
  eprint = {2002.09571},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.09571},
  urldate = {2022-09-13},
  abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/X3YTZ3ED/Beaulieu et al. - 2020 - Learning to Continually Learn.pdf;/Users/daniekru/Zotero/storage/Y7C7GYV2/2002.html}
}

@article{behrensLearningValueInformation2007,
  title = {Learning the Value of Information in an Uncertain World},
  author = {Behrens, Timothy E. J. and Woolrich, Mark W. and Walton, Mark E. and Rushworth, Matthew F. S.},
  year = {2007},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {9},
  pages = {1214--1221},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn1954},
  urldate = {2024-05-02},
  abstract = {Our decisions are guided by outcomes that are associated with decisions made in the past. However, the amount of influence each past outcome has on our next decision remains unclear. To ensure optimal decision-making, the weight given to decision outcomes should reflect their salience in predicting future outcomes, and this salience should be modulated by the volatility of the reward environment. We show that human subjects assess volatility in an optimal manner and adjust decision-making accordingly. This optimal estimate of volatility is reflected in the fMRI signal in the anterior cingulate cortex (ACC) when each trial outcome is observed. When a new piece of information is witnessed, activity levels reflect its salience for predicting future outcomes. Furthermore, variations in this ACC signal across the population predict variations in subject learning rates. Our results provide a formal account of how we weigh our different experiences in guiding our future actions.},
  copyright = {2007 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/HQA3X56G/Behrens et al. - 2007 - Learning the value of information in an uncertain .pdf}
}

@article{bensonHigherorderOrganizationComplex2016,
  title = {Higher-Order Organization of Complex Networks},
  author = {Benson, Austin R. and Gleich, David F. and Leskovec, Jure},
  year = {2016},
  month = jul,
  journal = {Science},
  volume = {353},
  number = {6295},
  pages = {163--166},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aad9029},
  urldate = {2022-11-04},
  abstract = {Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networks---at the level of small network subgraphs---remains largely unknown. Here, we develop a generalized framework for clustering networks on the basis of higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks, including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns.},
  file = {/Users/daniekru/Zotero/storage/TVW7UJY5/Benson et al. - 2016 - Higher-order organization of complex networks.pdf}
}

@article{bersethSMiRLSurpriseMinimizing,
  title = {{{SMiRL}}: {{Surprise Minimizing RL}} in {{Dynamic Environments}}},
  author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  pages = {13},
  abstract = {All living organisms struggle against the forces of nature to carve out niches where they can maintain homeostasis. We propose that such a search for order amidst chaos might offer a unifying principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing RL (SMiRL). SMiRL trains an agent with the objective of maximizing the probability of observed states under a model trained on previously seen states. The resulting agents can acquire proactive behaviors that seek out and maintain stable conditions, such as balancing and damage avoidance, that are closely tied to an environment's prevailing sources of entropy, such as wind, earthquakes, and other agents. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls and navigate to escape enemy agents, without any task-specific reward supervision. We further show that SMiRL can be used together with a standard task reward to accelerate reward-driven learning.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/5WACTJ5K/Berseth et al. - SMiRL Surprise Minimizing RL in Dynamic Environme.pdf}
}

@inproceedings{besbesStochasticMultiArmedBanditProblem2014,
  title = {Stochastic {{Multi-Armed-Bandit Problem}} with {{Non-stationary Rewards}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
  year = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-25},
  abstract = {In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward variation" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks."},
  file = {/Users/daniekru/Zotero/storage/DIVA8FYS/Besbes et al. - 2014 - Stochastic Multi-Armed-Bandit Problem with Non-sta.pdf}
}

@article{bibireataDynamicalPhaseSeparation2020,
  title = {Dynamical Phase Separation on Rhythmogenic Neuronal Networks},
  author = {Bibireata, Mihai and Slepukhin, Valentin M. and Levine, Alex J.},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.02868 [nlin, physics:physics, q-bio]},
  eprint = {2001.02868},
  primaryclass = {nlin, physics:physics, q-bio},
  urldate = {2020-05-27},
  abstract = {We explore the dynamics of the preB{\textbackslash}"\{o\}tzinger complex, the mammalian central pattern generator with \$N {\textbackslash}sim 10{\textasciicircum}3\$ neurons, which produces a collective metronomic signal that times the inspiration. Our analysis is based on a simple firing-rate model of excitatory neurons with dendritic adaptation (the Feldman Del Negro model [Nat. Rev. Neurosci. 7, 232 (2006), Phys. Rev. E 2010 :051911]) interacting on a fixed, directed Erd{\textbackslash}H\{o\}s-R{\textbackslash}'\{e\}nyi network. In the all-to-all coupled variant of the model, there is spontaneous symmetry breaking in which some fraction of the neurons become stuck in a high firing-rate state, while others become quiescent. This separation into firing and non-firing clusters persists into more sparsely connected networks, and is partially determined by \$k\$-cores in the directed graphs. The model has a number of features of the dynamical phase diagram that violate the predictions of mean-field analysis. In particular, we observe in the simulated networks that stable oscillations do not persist in the large-N limit, in contradiction to the predictions of mean-field theory. Moreover, we observe that the oscillations in these sparse networks are remarkably robust in response to killing neurons, surviving until only \${\textbackslash}approx 20 {\textbackslash}\%\$ of the network remains. This robustness is consistent with experiment.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Biological Physics,Quantitative Biology - Neurons and Cognition},
  file = {/Users/daniekru/Zotero/storage/WJ2637RY/Bibireata et al. - 2020 - Dynamical phase separation on rhythmogenic neurona.pdf;/Users/daniekru/Zotero/storage/RCCIHIW9/2001.html}
}

@article{bilashLateralEntorhinalCortex2023,
  title = {Lateral Entorhinal Cortex Inputs Modulate Hippocampal Dendritic Excitability by Recruiting a Local Disinhibitory Microcircuit},
  author = {Bilash, Olesia M. and Chavlis, Spyridon and Johnson, Cara D. and Poirazi, Panayiota and Basu, Jayeeta},
  year = {2023},
  month = jan,
  journal = {Cell Reports},
  volume = {42},
  number = {1},
  pages = {111962},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2022.111962},
  urldate = {2023-03-09},
  abstract = {The lateral entorhinal cortex (LEC) provides multisensory information to the hippocampus, directly to the distal dendrites of CA1 pyramidal neurons. LEC neurons perform important functions for episodic memory processing, coding for contextually salient elements of an environment or experience. However, we know little about the functional circuit interactions between the LEC and the hippocampus. We combine functional circuit mapping and computational modeling to examine how long-range glutamatergic LEC projections modulate compartment-specific excitation-inhibition dynamics in hippocampal area CA1. We demonstrate that glutamatergic LEC inputs can drive local dendritic spikes in CA1 pyramidal neurons, aided by the recruitment of a disinhibitory VIP interneuron microcircuit. Our circuit mapping and modeling further reveal that LEC~inputs also recruit CCK interneurons that may act as strong suppressors of dendritic spikes. These results~highlight a cortically driven GABAergic microcircuit mechanism that gates nonlinear dendritic computations, which may support compartment-specific coding of multisensory contextual features within the hippocampus.},
  langid = {english},
  keywords = {circuit interactions,computational model,dendrites,entorhinal cortex,excitation-inhibition balance,functional circuit mapping,GABAergic interneurons,hippocampus,memory,optogenetics},
  file = {/Users/daniekru/Zotero/storage/7XBEQYD8/Bilash et al. - 2023 - Lateral entorhinal cortex inputs modulate hippocam.pdf;/Users/daniekru/Zotero/storage/ZIY59F62/S2211124722018666.html}
}

@misc{binzModelingHumanExploration2022,
  title = {Modeling {{Human Exploration Through Resource-Rational Reinforcement Learning}}},
  author = {Binz, Marcel and Schulz, Eric},
  year = {2022},
  month = nov,
  number = {arXiv:2201.11817},
  eprint = {2201.11817},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-03},
  abstract = {Equipping artificial agents with useful exploration mechanisms remains a challenge to this day. Humans, on the other hand, seem to manage the trade-off between exploration and exploitation effortlessly. In the present article, we put forward the hypothesis that they accomplish this by making optimal use of limited computational resources. We study this hypothesis by meta-learning reinforcement learning algorithms that sacrifice performance for a shorter description length (defined as the number of bits required to implement the given algorithm). The emerging class of models captures human exploration behavior better than previously considered approaches, such as Boltzmann exploration, upper confidence bound algorithms, and Thompson sampling. We additionally demonstrate that changing the description length in our class of models produces the intended effects: reducing description length captures the behavior of brain-lesioned patients while increasing it mirrors cognitive development during adolescence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/RIP3QH3R/Binz and Schulz - 2022 - Modeling Human Exploration Through Resource-Ration.pdf}
}

@article{bittnerBehavioralTimeScale2017,
  title = {Behavioral Time Scale Synaptic Plasticity Underlies {{CA1}} Place Fields},
  author = {Bittner, Katie C. and Milstein, Aaron D. and Grienberger, Christine and Romani, Sandro and Magee, Jeffrey C.},
  year = {2017},
  month = sep,
  journal = {Science},
  volume = {357},
  number = {6355},
  pages = {1033--1036},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aan3846},
  urldate = {2023-02-07},
  abstract = {A different form of synaptic plasticity                            How do synaptic or other neuronal changes support learning? This subject has been dominated by Hebb's postulate of synaptic change. Although there is strong experimental support for Hebbian plasticity in a number of preparations, alternative ideas have also been developed over the years. Bittner               et al.               provide in vivo, in vitro, and modeling data to support the view that non-Hebbian plasticity may underlie the formation of hippocampal place fields (see the Perspective by Krupic). Instead of multiple pairings, a single strong Ca               2+               plateau potential in neuronal dendrites paired with spatial inputs may be sufficient to produce place cells.                                         Science               , this issue p.               1033               ; see also p.               974                        ,              A particular type of long--time scale plasticity shapes the formation of stable place fields in the hippocampus.           ,                             Learning is primarily mediated by activity-dependent modifications of synaptic strength within neuronal circuits. We discovered that place fields in hippocampal area CA1 are produced by a synaptic potentiation notably different from Hebbian plasticity. Place fields could be produced in vivo in a single trial by potentiation of input that arrived seconds before and after complex spiking. The potentiated synaptic input was not initially coincident with action potentials or depolarization. This rule, named behavioral time scale synaptic plasticity, abruptly modifies inputs that were neither causal nor close in time to postsynaptic activation. In slices, five pairings of subthreshold presynaptic activity and calcium (Ca               2+               ) plateau potentials produced a large potentiation with an asymmetric seconds-long time course. This plasticity efficiently stores entire behavioral sequences within synaptic weights to produce predictive place cell activity.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/6ZTU53X5/Bittner et al. - 2017 - Behavioral time scale synaptic plasticity underlie.pdf}
}

@article{blackmanTargetcellspecificShorttermPlasticity2013,
  title = {Target-Cell-Specific Short-Term Plasticity in Local Circuits},
  author = {Blackman, Arne V. and Abrahamsson, Therese and Costa, Rui Ponte and Lalanne, Txomin and Sj{\"o}str{\"o}m, P. Jesper},
  year = {2013},
  month = dec,
  journal = {Frontiers in Synaptic Neuroscience},
  volume = {5},
  pages = {11},
  issn = {1663-3563},
  doi = {10.3389/fnsyn.2013.00011},
  abstract = {Short-term plasticity (STP) denotes changes in synaptic strength that last up to tens of seconds. It is generally thought that STP impacts information transfer across synaptic connections and may thereby provide neurons with, for example, the ability to detect input coherence, to maintain stability and to promote synchronization. STP is due to a combination of mechanisms, including vesicle depletion and calcium accumulation in synaptic terminals. Different forms of STP exist, depending on many factors, including synapse type. Recent evidence shows that synapse dependence holds true even for connections that originate from a single presynaptic cell, which implies that postsynaptic target cell type can determine synaptic short-term dynamics. This arrangement is surprising, since STP itself is chiefly due to presynaptic mechanisms. Target-specific synaptic dynamics in addition imply that STP is not a bug resulting from synapses fatiguing when driven too hard, but rather a feature that is selectively implemented in the brain for specific functional purposes. As an example, target-specific STP results in sequential somatic and dendritic inhibition in neocortical and hippocampal excitatory cells during high-frequency firing. Recent studies also show that the Elfn1 gene specifically controls STP at some synapse types. In addition, presynaptic NMDA receptors have been implicated in synapse-specific control of synaptic dynamics during high-frequency activity. We argue that synapse-specific STP deserves considerable further study, both experimentally and theoretically, since its function is not well known. We propose that synapse-specific STP has to be understood in the context of the local circuit, which requires combining different scientific disciplines ranging from molecular biology through electrophysiology to computer modeling.},
  langid = {english},
  pmcid = {PMC3854841},
  pmid = {24367330},
  keywords = {development,network models,short-term plasticity,synapse formation,synapse specificity,synaptic disease},
  file = {/Users/daniekru/Zotero/storage/R3YRB8CR/Blackman et al. - 2013 - Target-cell-specific short-term plasticity in local circuits.pdf}
}

@article{boerlinPredictiveCodingDynamical2013,
  title = {Predictive {{Coding}} of {{Dynamical Variables}} in {{Balanced Spiking Networks}}},
  author = {Boerlin, Martin and Machens, Christian K. and Den{\`e}ve, Sophie},
  year = {2013},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {11},
  pages = {e1003258},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003258},
  urldate = {2024-05-13},
  abstract = {Two observations about the cortex have puzzled neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks that represent information efficiently in their spikes. We illustrate this insight with spiking networks that represent dynamical variables. Our approach is based on two assumptions: We assume that information about dynamical variables can be read out linearly from neural spike trains, and we assume that neurons only fire a spike if that improves the representation of the dynamical variables. Based on these assumptions, we derive a network of leaky integrate-and-fire neurons that is able to implement arbitrary linear dynamical systems. We show that the membrane voltage of the neurons is equivalent to a prediction error about a common population-level signal. Among other things, our approach allows us to construct an integrator network of spiking neurons that is robust against many perturbations. Most importantly, neural variability in our networks cannot be equated to noise. Despite exhibiting the same single unit properties as widely used population code models (e.g. tuning curves, Poisson distributed spike trains), balanced networks are orders of magnitudes more reliable. Our approach suggests that spikes do matter when considering how the brain computes, and that the reliability of cortical representations could have been strongly underestimated.},
  langid = {english},
  keywords = {Action potentials,Dynamical systems,Membrane potential,Network analysis,Neural networks,Neuronal tuning,Neurons,Sensory perception},
  file = {/Users/daniekru/Zotero/storage/9FYLREQT/Boerlin et al. - 2013 - Predictive Coding of Dynamical Variables in Balanc.pdf}
}

@article{bonoLearningPredictiveCognitive2023,
  title = {Learning Predictive Cognitive Maps with Spiking Neurons during Behavior and Replays},
  author = {Bono, Jacopo and Zannone, Sara and Pedrosa, Victor and Clopath, Claudia},
  editor = {Giocomo, Lisa M and Colgin, Laura L and Hasselmo, Michael E},
  year = {2023},
  month = mar,
  journal = {eLife},
  volume = {12},
  pages = {e80671},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.80671},
  urldate = {2023-06-19},
  abstract = {The hippocampus has been proposed to encode environments using a representation that contains predictive information about likely future states, called the successor representation. However, it is not clear how such a representation could be learned in the hippocampal circuit. Here, we propose a plasticity rule that can learn this predictive map of the environment using a spiking neural network. We connect this biologically plausible plasticity rule to reinforcement learning, mathematically and numerically showing that it implements the TD-lambda algorithm. By spanning these different levels, we show how our framework naturally encompasses behavioral activity and replays, smoothly moving from rate to temporal coding, and allows learning over behavioral timescales with a plasticity rule acting on a timescale of milliseconds. We discuss how biological parameters such as dwelling times at states, neuronal firing rates and neuromodulation relate to the delay discounting parameter of the TD algorithm, and how they influence the learned representation. We also find that, in agreement with psychological studies and contrary to reinforcement learning theory, the discount factor decreases hyperbolically with time. Finally, our framework suggests a role for replays, in both aiding learning in novel environments and finding shortcut trajectories that were not experienced during behavior, in agreement with experimental data.},
  keywords = {hippocampus,modelling,predictive,successor representation,to study},
  file = {/Users/daniekru/Zotero/storage/LFT6MGS6/Bono et al. - 2023 - Learning predictive cognitive maps with spiking ne.pdf}
}

@article{bonoLearningPredictiveCognitive2023a,
  title = {Learning Predictive Cognitive Maps with Spiking Neurons during Behavior and Replays},
  author = {Bono, Jacopo and Zannone, Sara and Pedrosa, Victor and Clopath, Claudia},
  editor = {Giocomo, Lisa M and Colgin, Laura L and Hasselmo, Michael E},
  year = {2023},
  month = mar,
  journal = {eLife},
  volume = {12},
  pages = {e80671},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.80671},
  urldate = {2024-10-11},
  abstract = {The hippocampus has been proposed to encode environments using a representation that contains predictive information about likely future states, called the successor representation. However, it is not clear how such a representation could be learned in the hippocampal circuit. Here, we propose a plasticity rule that can learn this predictive map of the environment using a spiking neural network. We connect this biologically plausible plasticity rule to reinforcement learning, mathematically and numerically showing that it implements the TD-lambda algorithm. By spanning these different levels, we show how our framework naturally encompasses behavioral activity and replays, smoothly moving from rate to temporal coding, and allows learning over behavioral timescales with a plasticity rule acting on a timescale of milliseconds. We discuss how biological parameters such as dwelling times at states, neuronal firing rates and neuromodulation relate to the delay discounting parameter of the TD algorithm, and how they influence the learned representation. We also find that, in agreement with psychological studies and contrary to reinforcement learning theory, the discount factor decreases hyperbolically with time. Finally, our framework suggests a role for replays, in both aiding learning in novel environments and finding shortcut trajectories that were not experienced during behavior, in agreement with experimental data.},
  keywords = {hippocampus,modelling,predictive,successor representation},
  file = {/Users/daniekru/Zotero/storage/MWJDWSFV/Bono et al. - 2023 - Learning predictive cognitive maps with spiking neurons during behavior and replays.pdf}
}

@article{bonoModelingSomaticDendritic2017,
  title = {Modeling Somatic and Dendritic Spike Mediated Plasticity at the Single Neuron and Network Level},
  author = {Bono, Jacopo and Clopath, Claudia},
  year = {2017},
  month = dec,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {706},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-00740-z},
  urldate = {2020-05-22},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/QJAJK7EK/Bono and Clopath - 2017 - Modeling somatic and dendritic spike mediated plas.pdf}
}

@article{bontonouUnifiedDeepLearning2019,
  title = {A {{Unified Deep Learning Formalism For Processing Graph Signals}}},
  author = {Bontonou, Myriam and Lassance, Carlos and Vialatte, Jean-Charles and Gripon, Vincent},
  year = {2019},
  month = may,
  journal = {arXiv:1905.00496 [cs, stat]},
  eprint = {1905.00496},
  primaryclass = {cs, stat},
  urldate = {2021-06-08},
  abstract = {Convolutional Neural Networks are very efficient at processing signals defined on a discrete Euclidean space (such as images). However, as they can not be used on signals defined on an arbitrary graph, other models have emerged, aiming to extend its properties. We propose to review some of the major deep learning models designed to exploit the underlying graph structure of signals. We express them in a unified formalism, giving them a new and comparative reading.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/3IX3GNF9/Bontonou et al. - 2019 - A Unified Deep Learning Formalism For Processing G.pdf;/Users/daniekru/Zotero/storage/HRKI4UJN/1905.html}
}

@article{bouchacourtFlexibleModelWorking2019,
  title = {A {{Flexible Model}} of {{Working Memory}}},
  author = {Bouchacourt, Flora and Buschman, Timothy J.},
  year = {2019},
  month = jul,
  journal = {Neuron},
  volume = {103},
  number = {1},
  pages = {147-160.e8},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.04.020},
  urldate = {2024-05-13},
  abstract = {Working memory is fundamental to cognition, allowing one to hold information ``in mind.'' A defining characteristic of working memory is its flexibility: we can hold anything in mind. However, typical models of working memory rely on finely tuned, content-specific attractors to persistently maintain neural activity and therefore do not allow for the flexibility observed in behavior. Here, we present a flexible model of working memory that maintains representations through random recurrent connections between two layers of neurons: a structured ``sensory'' layer and a randomly connected, unstructured layer. As the interactions are untuned with respect to the content being stored, the network maintains any arbitrary input. However, in our model, this flexibility comes at a cost: the random connections overlap, leading to interference between representations and limiting the memory capacity of the network. Additionally, our model captures several other key behavioral and neurophysiological characteristics of working memory.},
  keywords = {capacity limitations,cognitive control,cognitive flexibility,computational model,excitation-inhibition balance,mixed selectivity,working memory},
  file = {/Users/daniekru/Zotero/storage/WIEJTLCA/Bouchacourt and Buschman - 2019 - A Flexible Model of Working Memory.pdf;/Users/daniekru/Zotero/storage/ZHUIKJM6/S0896627319303770.html}
}

@book{braitenbergVehiclesExperimentsSynthetic1986,
  title = {Vehicles: {{Experiments}} in {{Synthetic Psychology}}},
  shorttitle = {Vehicles},
  author = {Braitenberg, Valentino},
  year = {1986},
  publisher = {MIT Press},
  abstract = {This volume describes Braitenberg vehicles. Braitenberg vehicles are conceived in a thought experiment by the author of this book to illustrate in an evolutive way the abilities of simple agents. The vehicles represent the simplest form of behavior based artificial intelligence or embodied cognition, (i.e. intelligent behavior) that emerges from sensorimotor interaction between the agent and its environment, without any need for an internal memory, representation of the environment, or inference. There are 14 vehicles in all -- a series of hypothetical, self-operating mobile machines that exhibit increasingly sophisticated behavior similar to that in the real biological or neuroscientific world. One might assemble Braitenberg's vehicles like cunning table-top toys that might look like the fantastic Albrecht drawings. Whatever their guise, they behave like living creatures, ranging from simple light-seeking automata to vehicles that an observing psychologist or philosopher might conclude were operated by concealed human beings. Each of the vehicles in the series incorporates the essential features of all the earlier models but represents an evolutionary advance to a higher plateau of complexity. Along the way, they come to embody the instincts of fight or flight, the characteristic behaviors impelled by love and by logic, manifestations of foresight, concept formation, creative thinking, personality, and free will. The author shows that these attributes and patterns of behavior can be internalized into machines using the simplest parts -- a collection of environmental sensors, some wheel-driving motors, various threshold devices, a few fictional (but logically and technologically plausible) components with special properties. The author locates many elements of his fantasy in current brain research in a concluding section of extensive biological notes.},
  googlebooks = {7KkUAT\_q\_sQC},
  isbn = {978-0-262-52112-3},
  langid = {english},
  keywords = {Psychology / Cognitive Psychology & Cognition}
}

@article{brucknerHumanAnimalWellBeing2021,
  title = {Human and {{Animal Well-Being}}},
  author = {Bruckner, Donald W.},
  year = {2021},
  journal = {Pacific Philosophical Quarterly},
  volume = {102},
  number = {3},
  pages = {393--412},
  issn = {1468-0114},
  doi = {10.1111/papq.12362},
  urldate = {2023-06-02},
  abstract = {There is almost no theoretical discussion of non-human animal well-being in the philosophical literature on well-being. To begin to rectify this, I develop a desire satisfaction theory of well-being for animals. I contrast this theory with my desire theory of well-being for humans, according to which a human benefits from satisfying desires for which she can offer reasons. I consider objections. The most important are (1) Eden Lin's claim that the correct theory of well-being cannot vary across different welfare subjects and (2) his objection against theories of human well-being that require exercising a sophisticated capacity such as reason giving.},
  copyright = {{\copyright} 2021 University of Southern California and John Wiley \& Sons Ltd},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZYTI9AFP/Bruckner - 2021 - Human and Animal Well-Being.pdf;/Users/daniekru/Zotero/storage/QKB8PN27/papq.html}
}

@article{brucknerPhilosophyAnimalWelfare2019,
  title = {Philosophy and Animal Welfare Science},
  author = {Bruckner, Donald W.},
  year = {2019},
  journal = {Philosophy Compass},
  volume = {14},
  number = {10},
  pages = {e12626},
  issn = {1747-9991},
  doi = {10.1111/phc3.12626},
  urldate = {2023-06-02},
  abstract = {Although human well-being is a topic of much contemporary philosophical discussion, there has been comparatively little theoretical discussion in philosophy of (nonhuman) animal well-being. Animal welfare science is a well-established scientific discipline that studies animal well-being from an empirical standpoint. This article examines parts of this literature that may be relevant to philosophical treatments of animal well-being and to other philosophical issues. First, I explain the dominant conceptions of well-being in animal welfare science and survey some debates in that literature surrounding these competing conceptions. Second, I explain the empirical methods used to measure animal well-being. Third, I argue for the philosophical relevance of the research in animal welfare science and outline some philosophical issues concerning animal welfare and its science that I claim are worthy of attention by philosophers.},
  copyright = {{\copyright} 2019 The Author(s) Philosophy Compass {\copyright} 2019 John Wiley \& Sons Ltd},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/5XHLKMCP/Bruckner - 2019 - Philosophy and animal welfare science.pdf;/Users/daniekru/Zotero/storage/G2TXTLK3/phc3.html}
}

@article{brunelEffectsNeuromodulationCortical2001,
  title = {Effects of {{Neuromodulation}} in a {{Cortical Network Model}} of {{Object Working Memory Dominated}} by {{Recurrent Inhibition}}},
  author = {Brunel, Nicolas and Wang, Xiao-Jing},
  year = {2001},
  month = jul,
  journal = {Journal of Computational Neuroscience},
  volume = {11},
  number = {1},
  pages = {63--85},
  issn = {1573-6873},
  doi = {10.1023/A:1011204814320},
  urldate = {2024-05-13},
  abstract = {Experimental evidence suggests that the maintenance of an item in working memory is achieved through persistent activity in selective neural assemblies of the cortex. To understand the mechanisms underlying this phenomenon, it is essential to investigate how persistent activity is affected by external inputs or neuromodulation. We have addressed these questions using a recurrent network model of object working memory. Recurrence is dominated by inhibition, although persistent activity is generated through recurrent excitation in small subsets of excitatory neurons.},
  langid = {english},
  keywords = {AMPA,dopamine,GABA,inferotemporal cortex,network model,NMDA,persistent activity,prefrontal cortex,spontaneous activity,working memory},
  file = {/Users/daniekru/Zotero/storage/KSR8CNL3/Brunel and Wang - 2001 - Effects of Neuromodulation in a Cortical Network M.pdf}
}

@article{brzoskoNeuromodulationSpikeTimingDependentPlasticity2019,
  title = {Neuromodulation of {{Spike-Timing-Dependent Plasticity}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Neuromodulation of {{Spike-Timing-Dependent Plasticity}}},
  author = {Brzosko, Zuzanna and Mierau, Susanna B. and Paulsen, Ole},
  year = {2019},
  month = aug,
  journal = {Neuron},
  volume = {103},
  number = {4},
  pages = {563--581},
  issn = {1097-4199},
  doi = {10.1016/j.neuron.2019.05.041},
  abstract = {Spike-timing-dependent synaptic plasticity (STDP) is a leading cellular model for behavioral learning and memory with rich computational properties. However, the relationship between the millisecond-precision spike timing required for STDP and the much slower timescales of behavioral learning is not well understood. Neuromodulation offers an attractive mechanism to connect these different timescales, and there is now strong experimental evidence that STDP is under neuromodulatory control by acetylcholine, monoamines, and other signaling molecules. Here, we review neuromodulation of STDP, the underlying mechanisms, functional implications, and possible involvement in brain disorders.},
  langid = {english},
  pmid = {31437453},
  keywords = {Action Potentials,Animals,Astrocytes,attention,Behavior,Brain Diseases,Brain Mapping,disease,Humans,Learning,long-term depression,long-term potentiation,memory,Memory Consolidation,Models Neurological,Molecular Targeted Therapy,Neurodegenerative Diseases,neurodevelopment,Neurodevelopmental Disorders,neuromodulation,Neuronal Plasticity,Neurons,Neurotransmitter Agents,Obsessive-Compulsive Disorder,Presynaptic Terminals,Receptors Neurotransmitter,reinforcement learning,Reinforcement Psychology,Signal Transduction,sleep,Species Specificity,spike-timing-dependent plasticity,STDP,Stroke,Substance-Related Disorders,synaptic plasticity,Time Factors,to study},
  file = {/Users/daniekru/Zotero/storage/4VK48ZJ8/Brzosko et al. - 2019 - Neuromodulation of Spike-Timing-Dependent Plastici.pdf}
}

@article{brzoskoRetroactiveModulationSpike2015,
  title = {Retroactive Modulation of Spike Timing-Dependent Plasticity by Dopamine},
  author = {Brzosko, Zuzanna and Schultz, Wolfram and Paulsen, Ole},
  editor = {Bartos, Marlene},
  year = {2015},
  month = oct,
  journal = {eLife},
  volume = {4},
  pages = {e09685},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.09685},
  urldate = {2023-05-16},
  abstract = {Most reinforcement learning models assume that the reward signal arrives after the activity that led to the reward, placing constraints on the possible underlying cellular mechanisms. Here we show that dopamine, a positive reinforcement signal, can retroactively convert hippocampal timing-dependent synaptic depression into potentiation. This effect requires functional NMDA receptors and is mediated in part through the activation of the cAMP/PKA cascade. Collectively, our results support the idea that reward-related signaling can act on a pre-established synaptic eligibility trace, thereby associating specific experiences with behaviorally distant, rewarding outcomes. This finding identifies a biologically plausible mechanism for solving the `distal reward problem'.},
  keywords = {dopamine,reward,synaptic plasticity,to study},
  file = {/Users/daniekru/Zotero/storage/PEDMCFDR/Brzosko et al. - 2015 - Retroactive modulation of spike timing-dependent p.pdf}
}

@article{brzoskoSequentialNeuromodulationHebbian2017,
  title = {Sequential Neuromodulation of {{Hebbian}} Plasticity Offers Mechanism for Effective Reward-Based Navigation},
  author = {Brzosko, Zuzanna and Zannone, Sara and Schultz, Wolfram and Clopath, Claudia and Paulsen, Ole},
  year = {2017},
  journal = {eLife},
  volume = {6},
  pages = {e27756},
  issn = {2050-084X},
  doi = {10.7554/eLife.27756},
  urldate = {2023-05-10},
  abstract = {Spike timing-dependent plasticity (STDP) is under neuromodulatory control, which is correlated with distinct behavioral states. Previously, we reported that dopamine, a reward signal, broadens the time window for synaptic potentiation and modulates the outcome of hippocampal STDP even when applied after the plasticity induction protocol (Brzosko et al., 2015). Here, we demonstrate that sequential neuromodulation of STDP by acetylcholine and dopamine offers an efficacious model of reward-based navigation. Specifically, our experimental data in mouse hippocampal slices show that acetylcholine biases STDP toward synaptic depression, whilst subsequent application of dopamine converts this depression into potentiation. Incorporating this bidirectional neuromodulation-enabled correlational synaptic learning rule into a computational model yields effective navigation toward changing reward locations, as in natural foraging behavior. Thus, temporally sequenced neuromodulation of STDP enables associations to be made between actions and outcomes and also provides a possible mechanism for aligning the time scales of cellular and behavioral learning., DOI: http://dx.doi.org/10.7554/eLife.27756.001},
  pmcid = {PMC5546805},
  pmid = {28691903},
  file = {/Users/daniekru/Zotero/storage/7LHY67K9/Brzosko et al. - Sequential neuromodulation of Hebbian plasticity o.pdf}
}

@article{burakAccuratePathIntegration2009,
  title = {Accurate {{Path Integration}} in {{Continuous Attractor Network Models}} of {{Grid Cells}}},
  author = {Burak, Yoram and Fiete, Ila R.},
  year = {2009},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {5},
  number = {2},
  pages = {e1000291},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000291},
  urldate = {2023-11-09},
  abstract = {Grid cells in the rat entorhinal cortex display strikingly regular firing responses to the animal's position in 2-D space and have been hypothesized to form the neural substrate for dead-reckoning. However, errors accumulate rapidly when velocity inputs are integrated in existing models of grid cell activity. To produce grid-cell-like responses, these models would require frequent resets triggered by external sensory cues. Such inadequacies, shared by various models, cast doubt on the dead-reckoning potential of the grid cell system. Here we focus on the question of accurate path integration, specifically in continuous attractor models of grid cell activity. We show, in contrast to previous models, that continuous attractor models can generate regular triangular grid responses, based on inputs that encode only the rat's velocity and heading direction. We consider the role of the network boundary in the integration performance of the network and show that both periodic and aperiodic networks are capable of accurate path integration, despite important differences in their attractor manifolds. We quantify the rate at which errors in the velocity integration accumulate as a function of network size and intrinsic noise within the network. With a plausible range of parameters and the inclusion of spike variability, our model networks can accurately integrate velocity inputs over a maximum of {$\sim$}10--100 meters and {$\sim$}1--10 minutes. These findings form a proof-of-concept that continuous attractor dynamics may underlie velocity integration in the dorsolateral medial entorhinal cortex. The simulations also generate pertinent upper bounds on the accuracy of integration that may be achieved by continuous attractor dynamics in the grid cell network. We suggest experiments to test the continuous attractor model and differentiate it from models in which single cells establish their responses independently of each other.},
  langid = {english},
  keywords = {Cognitive science,important,Network analysis,Neural networks,Neuronal tuning,Neurons,Rats,Single neuron function,to study,Velocity},
  file = {/Users/daniekru/Zotero/storage/XLM2L3I8/Burak and Fiete - 2009 - Accurate Path Integration in Continuous Attractor .pdf}
}

@article{burkhardtLargescaleNeurocomputationalModel2023,
  title = {A Large-Scale Neurocomputational Model of Spatial Cognition Integrating Memory with Vision},
  author = {Burkhardt, Micha and Bergelt, Julia and G{\"o}nner, Lorenz and Dinkelbach, Helge {\"U}lo and Beuth, Frederik and Schwarz, Alex and Bicanski, Andrej and Burgess, Neil and Hamker, Fred H.},
  year = {2023},
  month = oct,
  journal = {Neural Networks},
  volume = {167},
  pages = {473--488},
  issn = {08936080},
  doi = {10.1016/j.neunet.2023.08.034},
  urldate = {2024-03-08},
  abstract = {We introduce a large-scale neurocomputational model of spatial cognition called 'Spacecog', which integrates recent findings from mechanistic models of visual and spatial perception. As a highlevel cognitive ability, spatial cognition requires the processing of behaviourally relevant features in complex environments and, importantly, the updating of this information during processes of eye and body movement. The Spacecog model achieves this by interfacing spatial memory and imagery with mechanisms of object localisation, saccade execution, and attention through coordinate transformations in parietal areas of the brain. We evaluate the model in a realistic virtual environment where our neurocognitive model steers an agent to perform complex visuospatial tasks. Our modelling approach opens up new possibilities in the assessment of neuropsychological data and human spatial cognition.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/7Q9Y8ITU/Burkhardt et al. - 2023 - A large-scale neurocomputational model of spatial .pdf}
}

@book{burtiniImprovingOnlineMarketing2015,
  title = {￼{{Improving Online Marketing Experiments}} with {{Drifting Multi-Armed Bandits}}},
  author = {Burtini, Giuseppe and Loeppky, Jason and Lawrence, Ramon},
  year = {2015},
  month = apr,
  journal = {ICEIS 2015 - 17th International Conference on Enterprise Information Systems, Proceedings},
  volume = {1},
  publisher = {International Conference on Enterprise Information Systems},
  doi = {10.5220/0005458706300636},
  abstract = {Restless bandits model the exploration vs. exploitation trade-off in a changing (non-stationary) world. Restless bandits have been studied in both the context of continuously-changing (drifting) and change-point (sudden) restlessness. In this work, we study specific classes of drifting restless bandits selected for their relevance to modelling an online website optimization process. The contribution in this work is a simple, feasible weighted least squares technique capable of utilizing contextual arm parameters while considering the parameter space drifting non-stationary within reasonable bounds. We produce a reference implementation, then evaluate and compare its performance in several different true world states, finding experimentally that performance is robust to time drifting factors similar to those seen in many real world cases.},
  file = {/Users/daniekru/Zotero/storage/RV3P2QBX/Burtini et al. - 2015 - ￼Improving Online Marketing Experiments with Drift.pdf}
}

@article{bushWhatGridCells2014,
  title = {What Do Grid Cells Contribute to Place Cell Firing?},
  author = {Bush, Daniel and Barry, Caswell and Burgess, Neil},
  year = {2014},
  month = mar,
  journal = {Trends in Neurosciences},
  volume = {37},
  number = {3},
  pages = {136--145},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2013.12.003},
  urldate = {2023-11-03},
  abstract = {{$\bullet$}               It is commonly assumed that grid cell inputs generate hippocampal place fields, but recent empirical evidence brings this assumption into doubt.                                         {$\bullet$}               We suggest that place fields are primarily determined by environmental sensory inputs.                                         {$\bullet$}               Grid cells provide a complementary path integration input and large-scale spatial metric.                                         {$\bullet$}               Place and grid cell representations interact to support accurate coding of large-scale space.                                 , The unitary firing fields of hippocampal place cells are commonly assumed to be generated by input from entorhinal grid cell modules with differing spatial scales. Here, we review recent research that brings this assumption into doubt. Instead, we propose that place cell spatial firing patterns are determined by environmental sensory inputs, including those representing the distance and direction to environmental boundaries, while grid cells provide a complementary self-motion related input that contributes to maintaining place cell firing. In this view, grid and place cell firing patterns are not successive stages of a processing hierarchy, but complementary and interacting representations that work in combination to support the reliable coding of large-scale space.},
  pmcid = {PMC3945817},
  pmid = {24485517},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/Y8HZN2JN/Bush et al. - 2014 - What do grid cells contribute to place cell firing.pdf}
}

@article{bushWhatGridCells2014a,
  title = {What Do Grid Cells Contribute to Place Cell Firing?},
  author = {Bush, Daniel and Barry, Caswell and Burgess, Neil},
  year = {2014},
  month = mar,
  journal = {Trends in Neurosciences},
  volume = {37},
  number = {3},
  pages = {136--145},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2013.12.003},
  urldate = {2023-11-09},
  abstract = {{$\bullet$}               It is commonly assumed that grid cell inputs generate hippocampal place fields, but recent empirical evidence brings this assumption into doubt.                                         {$\bullet$}               We suggest that place fields are primarily determined by environmental sensory inputs.                                         {$\bullet$}               Grid cells provide a complementary path integration input and large-scale spatial metric.                                         {$\bullet$}               Place and grid cell representations interact to support accurate coding of large-scale space.                                 , The unitary firing fields of hippocampal place cells are commonly assumed to be generated by input from entorhinal grid cell modules with differing spatial scales. Here, we review recent research that brings this assumption into doubt. Instead, we propose that place cell spatial firing patterns are determined by environmental sensory inputs, including those representing the distance and direction to environmental boundaries, while grid cells provide a complementary self-motion related input that contributes to maintaining place cell firing. In this view, grid and place cell firing patterns are not successive stages of a processing hierarchy, but complementary and interacting representations that work in combination to support the reliable coding of large-scale space.},
  pmcid = {PMC3945817},
  pmid = {24485517},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/FBI2VFCS/Bush et al. - 2014 - What do grid cells contribute to place cell firing.pdf}
}

@article{byrneMemoryUrbanGeography1979,
  title = {Memory for {{Urban Geography}}},
  author = {Byrne, R. W.},
  year = {1979},
  month = feb,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {31},
  number = {1},
  pages = {147--154},
  publisher = {SAGE Publications},
  issn = {0033-555X},
  doi = {10.1080/14640747908400714},
  urldate = {2025-02-22},
  abstract = {Consistent patterns of errors are found in estimations of distances and angles along urban routes, even among subjects who know the areas well. These patterns can be used to discover the organization of the knowledge we use to find our way around in everyday life.In the first experiment, 80 undergraduates estimated by ratio scaling the walking distances between pairs of locations in St. Andrews. Routes varied independently in their location, number of major bends, and length. Relative overestimation of length was found with routes in the town centre, with routes having several major bends, and (perhaps as an experimental artifact) with short routes. In the second experiment, 30 Cambridge residents estimated the angles between pairs of roads, by drawing the configuration of roads at their junctions. The real angles were either in the range 60--70{$^\circ$} or 110--120{$^\circ$}. All the estimates differed little from 90{$^\circ$}, regardless of the true magnitude of the angle.The implications of these findings for theories of mental representation of largescale space are discussed. A model is supported in which a spatial area is represented as a ``network-map'', consisting of strings of locations forming a net of paths known to be traversable, but vector distance is not preserved.},
  langid = {english}
}

@article{cabessaSuperTuringComputationalPower2014,
  title = {The Super-{{Turing}} Computational Power of Plastic Recurrent Neural Networks},
  author = {Cabessa, J{\'e}r{\'e}mie and Siegelmann, Hava T.},
  year = {2014},
  month = dec,
  journal = {International Journal of Neural Systems},
  volume = {24},
  number = {08},
  pages = {1450029},
  issn = {0129-0657, 1793-6462},
  doi = {10.1142/S0129065714500294},
  urldate = {2023-10-10},
  abstract = {We study the computational capabilities of a biologically inspired neural model where the synaptic weights, the connectivity pattern, and the number of neurons can evolve over time rather than stay static. Our study focuses on the mere concept of plasticity of the model so that the nature of the updates is assumed to be not constrained. In this context, we show that the so-called plastic recurrent neural networks (RNNs) are capable of the precise super-Turing computational power --- as the static analog neural networks --- irrespective of whether their synaptic weights are modeled by rational or real numbers, and moreover, irrespective of whether their patterns of plasticity are restricted to bi-valued updates or expressed by any other more general form of updating. Consequently, the incorporation of only bi-valued plastic capabilities in a basic model of RNNs suffices to break the Turing barrier and achieve the super-Turing level of computation. The consideration of more general mechanisms of architectural plasticity or of real synaptic weights does not further increase the capabilities of the networks. These results support the claim that the general mechanism of plasticity is crucially involved in the computational and dynamical capabilities of biological neural networks. They further show that the super-Turing level of computation reflects in a suitable way the capabilities of brain-like models of computation.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/QXP55EC8/Cabessa and Siegelmann - 2014 - THE SUPER-TURING COMPUTATIONAL POWER OF PLASTIC RE.pdf}
}

@article{cabessaTuringCompleteNeural2019,
  title = {Turing Complete Neural Computation Based on Synaptic Plasticity},
  author = {Cabessa, J{\'e}r{\'e}mie},
  year = {2019},
  month = oct,
  journal = {PLoS ONE},
  volume = {14},
  number = {10},
  pages = {e0223451},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0223451},
  urldate = {2023-07-20},
  abstract = {In neural computation, the essential information is generally encoded into the neurons via their spiking configurations, activation values or (attractor) dynamics. The synapses and their associated plasticity mechanisms are, by contrast, mainly used to process this information and implement the crucial learning features. Here, we propose a novel Turing complete paradigm of neural computation where the essential information is encoded into discrete synaptic states, and the updating of this information achieved via synaptic plasticity mechanisms. More specifically, we prove that any 2-counter machine---and hence any Turing machine---can be simulated by a rational-weighted recurrent neural network employing spike-timing-dependent plasticity (STDP) rules. The computational states and counter values of the machine are encoded into discrete synaptic strengths. The transitions between those synaptic weights are then achieved via STDP. These considerations show that a Turing complete synaptic-based paradigm of neural computation is theoretically possible and potentially exploitable. They support the idea that synapses are not only crucially involved in information processing and learning features, but also in the encoding of essential information. This approach represents a paradigm shift in the field of neural computation.},
  pmcid = {PMC6795493},
  pmid = {31618230},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/QY85XDWX/Cabessa - 2019 - Turing complete neural computation based on synapt.pdf}
}

@inproceedings{cabessaTuringComputationNeural2022,
  title = {Turing {{Computation}} with {{Neural Networks Composed}} of {{Synfire Rings}}},
  booktitle = {2022 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Cabessa, J{\'e}r{\'e}mie},
  year = {2022},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN55064.2022.9892332},
  urldate = {2023-10-10},
  abstract = {Synfire rings are fundamental neural circuits capable of conveying self-sustained activities in a robust and temporally precise manner. We propose a Turing-complete paradigm for neural computation based on synfire rings. More specifically, we provide an algorithmic procedure which, for any fixed-space Turing machine, builds a corresponding Boolean neural network composed of synfire rings capable of simulating it. As a consequence, any fixed-space Turing machine with tapes of length N can be simulated in linear time by some Boolean neural network composed of O(N) rings and cells. The construction can naturally be extended to general Turing machines. Therefore, any Turing machine can be simulated in linear time by some Boolean neural network composed of infinitely many synfire rings. The linear time simulation relies on the possibility to mimic the behavior of the machines. In the long term, these results might contribute to the realization of biological neural computers.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/EDYQE8JL/Cabessa - 2022 - Turing Computation with Neural Networks Composed o.pdf;/Users/daniekru/Zotero/storage/TV3HSGFN/9892332.html}
}

@misc{caglarNetworksDecisionTrees2022,
  title = {Networks as Decision Trees},
  shorttitle = {Network as Decision Trees},
  author = {{\c C}a{\u g}lar, Aytekin},
  year = {2022},
  month = nov,
  file = {/Users/daniekru/Zotero/storage/5I2WK7HN/Ann_decision trees.pdf}
}

@article{campbellSelfmotionProcessingVisual2018,
  title = {Self-Motion Processing in Visual and Entorhinal Cortices: Inputs, Integration, and Implications for Position Coding},
  shorttitle = {Self-Motion Processing in Visual and Entorhinal Cortices},
  author = {Campbell, Malcolm G. and Giocomo, Lisa M.},
  year = {2018},
  month = oct,
  journal = {Journal of Neurophysiology},
  volume = {120},
  number = {4},
  pages = {2091--2106},
  issn = {0022-3077},
  doi = {10.1152/jn.00686.2017},
  urldate = {2025-02-18},
  abstract = {The sensory signals generated by self-motion are complex and multimodal, but the ability to integrate these signals into a unified self-motion percept to guide navigation is essential for animal survival. Here, we summarize classic and recent work on self-motion coding in the visual and entorhinal cortices of the rodent brain. We compare motion processing in rodent and primate visual cortices, highlighting the strengths of classic primate work in establishing causal links between neural activity and perception, and discuss the integration of motor and visual signals in rodent visual cortex. We then turn to the medial entorhinal cortex (MEC), where calculations using self-motion to update position estimates are thought to occur. We focus on several key sources of self-motion information to MEC: the medial septum, which provides locomotor speed information; visual cortex, whose input has been increasingly recognized as essential to both position and speed-tuned MEC cells; and the head direction system, which is a major source of directional information for self-motion estimates. These inputs create a large and diverse group of self-motion codes in MEC, and great interest remains in how these self-motion codes might be integrated by MEC grid cells to estimate position. However, which signals are used in these calculations and the mechanisms by which they are integrated remain controversial. We end by proposing future experiments that could further our understanding of the interactions between MEC cells that code for self-motion and position and clarify the relationship between the activity of these cells and spatial perception.},
  pmcid = {PMC6230811},
  pmid = {30089025},
  file = {/Users/daniekru/Zotero/storage/WAXQG44J/Campbell and Giocomo - 2018 - Self-motion processing in visual and entorhinal cortices inputs, integration, and implications for.pdf}
}

@article{carrollEncodingCertaintyBump2014,
  title = {Encoding Certainty in Bump Attractors},
  author = {Carroll, Sam and Josi{\'c}, Kre{\v s}imir and Kilpatrick, Zachary P.},
  year = {2014},
  month = aug,
  journal = {Journal of Computational Neuroscience},
  volume = {37},
  number = {1},
  pages = {29--48},
  issn = {1573-6873},
  doi = {10.1007/s10827-013-0486-0},
  urldate = {2024-05-15},
  abstract = {Persistent activity in neuronal populations has been shown to represent the spatial position of remembered stimuli. Networks that support bump attractors are often used to model such persistent activity. Such models usually exhibit translational symmetry. Thus activity bumps are neutrally stable, and perturbations in position do not decay away. We extend previous work on bump attractors by constructing model networks capable of encoding the certainty or salience of a stimulus stored in memory. Such networks support bumps that are not only neutrally stable to perturbations in position, but also perturbations in amplitude. Possible bump solutions then lie on a two-dimensional attractor, determined by a continuum of positions and amplitudes. Such an attractor requires precisely balancing the strength of recurrent synaptic connections. The amplitude of activity bumps represents certainty, and is determined by the initial input to the system. Moreover, bumps with larger amplitudes are more robust to noise, and over time provide a more faithful representation of the stored stimulus. In networks with separate excitatory and inhibitory populations, generating bumps with a continuum of possible amplitudes, requires tuning the strength of inhibition to precisely cancel background excitation.},
  langid = {english},
  keywords = {Bump attractor,Excitation-inhibition balance,Neural field,Spatial working memory},
  file = {/Users/daniekru/Zotero/storage/W494N8BD/Carroll et al. - 2014 - Encoding certainty in bump attractors.pdf}
}

@incollection{carvalhoContextMattersAdaptive2023,
  title = {Context {{Matters}}: {{Adaptive Mutation}} for {{Grammars}}},
  shorttitle = {Context {{Matters}}},
  author = {Carvalho, Pedro and M{\'e}gane, Jessica and Louren{\c c}o, Nuno and Machado, Penousal},
  year = {2023},
  volume = {13986},
  eprint = {2303.14522},
  primaryclass = {cs},
  pages = {117--132},
  doi = {10.1007/978-3-031-29573-7_8},
  urldate = {2023-09-07},
  abstract = {This work proposes Adaptive Facilitated Mutation, a self-adaptive mutation method for Structured Grammatical Evolution (SGE), biologically inspired by the theory of facilitated variation. In SGE, the genotype of individuals contains a list for each non-terminal of the grammar that defines the search space. In our proposed mutation, each individual contains an array with a different, self-adaptive mutation rate for each non-terminal. We also propose Function Grouped Grammars, a grammar design procedure, to enhance the benefits of the proposed mutation. Experiments were conducted on three symbolic regression benchmarks using Probabilistic Structured Grammatical Evolution (PSGE), a variant of SGE. Results show our approach is similar or better when compared with the standard grammar and mutation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,to study},
  file = {/Users/daniekru/Zotero/storage/CW2LZGHM/Carvalho et al. - 2023 - Context Matters Adaptive Mutation for Grammars.pdf;/Users/daniekru/Zotero/storage/VH8TQLJH/2303.html}
}

@article{cavenaghiNonStationaryMultiArmed2021,
  title = {Non {{Stationary Multi-Armed Bandit}}: {{Empirical Evaluation}} of a {{New Concept Drift-Aware Algorithm}}},
  shorttitle = {Non {{Stationary Multi-Armed Bandit}}},
  author = {Cavenaghi, Emanuele and Sottocornola, Gabriele and Stella, Fabio and Zanker, Markus},
  year = {2021},
  month = mar,
  journal = {Entropy},
  volume = {23},
  number = {3},
  pages = {380},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23030380},
  urldate = {2024-03-25},
  abstract = {The Multi-Armed Bandit (MAB) problem has been extensively studied in order to address real-world challenges related to sequential decision making. In this setting, an agent selects the best action to be performed at time-step t, based on the past rewards received by the environment. This formulation implicitly assumes that the expected payoff for each action is kept stationary by the environment through time. Nevertheless, in many real-world applications this assumption does not hold and the agent has to face a non-stationary environment, that is, with a changing reward distribution. Thus, we present a new MAB algorithm, named f-Discounted-Sliding-Window Thompson Sampling (f-dsw TS), for non-stationary environments, that is, when the data streaming is affected by concept drift. The f-dsw TS algorithm is based on Thompson Sampling (TS) and exploits a discount factor on the reward history and an arm-related sliding window to contrast concept drift in non-stationary environments. We investigate how to combine these two sources of information, namely the discount factor and the sliding window, by means of an aggregation function f(.). In particular, we proposed a pessimistic (f=min), an optimistic (f=max), as well as an averaged (f=mean) version of the f-dsw TS algorithm. A rich set of numerical experiments is performed to evaluate the f-dsw TS algorithm compared to both stationary and non-stationary state-of-the-art TS baselines. We exploited synthetic environments (both randomly-generated and controlled) to test the MAB algorithms under different types of drift, that is, sudden/abrupt, incremental, gradual and increasing/decreasing drift. Furthermore, we adapt four real-world active learning tasks to our framework---a prediction task on crimes in the city of Baltimore, a classification task on insects species, a recommendation task on local web-news, and a time-series analysis on microbial organisms in the tropical air ecosystem. The f-dsw TS approach emerges as the best performing MAB algorithm. At least one of the versions of f-dsw TS performs better than the baselines in synthetic environments, proving the robustness of f-dsw TS under different concept drift types. Moreover, the pessimistic version (f=min) results as the most effective in all real-world tasks.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {concept drift,machine learning,multi-armed bandit,non-stationary multi-armed bandit,Thompson Sampling,time-series analysis},
  file = {/Users/daniekru/Zotero/storage/IE86HVJ3/Cavenaghi et al. - 2021 - Non Stationary Multi-Armed Bandit Empirical Evalu.pdf}
}

@article{chandraEpisodicAssociativeMemory2025,
  title = {Episodic and Associative Memory from Spatial Scaffolds in the Hippocampus},
  author = {Chandra, Sarthak and Sharma, Sugandha and Chaudhuri, Rishidev and Fiete, Ila},
  year = {2025},
  month = jan,
  journal = {Nature},
  pages = {1--13},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-08392-y},
  urldate = {2025-01-20},
  abstract = {Hippocampal circuits in the brain enable two distinct cognitive functions: the construction of spatial maps for navigation, and the storage of sequential episodic memories1--5. Although there have been advances in modelling spatial representations in the hippocampus6--10, we lack good models of its role in episodic memory. Here we present a neocortical--entorhinal--hippocampal network model that implements a high-capacity general associative memory, spatial memory and episodic memory. By factoring content storage from the dynamics of generating error-correcting stable states, the circuit (which we call vector hippocampal scaffolded heteroassociative memory (Vector-HaSH)) avoids the memory cliff of prior memory models11,12, and instead exhibits a graceful trade-off between number of stored items and recall detail. A pre-structured internal scaffold based on grid cell states is essential for constructing even non-spatial episodic memory: it enables high-capacity sequence memorization by abstracting the chaining problem into one of learning low-dimensional transitions. Vector-HaSH reproduces several hippocampal experiments on spatial mapping and context-based representations, and provides a circuit model of the `memory palaces' used by memory athletes13. Thus, this work provides a unified understanding of the spatial mapping and associative and episodic memory roles of the hippocampus.},
  copyright = {2025 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational neuroscience,Hippocampus,Neural circuits},
  file = {/Users/daniekru/Zotero/storage/Z7EXJ5DV/Chandra et al. - 2025 - Episodic and associative memory from spatial scaffolds in the hippocampus.pdf}
}

@misc{chenReinforcementLearningGeneralization2020,
  title = {Reinforcement {{Learning Generalization}} with {{Surprise Minimization}}},
  author = {Chen, Jerry Zikun},
  year = {2020},
  month = jul,
  number = {arXiv:2004.12399},
  eprint = {2004.12399},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.12399},
  urldate = {2022-09-20},
  abstract = {Generalization remains a challenging problem for deep reinforcement learning algorithms, which are often trained and tested on the same set of deterministic game environments. When test environments are unseen and perturbed but the nature of the task remains the same, generalization gaps can arise. In this work, we propose and evaluate a surprise minimizing agent on a generalization benchmark to show an additional reward learned from a simple density model can show robustness in procedurally generated game environments that provide constant source of entropy and stochasticity.},
  archiveprefix = {arXiv},
  keywords = {AI,ML},
  file = {/Users/daniekru/Zotero/storage/592H9UER/Chen - 2020 - Reinforcement Learning Generalization with Surpris.pdf;/Users/daniekru/Zotero/storage/9NZP5XEH/2004.html}
}

@article{chenSpikingNeuralNetwork2023,
  title = {Spiking Neural Network with Working Memory Can Integrate and Rectify Spatiotemporal Features},
  author = {Chen, Yi and Liu, Hanwen and Shi, Kexin and Zhang, Malu and Qu, Hong},
  year = {2023},
  month = jun,
  journal = {Frontiers in Neuroscience},
  volume = {17},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2023.1167134},
  urldate = {2024-05-03},
  abstract = {{$<$}p{$>$}In the real world, information is often correlated with each other in the time domain. Whether it can effectively make a decision according to the global information is the key indicator of information processing ability. Due to the discrete characteristics of spike trains and unique temporal dynamics, spiking neural networks (SNNs) show great potential in applications in ultra-low-power platforms and various temporal-related real-life tasks. However, the current SNNs can only focus on the information a short time before the current moment, its sensitivity in the time domain is limited. This problem affects the processing ability of SNN in different kinds of data, including static data and time-variant data, and reduces the application scenarios and scalability of SNN. In this work, we analyze the impact of such information loss and then integrate SNN with working memory inspired by recent neuroscience research. Specifically, we propose Spiking Neural Networks with Working Memory (SNNWM) to handle input spike trains segment by segment. On the one hand, this model can effectively increase SNN's ability to obtain global information. On the other hand, it can effectively reduce the information redundancy between adjacent time steps. Then, we provide simple methods to implement the proposed network architecture from the perspectives of biological plausibility and neuromorphic hardware friendly. Finally, we test the proposed method on static and sequential data sets, and the experimental results show that the proposed model can better process the whole spike train, and achieve state-of-the-art results in short time steps. This work investigates the contribution of introducing biologically inspired mechanisms, e.g., working memory, and multiple delayed synapses to SNNs, and provides a new perspective to design future SNNs.{$<$}/p{$>$}},
  langid = {english},
  keywords = {CIFAR10,Convolutional Neural Network,Multi-dendrite,Spiking Neural network,working memory},
  file = {/Users/daniekru/Zotero/storage/F87L27R5/Chen et al. - 2023 - Spiking neural network with working memory can int.pdf}
}

@misc{chenSymbolicDiscoveryOptimization2023,
  title = {Symbolic {{Discovery}} of {{Optimization Algorithms}}},
  author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06675},
  eprint = {2302.06675},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-02-16},
  abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, \${\textbackslash}textbf\{Lion\}\$ (\${\textbackslash}textit\{Evo\${\textbackslash}textbf\{L\}\$ved S\${\textbackslash}textbf\{i\}\$gn M\${\textbackslash}textbf\{o\}\$me\${\textbackslash}textbf\{n\}\$tum\}\$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2\% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3\% \${\textbackslash}textit\{zero-shot\}\$ and 91.1\% \${\textbackslash}textit\{fine-tuning\}\$ accuracy on ImageNet, surpassing the previous best results by 2\% and 0.1\%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. The implementation of Lion is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/daniekru/Zotero/storage/7W8ZZY4W/Chen et al. - 2023 - Symbolic Discovery of Optimization Algorithms.pdf;/Users/daniekru/Zotero/storage/HXJH4R3Y/2302.html}
}

@article{chrastilCognitiveMapsCognitive2014,
  title = {From {{Cognitive Maps}} to {{Cognitive Graphs}}},
  author = {Chrastil, Elizabeth R. and Warren, William H.},
  year = {2014},
  month = nov,
  journal = {PLoS ONE},
  volume = {9},
  number = {11},
  pages = {e112544},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0112544},
  urldate = {2025-02-15},
  abstract = {We investigate the structure of spatial knowledge that spontaneously develops during free exploration of a novel environment. We present evidence that this structure is similar to a labeled graph: a network of topological connections between places, labeled with local metric information. In contrast to route knowledge, we find that the most frequent routes and detours to target locations had not been traveled during learning. Contrary to purely topological knowledge, participants typically traveled the shortest metric distance to a target, rather than topologically equivalent but longer paths. The results are consistent with the proposal that people learn a labeled graph of their environment.},
  pmcid = {PMC4229194},
  pmid = {25389769},
  file = {/Users/daniekru/Zotero/storage/J7FSHRRL/Chrastil and Warren - 2014 - From Cognitive Maps to Cognitive Graphs.pdf}
}

@article{citriSynapticPlasticityMultiple2008,
  title = {Synaptic {{Plasticity}}: {{Multiple Forms}}, {{Functions}}, and {{Mechanisms}}},
  shorttitle = {Synaptic {{Plasticity}}},
  author = {Citri, Ami and Malenka, Robert C.},
  year = {2008},
  month = jan,
  journal = {Neuropsychopharmacology},
  volume = {33},
  number = {1},
  pages = {18--41},
  publisher = {Nature Publishing Group},
  issn = {1740-634X},
  doi = {10.1038/sj.npp.1301559},
  urldate = {2024-12-09},
  abstract = {Experiences, whether they be learning in a classroom, a stressful event, or ingestion of a psychoactive substance, impact the brain by modifying the activity and organization of specific neural circuitry. A major mechanism by which the neural activity generated by an experience modifies brain function is via modifications of synaptic transmission; that is, synaptic plasticity. Here, we review current understanding of the mechanisms of the major forms of synaptic plasticity at excitatory synapses in the mammalian brain. We also provide examples of the possible developmental and behavioral functions of synaptic plasticity and how maladaptive synaptic plasticity may contribute to neuropsychiatric disorders.},
  copyright = {2008 American College of Neuropsychopharmacology},
  langid = {english},
  keywords = {Behavioral Sciences,Biological Psychology,general,Medicine/Public Health,Neurosciences,Pharmacotherapy,Psychiatry},
  file = {/Users/daniekru/Zotero/storage/QVUS3WTU/Citri and Malenka - 2008 - Synaptic Plasticity Multiple Forms, Functions, and Mechanisms.pdf}
}

@article{clairisValueConfidenceDeliberation2022,
  title = {Value, {{Confidence}}, {{Deliberation}}: {{A Functional Partition}} of the {{Medial Prefrontal Cortex Demonstrated}} across {{Rating}} and {{Choice Tasks}}},
  shorttitle = {Value, {{Confidence}}, {{Deliberation}}},
  author = {Clairis, Nicolas and Pessiglione, Mathias},
  year = {2022},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {42},
  number = {28},
  pages = {5580--5592},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1795-21.2022},
  urldate = {2024-11-27},
  abstract = {Deciding about courses of action involves minimizing costs and maximizing benefits. Decision neuroscience studies have implicated both the ventral and dorsal medial PFC (vmPFC and dmPFC) in signaling goal value and action cost, but the precise functional role of these regions is still a matter of debate. Here, we suggest a more general functional partition that applies not only to decisions but also to judgments about goal value (expected reward) and action cost (expected effort). In this conceptual framework, cognitive representations related to options (reward value and effort cost) are dissociated from metacognitive representations (confidence and deliberation) related to solving the task (providing a judgment or making a choice). We used an original approach aimed at identifying consistencies across several preference tasks, from likeability ratings to binary decisions involving both attribute integration and option comparison. fMRI results in human male and female participants confirmed the vmPFC as a generic valuation system, its activity increasing with reward value and decreasing with effort cost. In contrast, more dorsal regions were not concerned with the valuation of options but with metacognitive variables, confidence being reflected in mPFC activity and deliberation time in dmPFC activity. Thus, there was a dissociation between the effort attached to choice options (represented in the vmPFC) and the effort invested in deliberation (represented in the dmPFC), the latter being expressed in pupil dilation. More generally, assessing commonalities across preference tasks might help in reaching a unified view of the neural mechanisms underlying the cost/benefit tradeoffs that drive human behavior. SIGNIFICANCE STATEMENT Decision neuroscience studies have implicated the medial PFC in forming the cognitive representations that drive human choice behavior. However, different studies using different tasks have suggested somewhat inconsistent links between precise computational variables and specific brain regions. Here, we use fMRI to demonstrate a robust functional partition of the medial PFC that generalizes across tasks involving an estimation of goal value and/or action cost to provide a judgment or make a choice. This general functional partition makes a critical dissociation between neural representations of decisional factors (the expected costs and benefits attached to a given option) and metacognitive estimates (confidence in the judgment or choice, and effort invested in the deliberation process).},
  chapter = {Research Articles},
  copyright = {Copyright {\copyright} 2022 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {35654606},
  keywords = {decision-making,effort,fMRI,metacognition,pupillometry,reward},
  file = {/Users/daniekru/Zotero/storage/E4JEQH55/Clairis and Pessiglione - 2022 - Value, Confidence, Deliberation A Functional Partition of the Medial Prefrontal Cortex Demonstrated.pdf}
}

@article{cleggSequenceLearning1998,
  title = {Sequence Learning},
  author = {Clegg, Benjamin A and DiGirolamo, Gregory J and Keele, Steven W},
  year = {1998},
  month = aug,
  journal = {Trends in Cognitive Sciences},
  volume = {2},
  number = {8},
  pages = {275--281},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(98)01202-9},
  urldate = {2023-01-23},
  abstract = {The ability to sequence information is fundamental to human performance. When subjects are asked to respond to one of several possible spatial locations of a stimulus, reaction times and error rates decrease when the target follows a sequence. In this article, we review the numerous theoretical and methodological perspectives that have been used to study sequence learning. The opportunity now exists to integrate evidence from different domains of cognitive science to begin to provide a comprehensive account of sequence learning. We suggest that subjects can learn sequences based on different information in a hierarchical representation, including either sequences of stimuli or sequences of responses. This learning can occur both with and without explicit awareness of the sequence. Multiple modes of learning exist and are subserved by different neural circuits.},
  langid = {english},
  keywords = {explicit knowledge,hierarchy,sequence learning,skill acquisition,to study},
  file = {/Users/daniekru/Zotero/storage/KPESTFZV/Clegg et al. - 1998 - Sequence learning.pdf;/Users/daniekru/Zotero/storage/FKJHSCYK/S1364661398012029.html}
}

@article{cohenShouldStayShould2007,
  title = {Should {{I}} Stay or Should {{I}} Go? {{How}} the Human Brain Manages the Trade-off between Exploitation and Exploration [{{Proceedings}} Paper]},
  shorttitle = {Should {{I}} Stay or Should {{I}} Go?},
  author = {Cohen, Jonathan and McClure, Samuel M. and Yu, Angela},
  year = {2007},
  month = mar,
  journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
  volume = {362},
  pages = {933--42},
  doi = {10.1098/rstb.2007.2098},
  abstract = {Many large and small decisions we make in our daily lives-which ice cream to choose, what research projects to pursue, which partner to marry-require an exploration of alternatives before committing to and exploiting the benefits of a particular choice. Furthermore, many decisions require re-evaluation, and further exploration of alternatives, in the face of changing needs or circumstances. That is, often our decisions depend on a higher level choice: whether to exploit well known but possibly suboptimal alternatives or to explore risky but potentially more profitable ones. How adaptive agents choose between exploitation and exploration remains an important and open question that has received relatively limited attention in the behavioural and brain sciences. The choice could depend on a number of factors, including the familiarity of the environment, how quickly the environment is likely to change and the relative value of exploiting known sources of reward versus the cost of reducing uncertainty through exploration. There is no known generally optimal solution to the exploration versus exploitation problem, and a solution to the general case may indeed not be possible. However, there have been formal analyses of the optimal policy under constrained circumstances. There have also been specific suggestions of how humans and animals may respond to this problem under particular experimental conditions as well as proposals about the brain mechanisms involved. Here, we provide a brief review of this work, discuss how exploration and exploitation may be mediated in the brain and highlight some promising future directions for research.},
  file = {/Users/daniekru/Zotero/storage/RYJ3REI3/Cohen et al. - 2007 - Should I stay or should I go How the human brain .pdf}
}

@article{cohenTemporalDynamicsBrain1997,
  title = {Temporal Dynamics of Brain Activation during a Working Memory Task},
  author = {Cohen, Jonathan D. and Perlstein, William M. and Braver, Todd S. and Nystrom, Leigh E. and Noll, Douglas C. and Jonides, John and Smith, Edward E.},
  year = {1997},
  month = apr,
  journal = {Nature},
  volume = {386},
  number = {6625},
  pages = {604--608},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/386604a0},
  urldate = {2024-05-11},
  abstract = {Working memory is responsible for the short-term storage and online manipulation of information necessary for higher cognitive functions, such as language, planning and problem-solving1,2. Traditionally, working memory has been divided into two types of processes: executive control (governing the encoding manipulation and retrieval of information in working memory) and active maintenance (keeping information available 'online'). It has also been proposed that these two types of processes may be subserved by distinct cortical structures, with the prefrontal cortex housing the executive control processes, and more posterior regions housing the content-specific buffers (for example verbal versus visuospatial) responsible for active maintenance3,4. However, studies in non-human primates suggest that dorsolateral regions of the prefrontal cortex may also be involved in active maintenance5--8. We have used functional magnetic resonance imaging to examine brain activation in human subjects during performance of a working memory task. We used the temporal resolution of this technique to examine the dynamics of regional activation, and to show that prefrontal cortex along with parietal cortex appears to play a role in active maintenance.},
  copyright = {1997 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/daniekru/Zotero/storage/SWCQXSZ6/Cohen et al. - 1997 - Temporal dynamics of brain activation during a wor.pdf}
}

@article{colginRhythmsHippocampalNetwork2016,
  title = {Rhythms of the Hippocampal Network},
  author = {Colgin, Laura Lee},
  year = {2016},
  month = apr,
  journal = {Nature Reviews Neuroscience},
  volume = {17},
  number = {4},
  pages = {239--249},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn.2016.21},
  urldate = {2020-10-30},
  abstract = {The hippocampus shows three main classes of rhythms: theta ({$\sim$}4--12 Hz), sharp wave--ripples ({$\sim$}150--200 Hz ripples superimposed on {$\sim$}0.01--3 Hz sharp waves) and gamma ({$\sim$}25--100 Hz).Theta rhythm generation involves a variety of mechanisms, including theta rhythmic firing in septal and hippocampal interneurons, excitatory inputs to hippocampus and intrinsic properties of hippocampal neurons.Theta rhythms are likely to be important for the formation of memories of sequences of events.Sharp wave--ripple complexes are composed of two distinct network patterns: sharp waves (excitatory events that propagate from CA3 to CA1) and ripples (which reflect high frequency firing in hippocampal interneurons).Accumulating evidence suggests that sharp wave--ripples are important for intrinsic hippocampal operations, including offline memory processing, retrieval of previously stored memories and planning of future behaviours.The class of brain rhythms traditionally defined as gamma probably contains at least two different variants of oscillatory activity.Recent findings suggest that slow ({$\sim$}25--55 Hz) and fast ({$\sim$}60--100 Hz) variants of gamma have different origins and may have different functions.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/KIGM89QD/Colgin - 2016 - Rhythms of the hippocampal network.pdf;/Users/daniekru/Zotero/storage/4BE8PJA6/nrn.2016.html}
}

@article{collinsOpponentActorLearning2014,
  title = {Opponent Actor Learning ({{OpAL}}): {{Modeling}} Interactive Effects of Striatal Dopamine on Reinforcement Learning and Choice Incentive.},
  shorttitle = {Opponent Actor Learning ({{OpAL}})},
  author = {Collins, Anne G. E.},
  year = {2014},
  journal = {Psychological Review},
  volume = {121},
  number = {3},
  pages = {337},
  publisher = {US: American Psychological Association},
  issn = {1939-1471},
  doi = {10.1037/a0037015},
  urldate = {2022-09-29},
  abstract = {APA PsycNet FullTextHTML page},
  langid = {english},
  keywords = {dopamine},
  file = {/Users/daniekru/Zotero/storage/QNI48837/Collins - Opponent actor learning (OpAL) Modeling interacti.pdf;/Users/daniekru/Zotero/storage/ZG4GNCAE/2014-31650-003.html}
}

@misc{ComplementaryLearningSystems,
  title = {Complementary Learning Systems within the Hippocampus: A Neural Network Modelling Approach to Reconciling Episodic Memory with Statistical Learning},
  shorttitle = {Complementary Learning Systems within the Hippocampus},
  doi = {10.1098/rstb.2016.0049},
  urldate = {2024-03-14},
  howpublished = {https://royalsocietypublishing.org/doi/epdf/10.1098/rstb.2016.0049},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/HMSB2Q25/Complementary learning systems within the hippocam.pdf;/Users/daniekru/Zotero/storage/D4BX3QKW/rstb.2016.html}
}

@article{constantinescuOrganizingConceptualKnowledge2016,
  title = {Organizing Conceptual Knowledge in Humans with a Gridlike Code},
  author = {Constantinescu, Alexandra O. and O'Reilly, Jill X. and Behrens, Timothy E. J.},
  year = {2016},
  month = jun,
  journal = {Science},
  volume = {352},
  number = {6292},
  pages = {1464--1468},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaf0941},
  urldate = {2023-09-24},
  abstract = {It has been hypothesized that the brain organizes concepts into a mental map, allowing conceptual relationships to be navigated in a manner similar to that of space. Grid cells use a hexagonally symmetric code to organize spatial representations and are the likely source of a precise hexagonal symmetry in the functional magnetic resonance imaging signal. Humans navigating conceptual two-dimensional knowledge showed the same hexagonal signal in a set of brain regions markedly similar to those activated during spatial navigation. This gridlike signal is consistent across sessions acquired within an hour and more than a week apart. Our findings suggest that global relational codes may be used to organize nonspatial conceptual representations and that these codes may have a hexagonal gridlike pattern when conceptual knowledge is laid out in two continuous dimensions.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/QMX952IU/Constantinescu et al. - 2016 - Organizing conceptual knowledge in humans with a g.pdf}
}

@article{constantinidisPersistentSpikingActivity2018,
  title = {Persistent {{Spiking Activity Underlies Working Memory}}},
  author = {Constantinidis, Christos and Funahashi, Shintaro and Lee, Daeyeol and Murray, John D. and Qi, Xue-Lian and Wang, Min and Arnsten, Amy F. T.},
  year = {2018},
  month = aug,
  journal = {Journal of Neuroscience},
  volume = {38},
  number = {32},
  pages = {7020--7028},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2486-17.2018},
  urldate = {2024-05-13},
  abstract = {Persistent activity generated in the PFC during the delay period of working memory tasks represents information about stimuli held in memory and determines working memory performance. Alternative models of working memory, depending on the rhythmicity of discharges or exclusively on short-term synaptic plasticity, are inconsistent with the neurophysiological data. Dual Perspectives Companion Paper:Working Memory: Delay Activity, Yes! Persistent Activity? Maybe Not, by Mikael Lundqvist, Pawel Herman, and Earl K. Miller},
  chapter = {Dual Perspectives},
  copyright = {Copyright {\copyright} 2018 the authors 0270-6474/18/387020-09\$15.00/0},
  langid = {english},
  pmid = {30089641},
  keywords = {delay period,monkey,neurophysiology,prefrontal cortex,working memory},
  file = {/Users/daniekru/Zotero/storage/KBDE6QXC/Constantinidis et al. - 2018 - Persistent Spiking Activity Underlies Working Memo.pdf}
}

@article{coolsChemistryAdaptiveMind2019,
  title = {Chemistry of the {{Adaptive Mind}}: {{Lessons}} from {{Dopamine}}},
  shorttitle = {Chemistry of the {{Adaptive Mind}}},
  author = {Cools, Roshan},
  year = {2019},
  month = oct,
  journal = {Neuron},
  volume = {104},
  number = {1},
  pages = {113--131},
  issn = {1097-4199},
  doi = {10.1016/j.neuron.2019.09.035},
  abstract = {The brain faces various computational tradeoffs, such as the stability-flexibility dilemma. The major ascending neuromodulatory systems are well suited to dynamically regulate these tradeoffs depending on changing task demands. This follows from various general principles of chemical neuromodulation, which are illustrated with evidence from pharmacological neuroimaging studies on striatal dopamine's role in output gating and cost-benefit choice of cognitive tasks. The work raises open questions, including those regarding the top-down cortical control of the midbrain dopamine system, and begins to elucidate the mechanisms underlying the variability in catecholaminergic drug effects. Such drug effects depend on the baseline state of distinct target brain regions, reflecting, in part, the systems' self-regulatory capacity to maintain equilibrium. It is hypothesized that the basal tone of different dopaminergic projection systems reflects the perceived statistics of the environment computed in frontal cortex. By normalizing dopamine levels, dopaminergic drugs might counteract the bias elicited by the perceived environment.},
  langid = {english},
  pmid = {31600509},
  keywords = {Brain,Cognition,Dopamine,Dopamine Agents,Frontal Lobe,Homeostasis,Humans,Neostriatum,Neuroimaging},
  file = {/Users/daniekru/Zotero/storage/6LJFH3CC/Cools - 2019 - Chemistry of the Adaptive Mind Lessons from Dopam.pdf}
}

@article{costaDopamineModulatesNovelty2014,
  title = {Dopamine Modulates Novelty Seeking Behavior during Decision Making},
  author = {Costa, Vincent D. and Tran, Valery L. and Turchi, Janita and Averbeck, Bruno B.},
  year = {2014},
  journal = {Behavioral Neuroscience},
  volume = {128},
  number = {5},
  pages = {556--566},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-0084},
  doi = {10.1037/a0037128},
  abstract = {Novelty seeking refers to the tendency of humans and animals to explore novel and unfamiliar stimuli and environments. The idea that dopamine modulates novelty seeking is supported by evidence that novel stimuli excite dopamine neurons and activate brain regions receiving dopaminergic input. In addition, dopamine is shown to drive exploratory behavior in novel environments. It is not clear whether dopamine promotes novelty seeking when it is framed as the decision to explore novel options versus the exploitation of familiar options. To test this hypothesis, we administered systemic injections of saline or GBR-12909, a selective dopamine transporter (DAT) inhibitor, to monkeys and assessed their novelty seeking behavior during a probabilistic decision making task. The task involved pseudorandom introductions of novel choice options. This allowed monkeys the opportunity to explore novel options or to exploit familiar options that they had already sampled. We found that DAT blockade increased the monkeys' preference for novel options. A reinforcement learning (RL) model fit to the monkeys' choice data showed that increased novelty seeking after DAT blockade was driven by an increase in the initial value the monkeys assigned to novel options. However, blocking DAT did not modulate the rate at which the monkeys learned which cues were most predictive of reward or their tendency to exploit that knowledge. These data demonstrate that dopamine enhances novelty-driven value and imply that excessive novelty seeking---characteristic of impulsivity and behavioral addictions---might be caused by increases in dopamine, stemming from less reuptake. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Animal Exploratory Behavior,Animal Foraging Behavior,Animal Learning,Curiosity,Decision Making,Dopamine,Impulsiveness,Monkeys,Reinforcement,Reuptake,Sensation Seeking,to study},
  file = {/Users/daniekru/Zotero/storage/JPM5RYUD/Costa et al. - 2014 - Dopamine modulates novelty seeking behavior during.pdf;/Users/daniekru/Zotero/storage/842PDETF/2014-23803-001.html}
}

@article{costaSubcorticalSubstratesExploreexploit2019,
  title = {Subcortical Substrates of Explore-Exploit Decisions in Primates},
  author = {Costa, Vincent D. and Mitz, Andrew R. and Averbeck, Bruno B.},
  year = {2019},
  month = aug,
  journal = {Neuron},
  volume = {103},
  number = {3},
  pages = {533-545.e5},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.05.017},
  urldate = {2024-03-23},
  abstract = {The explore-exploit dilemma refers to the challenge of deciding when to forego immediate rewards and explore new opportunities that could lead to greater rewards in the future. While motivational neural circuits facilitate reinforcement learning based on past choices and outcomes, it is unclear if they also support computations relevant for deciding when to explore. We recorded neural activity in the amygdala and ventral striatum of rhesus macaques, as they solved a task that required them to balance novelty driven exploration with exploitation of what they had already learned. Using a partially-bserved Markov decision process model to quantify explore-exploit trade-offs, we identified that the ventral striatum and amygdala differ in how they represent the immediate value of exploitative choices and the future value of exploratory choices. These findings show that subcortical motivational circuits are important in guiding exploratory decisions., How do we decide when to explore a new opportunity or stick with what we know? Costa et al. reveal that neurons in amygdala and ventral striatum, motivational centers of the brain, help to solve this complex reinforcement learning problem.},
  pmcid = {PMC6687547},
  pmid = {31196672},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/JHLXR879/Costa et al. - 2019 - Subcortical substrates of explore-exploit decision.pdf}
}

@article{cowellRoadmapUnderstandingMemory2019,
  title = {A {{Roadmap}} for {{Understanding Memory}}: {{Decomposing Cognitive Processes}} into {{Operations}} and {{Representations}}},
  shorttitle = {A {{Roadmap}} for {{Understanding Memory}}},
  author = {Cowell, Rosemary A. and Barense, Morgan D. and Sadil, Patrick S.},
  year = {2019},
  month = jul,
  journal = {eNeuro},
  volume = {6},
  number = {4},
  pages = {ENEURO.0122-19.2019},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0122-19.2019},
  urldate = {2023-01-18},
  abstract = {Thanks to patients Phineas Gage and Henry Molaison, we have long known that behavioral control depends on the frontal lobes, whereas declarative memory depends on the medial temporal lobes (MTL). For decades, cognitive functions---behavioral control, declarative memory---have served as labels for characterizing the division of labor in cortex. This approach has made enormous contributions to understanding how the brain enables the mind, providing a systems-level explanation of brain function that constrains lower-level investigations of neural mechanism. Today, the approach has evolved such that functional labels are often applied to brain networks rather than focal brain regions. Furthermore, the labels have diversified to include both broadly-defined cognitive functions (declarative memory, visual perception) and more circumscribed mental processes (recollection, familiarity, priming). We ask whether a process---a high-level mental phenomenon corresponding to an introspectively-identifiable cognitive event---is the most productive label for dissecting memory. For example, recollection conflates a neurocomputational operation (pattern completion-based retrieval) with a class of representational content (associative, high-dimensional memories). Because a full theory of memory must identify operations and representations separately, and specify how they interact, we argue that processes like recollection constitute inadequate labels for characterizing neural mechanisms. Instead, we advocate considering the component operations and representations of processes like recollection in isolation. For the organization of memory, the evidence suggests that pattern completion is recapitulated widely across the ventral visual stream and MTL, but the division of labor between sites within this pathway can be explained by representational content.},
  pmcid = {PMC6620388},
  pmid = {31189554},
  file = {/Users/daniekru/Zotero/storage/X5ALAHWQ/Cowell et al. - 2019 - A Roadmap for Understanding Memory Decomposing Co.pdf}
}

@misc{cuevaEmergenceGridlikeRepresentations2018,
  title = {Emergence of Grid-like Representations by Training Recurrent Neural Networks to Perform Spatial Localization},
  author = {Cueva, Christopher J. and Wei, Xue-Xin},
  year = {2018},
  month = mar,
  number = {arXiv:1803.07770},
  eprint = {1803.07770},
  primaryclass = {cs, q-bio, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.07770},
  urldate = {2023-09-24},
  abstract = {Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning,to study},
  file = {/Users/daniekru/Zotero/storage/L5GNIWK7/Cueva and Wei - 2018 - Emergence of grid-like representations by training.pdf;/Users/daniekru/Zotero/storage/NSHPFC6B/1803.html}
}

@article{cuiContinuousOnlineSequence2016,
  title = {Continuous {{Online Sequence Learning}} with an {{Unsupervised Neural Network Model}}},
  author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  year = {2016},
  month = nov,
  journal = {Neural Computation},
  volume = {28},
  number = {11},
  pages = {2474--2504},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00893},
  urldate = {2023-01-23},
  abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods---autoregressive integrated moving average; feedforward neural networks---time delay neural network and online sequential extreme learning machine; and recurrent neural networks---long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/E9R5XXBW/Cui et al. - 2016 - Continuous Online Sequence Learning with an Unsupe.pdf;/Users/daniekru/Zotero/storage/3TIQFX3E/Continuous-Online-Sequence-Learning-with-an.html}
}

@article{cussat-blancGeneRegulatoryNetwork2015,
  title = {Gene {{Regulatory Network Evolution Through Augmenting Topologies}}},
  author = {{Cussat-Blanc}, Sylvain and Harrington, Kyle and Pollack, Jordan},
  year = {2015},
  month = dec,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {19},
  number = {6},
  pages = {823--837},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2015.2396199},
  abstract = {Artificial gene regulatory networks (GRNs) are biologically inspired dynamical systems used to control various kinds of agents, from the cells in developmental models to embodied robot swarms. Most recent work uses a genetic algorithm (GA) or an evolution strategy in order to optimize the network for a specific task. However, the empirical performances of these algorithms are unsatisfactory. This paper presents an algorithm that primarily exploits a network distance metric, which allows genetic similarity to be used for speciation and variation of GRNs. This algorithm, inspired by the successful neuroevolution of augmenting topologies algorithm's use in evolving neural networks and compositional pattern-producing networks, is based on a specific initialization method, a crossover operator based on gene alignment, and speciation based upon GRN structures. We demonstrate the effectiveness of this new algorithm by comparing our approach both to a standard GA and to evolutionary programming on four different experiments from three distinct problem domains, where the proposed algorithm excels on all experiments.},
  keywords = {Bioinformatics,Computational modeling,Evolution,Gene Regulatory Networks,gene regulatory networks (GRNs),Genetic Algorithm,genetic algorithm (GA),Genomics,Proteins,Sensors,Sociology,speciation,Speciation},
  file = {/Users/daniekru/Zotero/storage/Z2E3C3NE/Cussat-Blanc et al. - 2015 - Gene Regulatory Network Evolution Through Augmenti.pdf;/Users/daniekru/Zotero/storage/825UA826/7018989.html}
}

@inproceedings{cussat-blancGeneticallyregulatedNeuromodulationFacilitates2015,
  title = {Genetically-Regulated {{Neuromodulation Facilitates Multi-Task Reinforcement Learning}}},
  booktitle = {Proceedings of the 2015 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {{Cussat-Blanc}, Sylvain and Harrington, Kyle},
  year = {2015},
  month = jul,
  series = {{{GECCO}} '15},
  pages = {551--558},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2739480.2754730},
  urldate = {2022-09-06},
  abstract = {In this paper, we use a gene regulatory network (GRN) to regulate a reinforcement learning controller, the State- Action-Reward-State-Action (SARSA) algorithm. The GRN serves as a neuromodulator of SARSA's learning parame- ters: learning rate, discount factor, and memory depth. We have optimized GRNs with an evolutionary algorithm to regulate these parameters on specific problems but with no knowledge of problem structure. We show that genetically- regulated neuromodulation (GRNM) performs comparably or better than SARSA with fixed parameters. We then ex- tend the GRNM SARSA algorithm to multi-task problem generalization, and show that GRNs optimized on multi- ple problem domains can generalize to previously unknown problems with no further optimization.},
  isbn = {978-1-4503-3472-3},
  keywords = {gene regulatory network,learning: parameter learning,multi-task learning,neuromodulation,parameter control,reinforcement learning},
  file = {/Users/daniekru/Zotero/storage/3ECE4KIX/Cussat-Blanc and Harrington - 2015 - Genetically-regulated Neuromodulation Facilitates .pdf}
}

@article{dabaghianGridCellsBorder,
  title = {Grid {{Cells}}, {{Border Cells}} and {{Discrete Complex Analysis}}},
  author = {Dabaghian, Yuri},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/7ZAUJSXP/Dabaghian - Grid Cells, Border Cells and Discrete Complex Analysis.pdf}
}

@book{dangEmergenceNonLinearMixed2020,
  title = {Emergence of {{Non-Linear Mixed Selectivity}} in {{Prefrontal Cortex}} after {{Training}}},
  author = {Dang, Wenhao and Jaffe, Russell and Qi, Xue-Lian and Constantinidis, Christos},
  year = {2020},
  month = aug,
  doi = {10.1101/2020.08.02.233247},
  abstract = {Neurons in the prefrontal cortex are typically activated by multiple factors when performing a cognitive task, and by different tasks altogether. The selectivity of single neurons for the same stimulus dimension often changes depending on context or task performed, a phenomenon known as nonlinear mixed selectivity. It has been hypothesized that neurons with such mixed selectivity offer a computational advantage for performing cognitive tasks due to high-dimensional neural representations. In this study, we sought to determine how nonlinear mixed selectivity is affected by training to perform a cognitive task by examining the neural responses of monkeys before and after they were trained to perform visual working memory tasks. We also compared nonlinear mixed selectivity in different sub-regions of the prefrontal cortex that play different roles in these tasks. Our findings indicate that a small population of prefrontal neurons exhibit nonlinear mixed selectivity even prior to any training to perform cognitive tasks. Learning to perform working memory tasks induces a modest increase in the proportion of neurons with both linear and non-linear mixed selectivity. However, we saw little evidence that nonlinear mixed selectivity is predictive of task performance. Our results provide insights on the representation of stimulus and task information in neuronal populations. SIGNIFICANCE STATEMENT Working memory depends on the ability of neurons to represent stimuli in their pattern of discharges when they are no longer present. How neurons represent simultaneously different types of information remains a complex computational problem. It has been hypothesized that nonlinear mixed selectivity emerges as a result of training to perform tasks that require maintenance of stimuli and task parameters in memory. We tested experimentally this hypothesis by examining neuronal responses at different areas of the prefrontal cortex, before and after training to perform cognitive tasks. We reveal the regions of the prefrontal cortex that are most responsible for different types of selectivity, as well as how these types of selectivity vary as a result of training, the context of information represented in working memory tasks, and their modulating factors. These insights are critical to formulating a practical understanding of working memory, and by extension, of memory-related disorders dependent on neural selectivity.},
  file = {/Users/daniekru/Zotero/storage/HNI3NVMZ/Dang et al. - 2020 - Emergence of Non-Linear Mixed Selectivity in Prefr.pdf}
}

@article{dannenbergModulationHippocampalCircuits2017,
  title = {Modulation of {{Hippocampal Circuits}} by {{Muscarinic}} and {{Nicotinic Receptors}}},
  author = {Dannenberg, Holger and Young, Kimberly and Hasselmo, Michael},
  year = {2017},
  journal = {Frontiers in Neural Circuits},
  volume = {11},
  issn = {1662-5110},
  urldate = {2023-05-15},
  abstract = {This article provides a review of the effects of activation of muscarinic and nicotinic receptors on the physiological properties of circuits in the hippocampal formation. Previous articles have described detailed computational hypotheses about the role of cholinergic neuromodulation in enhancing the dynamics for encoding in cortical structures and the role of reduced cholinergic modulation in allowing consolidation of previously encoded information. This article will focus on addressing the broad scope of different modulatory effects observed within hippocampal circuits, highlighting the heterogeneity of cholinergic modulation in terms of the physiological effects of activation of muscarinic and nicotinic receptors and the heterogeneity of effects on different subclasses of neurons.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/WWT4X9XA/Dannenberg et al. - 2017 - Modulation of Hippocampal Circuits by Muscarinic a.pdf}
}

@article{dardenneRolePrefrontalCortex2012,
  title = {Role of Prefrontal Cortex and the Midbrain Dopamine System in Working Memory Updating},
  author = {D'Ardenne, Kimberlee and Eshel, Neir and Luka, Joseph and Lenartowicz, Agatha and Nystrom, Leigh E. and Cohen, Jonathan D.},
  year = {2012},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {49},
  pages = {19900--19909},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1116727109},
  urldate = {2024-04-26},
  abstract = {Humans are adept at switching between goal-directed behaviors quickly and effectively. The prefrontal cortex (PFC) is thought to play a critical role by encoding, updating, and maintaining internal representations of task context in working memory. It has also been hypothesized that the encoding of context representations in PFC is regulated by phasic dopamine gating signals. Here we use multimodal methods to test these hypotheses. First we used functional MRI (fMRI) to identify regions of PFC associated with the representation of context in a working memory task. Next we used single-pulse transcranial magnetic stimulation (TMS), guided spatially by our fMRI findings and temporally by previous event-related EEG recordings, to disrupt context encoding while participants performed the same working memory task. We found that TMS pulses to the right dorsolateral PFC (DLPFC) immediately after context presentation, and well in advance of the response, adversely impacted context-dependent relative to context-independent responses. This finding causally implicates right DLPFC function in context encoding. Finally, using the same paradigm, we conducted high-resolution fMRI measurements in brainstem dopaminergic nuclei (ventral tegmental area and substantia nigra) and found phasic responses after presentation of context stimuli relative to other stimuli, consistent with the timing of a gating signal that regulates the encoding of representations in PFC. Furthermore, these responses were positively correlated with behavior, as well as with responses in the same region of right DLPFC targeted in the TMS experiment, lending support to the hypothesis that dopamine phasic signals regulate encoding, and thereby the updating, of context representations in PFC.},
  file = {/Users/daniekru/Zotero/storage/J3WVZM2K/D’Ardenne et al. - 2012 - Role of prefrontal cortex and the midbrain dopamin.pdf}
}

@article{dawCorticalSubstratesExploratory2006,
  title = {Cortical Substrates for Exploratory Decisions in Humans},
  author = {Daw, Nathaniel D. and O'Doherty, John P. and Dayan, Peter and Seymour, Ben and Dolan, Raymond J.},
  year = {2006},
  month = jun,
  journal = {Nature},
  volume = {441},
  number = {7095},
  pages = {876--879},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature04766},
  urldate = {2024-05-07},
  abstract = {Humans are remarkably curious, and that is useful in helping us to learn about new environments and possibilities. But curiosity killed the cat, they say, and it also carries with it substantial potential risks and costs for us. Statisticians, engineers and economists have long considered ways of balancing the costs and benefits of exploration. Tests involving a gambling task and an fMRI brain scanner now show that humans appear to obey similar principles when considering their options. The players had to balance the desire to select the richest option based on accumulated experience against the desire to choose a less familiar option that might have a larger payoff. The frontopolar cortex, a brain area known to be involved in cognitive control, was preferentially active during exploratory decisions. The results suggest a neurobiological account of human exploration and point to a new area for behavioural and neural investigations.},
  copyright = {2006 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/daniekru/Zotero/storage/MY468LJ7/Daw et al. - 2006 - Cortical substrates for exploratory decisions in h.pdf}
}

@incollection{dawkinsChapterTwoAnimal2015,
  title = {Chapter {{Two}} - {{Animal Welfare}} and the {{Paradox}} of {{Animal Consciousness}}},
  booktitle = {Advances in the {{Study}} of {{Behavior}}},
  author = {Dawkins, Marian},
  editor = {Naguib, Marc and Brockmann, H. Jane and Mitani, John C. and Simmons, Leigh W. and Barrett, Louise and Healy, Sue and Slater, Peter J. B.},
  year = {2015},
  month = may,
  volume = {47},
  pages = {5--38},
  publisher = {Academic Press},
  doi = {10.1016/bs.asb.2014.11.001},
  urldate = {2023-06-02},
  abstract = {Animal welfare science has a potentially paradoxical attitude to animal consciousness. On the one hand, the belief that animals are conscious is what draws people to want to study animal welfare, but on the other, consciousness remains `the hard problem' and seems currently to be beyond the usual methods of science. This article asks whether the study of animal welfare that includes `feelings' can be truly scientific by examining changing scientific attitudes to studying consciousness that have taken place over the last 50~years. Human psychologists have a similar problem in studying human consciousness and their findings provide a framework for studying feelings in nonhuman animals. Animal welfare scientists have at least four different ways of dealing with the potential paradox of animal consciousness. These are the following: (1) To argue that there are no problems and so there is no paradox (2) To admit the difficulties of studying consciousness and to settle for the next best thing---the likely (but not certain) behavioral correlates of consciousness (3) To admit the difficulties but then try to find ways of studying consciousness more directly (4) To ignore the problem altogether and concentrate on studying animal welfare in ways that are independent of assumptions about animal consciousness. I conclude that it is possible to have a science of animal welfare that avoids being paradoxical and is able to make a genuine contribution to the greatest remaining mystery in biology---why suffering, pleasure, and pain feel like anything at all.},
  langid = {english},
  keywords = {Animal consciousness,Animal minds,Pain,Sentience,Suffering,Welfare},
  file = {/Users/daniekru/Zotero/storage/UEKFQMCF/Dawkins - 2015 - Chapter Two - Animal Welfare and the Paradox of An.pdf;/Users/daniekru/Zotero/storage/75W8GJVV/S0065345414000023.html}
}

@article{dawkinsScienceAnimalSuffering2008,
  title = {The {{Science}} of {{Animal Suffering}}},
  author = {Dawkins, Marian Stamp},
  year = {2008},
  journal = {Ethology},
  volume = {114},
  number = {10},
  pages = {937--945},
  issn = {1439-0310},
  doi = {10.1111/j.1439-0310.2008.01557.x},
  urldate = {2023-05-29},
  abstract = {Can suffering in non-human animals be studied scientifically? Apart from verbal reports of subjective feelings, which are uniquely human, I argue that it is possible to study the negative emotions we refer to as suffering by the same methods we use in ourselves. In particular, by asking animals what they find positively and negatively reinforcing (what they want and do not want), we can define positive and negative emotional states. Such emotional states may or may not be accompanied by subjective feelings but fortunately it is not necessary to solve the problem of consciousness to construct a scientific study of suffering and welfare. Improvements in animal welfare can be based on the answers to two questions: Q1: Will it improve animal health? and Q2: Will it give the animals something they want? This apparently simple formulation has the advantage of capturing what most people mean by `improving welfare' and so halting a potentially dangerous split between scientific and non-scientific definitions of welfare. It can also be used to validate other controversial approaches to welfare such as naturalness, stereotypies, physiological and biochemical measures. Health and what animals want are thus not just two of many measures of welfare. They provide the definition of welfare against which others can be validated. They also tell us what research we have to do and how we can judge whether welfare of animals has been genuinely improved. What is important, however, is for this research to be done in situ so that it is directly applicable to the real world of farming, the sea or an animal's wild habitat. It is here that ethology can make major contributions.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/EFEZM8FX/Dawkins - 2008 - The Science of Animal Suffering.pdf;/Users/daniekru/Zotero/storage/NJZDFXSU/j.1439-0310.2008.01557.html}
}

@article{dawkinsUserGuideAnimal2006,
  title = {A User's Guide to Animal Welfare Science},
  author = {Dawkins, Marian Stamp},
  year = {2006},
  month = feb,
  journal = {Trends in Ecology \& Evolution},
  volume = {21},
  number = {2},
  pages = {77--82},
  issn = {0169-5347},
  doi = {10.1016/j.tree.2005.10.017},
  urldate = {2023-05-29},
  abstract = {Here, I provide a guide for those new to the burgeoning field of animal welfare science as to what this comprehensive, relatively young discipline is all about. Drawing on all branches of biology, including behavioural ecology and neuroscience, the science of animal welfare asks three big questions: Are animals conscious? How can we assess good and bad welfare in animals? How can we use science to improve animal welfare in practice? I also provide guidelines for an evidence-based approach to welfare issues for policy makers and other users of animal welfare research.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/J6TYMJCI/Dawkins - 2006 - A user's guide to animal welfare science.pdf;/Users/daniekru/Zotero/storage/PB3KN2SV/S0169534705003393.html}
}

@article{dayaApplyingNeuralNetwork2010,
  title = {Applying {{Neural Network Architecture}} for {{Inverse Kinematics Problem}} in {{Robotics}}},
  author = {Daya, Bassam and Khawandi, Shadi and Akoum, Mohamed},
  year = {2010},
  month = mar,
  journal = {Journal of Software Engineering and Applications},
  volume = {03},
  number = {03},
  pages = {230},
  publisher = {Scientific Research Publishing},
  doi = {10.4236/jsea.2010.33028},
  urldate = {2020-05-24},
  abstract = {One of the most important problems in robot kinematics and control is, finding the solution of Inverse Kinematics. Inverse kinematics computation has been one of the main problems in robotics research. As the Complexity of robot increases, obtaining the inverse kinematics is difficult and computationally expensive. Traditional methods such as geometric, iterative and algebraic are inadequate if the joint structure of the manipulator is more complex. As alternative approaches, neural networks and optimal search methods have been widely used for inverse kinematics modeling and control in robotics This paper proposes neural network architecture that consists of 6 sub-neural networks to solve the inverse kinematics problem for robotics manipulators with 2 or higher degrees of freedom. The neural networks utilized are multi-layered perceptron (MLP) with a back-propagation training algorithm. This approach will reduce the complexity of the algorithm and calculation (matrix inversion) faced when using the Inverse Geometric Models implementation (IGM) in robotics. The obtained results are presented and analyzed in order to prove the efficiency of the proposed approach.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZGYUPULN/Daya et al. - 2010 - Applying Neural Network Architecture for Inverse K.pdf;/Users/daniekru/Zotero/storage/PB3JLVPY/1514.html}
}

@article{dayanDecisionTheoryReinforcement2008,
  title = {Decision Theory, Reinforcement Learning, and the Brain},
  author = {Dayan, Peter and Daw, Nathaniel D.},
  year = {2008},
  month = dec,
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  volume = {8},
  number = {4},
  pages = {429--453},
  issn = {1531-135X},
  doi = {10.3758/CABN.8.4.429},
  urldate = {2024-05-07},
  abstract = {Decision making is a core competence for animals and humans acting and surviving in environments they only partially comprehend, gaining rewards and punishments for their troubles. Decision-theoretic concepts permeate experiments and computational models in ethology, psychology, and neuroscience. Here, we review a well-known, coherent Bayesian approach to decision making, showing how it unifies issues in Markovian decision problems, signal detection psychophysics, sequential sampling, and optimal exploration and discuss paradigmatic psychological and neural examples of each problem. We discuss computational issues concerning what subjects know about their task and how ambitious they are in seeking optimal solutions; we address algorithmic topics concerning model-based and model-free methods for making choices; and we highlight key aspects of the neural implementation of decision making.},
  langid = {english},
  keywords = {Belief State,Ective State,Lateral Intraparietal Area,Markov Decision Problem,Temporal Difference Model},
  file = {/Users/daniekru/Zotero/storage/NDKXYHS4/Dayan and Daw - 2008 - Decision theory, reinforcement learning, and the b.pdf}
}

@article{dayanTwentyFiveLessonsComputational2012,
  title = {Twenty-{{Five Lessons}} from {{Computational Neuromodulation}}},
  author = {Dayan, Peter},
  year = {2012},
  month = oct,
  journal = {Neuron},
  volume = {76},
  number = {1},
  pages = {240--256},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2012.09.027},
  urldate = {2022-10-24},
  abstract = {Neural processing faces three rather different, and perniciously tied, communication problems. First, computation is radically distributed, yet point-to-point interconnections are limited. Second, the bulk of these connections are semantically uniform, lacking differentiation at their targets that could tag particular sorts of information. Third, the brain's structure is relatively fixed, and yet different sorts of input, forms of processing, and rules for determining the output are appropriate under different, and possibly rapidly changing, conditions. Neuromodulators address these problems by their multifarious and broad distribution, by enjoying specialized receptor types in partially specific anatomical arrangements, and by their ability to mold the activity and sensitivity of neurons and the strength and plasticity of their synapses. Here, I offer a computationally focused review of algorithmic and implementational motifs associated with neuromodulators, using decision making in the face of uncertainty as a running example.},
  langid = {english},
  keywords = {insightful},
  file = {/Users/daniekru/Zotero/storage/8BRGHVI2/Dayan - 2012 - Twenty-Five Lessons from Computational Neuromodula.pdf;/Users/daniekru/Zotero/storage/YNFGW2VT/S0896627312008628.html}
}

@article{decothiPredictiveMapsRats2022,
  title = {Predictive Maps in Rats and Humans for Spatial Navigation},
  author = {{de Cothi}, William and Nyberg, Nils and Griesbauer, Eva-Maria and Ghanam{\'e}, Carole and Zisch, Fiona and Lefort, Julie M. and Fletcher, Lydia and Newton, Coco and Renaudineau, Sophie and Bendor, Daniel and Grieves, Roddy and Duvelle, {\'E}l{\'e}onore and Barry, Caswell and Spiers, Hugo J.},
  year = {2022},
  month = sep,
  journal = {Current Biology},
  volume = {32},
  number = {17},
  pages = {3676-3689.e5},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2022.06.090},
  urldate = {2025-02-16},
  abstract = {Much of our understanding of navigation comes from the study of individual species, often with specific tasks tailored to those species. Here, we provide a novel experimental and analytic framework integrating across humans, rats, and simulated reinforcement learning (RL) agents to interrogate the dynamics of behavior during spatial navigation. We developed a novel open-field navigation task (``Tartarus maze'') requiring dynamic adaptation (shortcuts and detours) to frequently changing obstructions on the path to a hidden goal. Humans and rats were remarkably similar in their trajectories. Both species showed the greatest similarity to RL agents utilizing a ``successor representation,'' which creates a predictive map. Humans also displayed trajectory features similar to model-based RL agents, which implemented an optimal tree-search planning procedure. Our results help refine models seeking to explain mammalian navigation in dynamic environments and highlight the utility of modeling the behavior of different species to uncover the shared mechanisms that support behavior.,                                        {$\bullet$}               We tested humans, rats, and RL agents on a novel modular maze                                         {$\bullet$}               Humans and rats were remarkably similar in their choice of trajectories                                         {$\bullet$}               Both species were most similar to agents utilizing a SR                                         {$\bullet$}               Humans also displayed features of model-based planning in early trials                                 , de Cothi et~al. use a novel open-field modular maze to test the spatial navigation abilities of humans and rats, comparing them to simulated reinforcement learning agents. They find that humans and rats are remarkably similar in their choice of trajectories, with both species displaying most similarity to agents utilizing a successor representation.},
  pmcid = {PMC9616735},
  pmid = {35863351},
  file = {/Users/daniekru/Zotero/storage/HXNN2AJ7/de Cothi et al. - 2022 - Predictive maps in rats and humans for spatial navigation.pdf}
}

@misc{dedieuLearningHigherorderSequential2019,
  title = {Learning Higher-Order Sequential Structure with Cloned {{HMMs}}},
  author = {Dedieu, Antoine and Gothoskar, Nishad and Swingle, Scott and Lehrach, Wolfgang and {L{\'a}zaro-Gredilla}, Miguel and George, Dileep},
  year = {2019},
  month = may,
  number = {arXiv:1905.00507},
  eprint = {1905.00507},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.00507},
  urldate = {2023-09-05},
  abstract = {Variable order sequence modeling is an important problem in artificial and natural intelligence. While overcomplete Hidden Markov Models (HMMs), in theory, have the capacity to represent long-term temporal structure, they often fail to learn and converge to local minima. We show that by constraining HMMs with a simple sparsity structure inspired by biology, we can make it learn variable order sequences efficiently. We call this model cloned HMM (CHMM) because the sparsity structure enforces that many hidden states map deterministically to the same emission state. CHMMs with over 1 billion parameters can be efficiently trained on GPUs without being severely affected by the credit diffusion problem of standard HMMs. Unlike n-grams and sequence memoizers, CHMMs can model temporal dependencies at arbitrarily long distances and recognize contexts with 'holes' in them. Compared to Recurrent Neural Networks and their Long Short-Term Memory extensions (LSTMs), CHMMs are generative models that can natively deal with uncertainty. Moreover, CHMMs return a higher-order graph that represents the temporal structure of the data which can be useful for community detection, and for building hierarchical models. Our experiments show that CHMMs can beat n-grams, sequence memoizers, and LSTMs on character-level language modeling tasks. CHMMs can be a viable alternative to these methods in some tasks that require variable order sequence modeling and the handling of uncertainty.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/4SP4KNFL/Dedieu et al. - 2019 - Learning higher-order sequential structure with cl.pdf;/Users/daniekru/Zotero/storage/N7EPCXQR/1905.html}
}

@misc{deletangNeuralNetworksChomsky2023,
  title = {Neural {{Networks}} and the {{Chomsky Hierarchy}}},
  author = {Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and {Grau-Moya}, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
  year = {2023},
  month = feb,
  number = {arXiv:2207.02098},
  eprint = {2207.02098},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-02},
  abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (20'910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory,Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/BPP2XXUB/Delétang et al. - 2023 - Neural Networks and the Chomsky Hierarchy.pdf;/Users/daniekru/Zotero/storage/8DGIKPJR/2207.html}
}

@article{delnegroRespiratoryRhythmEmergent2002,
  title = {Respiratory {{Rhythm}}: {{An Emergent Network Property}}?},
  shorttitle = {Respiratory {{Rhythm}}},
  author = {Del Negro, Christopher A and {Morgado-Valle}, Consuelo and Feldman, Jack L},
  year = {2002},
  month = may,
  journal = {Neuron},
  volume = {34},
  number = {5},
  pages = {821--830},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(02)00712-2},
  urldate = {2020-05-27},
  abstract = {We tested the hypothesis that pacemaker neurons generate breathing rhythm in mammals. We monitored respiratory-related motor nerve rhythm in neonatal rodent slice preparations. Blockade of the persistent sodium current (INaP), which was postulated to underlie voltage-dependent bursting in respiratory pacemaker neurons, with riluzole ({$\leq$}200 {$\mu$}M) did not alter the frequency of respiratory-related motor output. Yet, in every pacemaker neuron recorded (50/50), bursting was abolished at much lower concentrations of riluzole ({$\leq$}20 {$\mu$}M). Thus, eliminating the pacemaker population (our statistics confirm that this population is reduced at least 94\%, p {$<$} 0.05) does not affect respiratory rhythm. These results suggest that voltage-dependent bursting in pacemaker neurons is not essential for respiratory rhythmogenesis, which may instead be an emergent network property.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/XBQTAGEQ/Del Negro et al. - 2002 - Respiratory Rhythm An Emergent Network Property.pdf;/Users/daniekru/Zotero/storage/XE9W7EMW/S0896627302007122.html}
}

@article{delonAnimalWelfareScience2021,
  title = {Animal Welfare: Science without Hard Problems},
  shorttitle = {Animal Welfare},
  author = {Delon, Nicolas},
  year = {2021},
  month = nov,
  journal = {Metascience},
  volume = {30},
  number = {3},
  pages = {463--466},
  issn = {1467-9981},
  doi = {10.1007/s11016-021-00698-1},
  urldate = {2023-06-02},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/MSY67SVI/Delon - 2021 - Animal welfare science without hard problems.pdf}
}

@inproceedings{demarseAdaptiveFlightControl2005,
  title = {Adaptive Flight Control with Living Neuronal Networks on Microelectrode Arrays},
  booktitle = {Proceedings. 2005 {{IEEE International Joint Conference}} on {{Neural Networks}}, 2005.},
  author = {DeMarse, T.B. and Dockendorf, K.P.},
  year = {2005},
  volume = {3},
  pages = {1548--1551},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/IJCNN.2005.1556108},
  urldate = {2019-12-07},
  abstract = {The brain is perhaps one of the most robust and fault tolerant computational devices in existence and yet little is known about its mechanisms. Microelectrode arrays have recently been developed in which the computational properties of networks of living neurons can be studied in detail. In this paper we report work investigating the ability of living neurons to act as a set of neuronal weights which were used to control the flight of a simulated aircraft. These weights were manipulated via high frequency stimulation inputs to produce a system in which a living neuronal network would ``learn'' to control an aircraft for straight and level flight.},
  isbn = {978-0-7803-9048-5},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZZELCDCN/DeMarse and Dockendorf - 2005 - Adaptive flight control with living neuronal netwo.pdf}
}

@article{dennisActivationMuscarinicM12016,
  title = {Activation of {{Muscarinic M1 Acetylcholine Receptors Induces Long-Term Potentiation}} in the {{Hippocampus}}},
  author = {Dennis, Siobhan H. and Pasqui, Francesca and Colvin, Ellen M. and Sanger, Helen and Mogg, Adrian J. and Felder, Christian C. and Broad, Lisa M. and Fitzjohn, Steve M. and Isaac, John T.R. and Mellor, Jack R.},
  year = {2016},
  month = jan,
  journal = {Cerebral Cortex},
  volume = {26},
  number = {1},
  pages = {414--426},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhv227},
  urldate = {2023-04-06},
  abstract = {Muscarinic M1 acetylcholine receptors (M1Rs) are highly expressed in the hippocampus, and their inhibition or ablation disrupts the encoding of spatial memory. It has been hypothesized that the principal mechanism by which M1Rs influence spatial memory is by the regulation of hippocampal synaptic plasticity. Here, we use a combination of recently developed, well characterized, selective M1R agonists and M1R knock-out mice to define the roles of M1Rs in the regulation of hippocampal neuronal and synaptic function. We confirm that M1R activation increases input resistance and depolarizes hippocampal CA1 pyramidal neurons and show that this profoundly increases excitatory postsynaptic potential-spike coupling. Consistent with a critical role for M1Rs in synaptic plasticity, we now show that M1R activation produces a robust potentiation of glutamatergic synaptic transmission onto CA1 pyramidal neurons that has all the hallmarks of long-term potentiation (LTP): The potentiation requires NMDA receptor activity and bi-directionally occludes with synaptically induced LTP. Thus, we describe synergistic mechanisms by which acetylcholine acting through M1Rs excites CA1 pyramidal neurons and induces LTP, to profoundly increase activation of CA1 pyramidal neurons. These features are predicted to make a major contribution to the pro-cognitive effects of cholinergic transmission in rodents and humans.},
  file = {/Users/daniekru/Zotero/storage/LDD98T5I/Dennis et al. - 2016 - Activation of Muscarinic M1 Acetylcholine Receptor.pdf;/Users/daniekru/Zotero/storage/VSEXBKLX/2367383.html}
}

@article{didomenicoDopaminergicModulationPrefrontal2023,
  title = {Dopaminergic {{Modulation}} of {{Prefrontal Cortex Inhibition}}},
  author = {Di Domenico, Danila and Mapelli, Lisa},
  year = {2023},
  month = may,
  journal = {Biomedicines},
  volume = {11},
  number = {5},
  pages = {1276},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-9059},
  doi = {10.3390/biomedicines11051276},
  urldate = {2024-04-26},
  abstract = {The prefrontal cortex is the highest stage of integration in the mammalian brain. Its functions vary greatly, from working memory to decision-making, and are primarily related to higher cognitive functions. This explains the considerable effort devoted to investigating this area, revealing the complex molecular, cellular, and network organization, and the essential role of various regulatory controls. In particular, the dopaminergic modulation and the impact of local interneurons activity are critical for prefrontal cortex functioning, controlling the excitatory/inhibitory balance and the overall network processing. Though often studied separately, the dopaminergic and GABAergic systems are deeply intertwined in influencing prefrontal network processing. This mini review will focus on the dopaminergic modulation of GABAergic inhibition, which plays a significant role in shaping prefrontal cortex activity.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {dopaminergic system,GABAergic system,prefrontal cortex},
  file = {/Users/daniekru/Zotero/storage/6J7N6B8T/Di Domenico and Mapelli - 2023 - Dopaminergic Modulation of Prefrontal Cortex Inhib.pdf}
}

@article{dingSimplifiedPlasticityModel2022,
  title = {A {{Simplified Plasticity Model Based}} on {{Synaptic Tagging}} and {{Capture Theory}}: {{Simplified STC}}},
  shorttitle = {A {{Simplified Plasticity Model Based}} on {{Synaptic Tagging}} and {{Capture Theory}}},
  author = {Ding, Yiwen and Wang, Ye and Cao, Lihong},
  year = {2022},
  journal = {Frontiers in Computational Neuroscience},
  volume = {15},
  issn = {1662-5188},
  urldate = {2023-05-12},
  abstract = {The formation and consolidation of memory play a vital role for survival in an ever-changing environment. In the brain, the change and stabilization of potentiated and depressed synapses are the neural basis of memory formation and maintenance. These changes can be induced by rather short stimuli (only a few seconds or even less) but should then be stable for months or years. Recently, the neural mechanism of conversion from rapid change during the early phase of synaptic plasticity into a stable memory trace in the late phase of synaptic plasticity is more and more clear at the protein and molecular levels, among which synaptic tagging and capture (STC) theory is one of the most popular theories. According to the STC theory, the change and stabilization of synaptic efficiency mainly depend on three processes related to calcium concentration, including synaptic tagging, synthesis of plasticity-related product (PRP), and the capture of PRP by tagged synapse. Based on the STC theory, several computational models are proposed. However, these models hardly take simplicity and biological interpretability into account simultaneously. Here, we propose a simplified STC (SM-STC) model to address this issue. In the SM-STC model, the concentration of calcium ion in each neuronal compartment and synapse is first calculated, and then the tag state of synapse and PRP are updated, and the coupling effect of tagged synapse and PRP is further considered to determine the plasticity state of the synapse, either potentiation or depression. We simulated the Schaffer collaterals pathway of the hippocampus targeting a multicompartment CA1 neuron for several hours of biological time. The results show that the SM-STC model can produce a broad range of experimental phenomena known in the physiological experiments, including long-term potentiation induced by high-frequency stimuli, long-term depression induced by low-frequency stimuli, and cross-capture with two stimuli separated by a delay. Thus, the SM-STC model proposed in this study provides an effective learning rule for brain-like computation on the premise of ensuring biological plausibility and computational efficiency.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/UMCTQFYS/Ding et al. - 2022 - A Simplified Plasticity Model Based on Synaptic Ta.pdf}
}

@article{doellerEvidenceGridCells2010,
  title = {Evidence for Grid Cells in a Human Memory Network},
  author = {Doeller, Christian F. and Barry, Caswell and Burgess, Neil},
  year = {2010},
  month = feb,
  journal = {Nature},
  volume = {463},
  number = {7281},
  pages = {657--661},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature08704},
  urldate = {2023-09-24},
  abstract = {The discovery by Edvard Moser and colleagues that rats and mice possess an orientation map of their surroundings, produced and updated by a network of cerebral cortex neurons known as 'grid cells' was one of the most exciting neuroscientific findings in recent years. These cells provide a strikingly periodic representation of self-location. The question naturally arises, does a similar mechanism operate in humans? The answer is provided in a paper by Christian Doeller, Caswell Barry and Neil Burgess in which single-unit recordings of grid cells in freely moving rats were combined with whole-brain functional magnetic resonance imaging (fMRI) in humans navigating within virtual environments. Doeller et al. were able to detect a macroscopic fMRI signal representing a subject's position in a virtual reality environment that met the criteria for defining grid-cell encoding. Thus, humans appear to represent position and support spatial cognition in a manner very like that used by rodents.},
  copyright = {2010 Macmillan Publishers Limited. All rights reserved},
  langid = {english},
  keywords = {Computational neuroscience,Learning and memory,Neuronal development,to study},
  file = {/Users/daniekru/Zotero/storage/E7NR57BA/Doeller et al. - 2010 - Evidence for grid cells in a human memory network.pdf}
}

@article{donatoHowYouBuild2023,
  title = {How {{Do You Build}} a {{Cognitive Map}}? {{The Development}} of {{Circuits}} and {{Computations}} for the {{Representation}} of {{Space}} in the {{Brain}}},
  shorttitle = {How {{Do You Build}} a {{Cognitive Map}}?},
  author = {Donato, Flavio and Schwartzlose, Anja Xu and Mendes, Renan Augusto Viana},
  year = {2023},
  month = jul,
  journal = {Annual Review of Neuroscience},
  volume = {46},
  number = {Volume 46, 2023},
  pages = {281--299},
  publisher = {Annual Reviews},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-090922-010618},
  urldate = {2025-02-17},
  abstract = {In mammals, the activity of neurons in the entorhinal-hippocampal network is modulated by the animal\&apos;s position and its movement through space. At multiple stages of this distributed circuit, distinct populations of neurons can represent a rich repertoire of navigation-related variables like the animal\&apos;s location, the speed and direction of its movements, or the presence of borders and objects. Working together, spatially tuned neurons give rise to an internal representation of space, a cognitive map that supports an animal\&apos;s ability to navigate the world and to encode and consolidate memories from experience. The mechanisms by which, during development, the brain acquires the ability to create an internal representation of space are just beginning to be elucidated. In this review, we examine recent work that has begun to investigate the ontogeny of circuitry, firing patterns, and computations underpinning the representation of space in the mammalian brain.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/C9X93KF5/Donato et al. - 2023 - How Do You Build a Cognitive Map The Development of Circuits and Computations for the Representatio.pdf;/Users/daniekru/Zotero/storage/KQLDDXYH/annurev-neuro-090922-010618.html}
}

@misc{DopamineNeuronsLearn,
  title = {Dopamine Neurons Learn Relative Chosen Value from Probabilistic Rewards {\textbar} {{eLife}}},
  urldate = {2024-03-24},
  howpublished = {https://elifesciences.org/articles/18044},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/PM8NN2RE/18044.html}
}

@misc{DopaminePredictionError,
  title = {Dopamine, {{Prediction Error}} and {{Beyond}} - {{Kelly M}}. {{J}}. {{Diederen}}, {{Paul C}}. {{Fletcher}}, 2021},
  urldate = {2025-02-27},
  howpublished = {https://journals.sagepub.com/doi/full/10.1177/1073858420907591},
  file = {/Users/daniekru/Zotero/storage/HJC5QU3Y/1073858420907591.html}
}

@article{doyaMetalearningNeuromodulation2002,
  title = {Metalearning and Neuromodulation},
  author = {Doya, Kenji},
  year = {2002},
  month = jun,
  journal = {Neural Networks},
  volume = {15},
  number = {4},
  pages = {495--506},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(02)00044-8},
  urldate = {2022-10-24},
  abstract = {This paper presents a computational theory on the roles of the ascending neuromodulatory systems from the viewpoint that they mediate the global signals that regulate the distributed learning mechanisms in the brain. Based on the review of experimental data and theoretical models, it is proposed that dopamine signals the error in reward prediction, serotonin controls the time scale of reward prediction, noradrenaline controls the randomness in action selection, and acetylcholine controls the speed of memory update. The possible interactions between those neuromodulators and the environment are predicted on the basis of computational theory of metalearning.},
  langid = {english},
  keywords = {Acetylcholine,Discount factor,Dopamine,Metalearning,Neuromodulator,Noradrenaline,Reinforcement learning,Serotonin,to study},
  file = {/Users/daniekru/Zotero/storage/ADWLYGME/Doya - 2002 - Metalearning and neuromodulation.pdf;/Users/daniekru/Zotero/storage/KE7QYL49/S0893608002000448.html}
}

@article{duszkiewiczNoveltyDopaminergicModulation2019,
  title = {Novelty and {{Dopaminergic Modulation}} of {{Memory Persistence}}: {{A Tale}} of {{Two Systems}}},
  shorttitle = {Novelty and {{Dopaminergic Modulation}} of {{Memory Persistence}}},
  author = {Duszkiewicz, Adrian J. and McNamara, Colin G. and Takeuchi, Tomonori and Genzel, Lisa},
  year = {2019},
  month = feb,
  journal = {Trends in Neurosciences},
  volume = {42},
  number = {2},
  pages = {102--114},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2018.10.002},
  urldate = {2023-04-12},
  abstract = {Adaptation to the ever-changing world is critical for survival, and our brains are particularly tuned to remember events that differ from previous experiences. Novel experiences induce dopamine release in the hippocampus, a process which promotes memory persistence. While axons from the ventral tegmental area (VTA) were generally thought to be the exclusive source of hippocampal dopamine, recent studies have demonstrated that noradrenergic neurons in the locus coeruleus (LC) corelease noradrenaline and dopamine in the hippocampus and that their dopamine release boosts memory retention as well. In this opinion article, we propose that the projections originating from the VTA and the LC belong to two distinct systems that enhance memory of novel events. Novel experiences that share some commonality with past ones (`common novelty') activate the VTA and promote semantic memory formation via systems memory consolidation. By contrast, experiences that bear only a minimal relationship to past experiences (`distinct novelty') activate the LC to trigger strong initial memory consolidation in the hippocampus, resulting in vivid and long-lasting episodic memories.},
  langid = {english},
  keywords = {dopamine,episodic memory,hippocampus,memory consolidation,novelty,semantic memory},
  file = {/Users/daniekru/Zotero/storage/CAPKKDIT/Duszkiewicz et al. - 2019 - Novelty and Dopaminergic Modulation of Memory Pers.pdf;/Users/daniekru/Zotero/storage/FY7FGYP3/S016622361830273X.html}
}

@article{duvelleInsensitivityPlaceCells2019,
  title = {Insensitivity of {{Place Cells}} to the {{Value}} of {{Spatial Goals}} in a {{Two-Choice Flexible Navigation Task}}},
  author = {Duvelle, {\'E}l{\'e}onore and Grieves, Roddy M. and Hok, Vincent and Poucet, Bruno and Arleo, Angelo and Jeffery, Kate J. and Save, Etienne},
  year = {2019},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {13},
  pages = {2522--2541},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1578-18.2018},
  urldate = {2025-02-22},
  abstract = {Hippocampal place cells show position-specific activity thought to reflect a self-localization signal. Several reports also point to some form of goal encoding by place cells. We investigated this by asking whether they also encode the value of spatial goals, which is crucial information for optimizing goal-directed navigation. We used a continuous place navigation task in which male rats navigate to one of two (freely chosen) unmarked locations and wait, triggering the release of reward, which is then located and consumed elsewhere. This allows sampling of place fields and dissociates spatial goal from reward consumption. The two goals varied in the amount of reward provided, allowing assessment of whether the rats factored goal value into their navigational choice and of possible neural correlates of this value. Rats successfully learned the task, indicating goal localization, and they preferred higher-value goals, indicating processing of goal value. Replicating previous findings, there was goal-related activity in the out-of-field firing of CA1 place cells, with a ramping-up of firing rate during the waiting period, but no general overrepresentation of goals by place fields, an observation that we extended to CA3 place cells. Importantly, place cells were not modulated by goal value. This suggests that dorsal hippocampal place cells encode space independently of its associated value despite the effect of that value on spatial behavior. Our findings are consistent with a model of place cells in which they provide a spontaneously constructed value-free spatial representation rather than encoding other navigationally relevant but nonspatial information. SIGNIFICANCE STATEMENT We investigated whether hippocampal place cells, which compute a self-localization signal, also encode the relative value of places, which is essential information for optimal navigation. When choosing between two spatial goals of different value, rats preferred the higher-value goal. We saw out-of-field goal firing in place cells, replicating previous observations that the cells are influenced by the goal, but their activity was not modulated by the value of these goals. Our results suggest that place cells do not encode all of the navigationally relevant aspects of a place, but instead form a value-free ``map'' that links to such aspects in other parts of the brain.},
  chapter = {Research Articles},
  copyright = {Copyright {\copyright} 2019 Duvelle et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {30696727},
  keywords = {goal value,goal-directed behavior,hippocampus,place cells,rat},
  file = {/Users/daniekru/Zotero/storage/M2USUNV3/Duvelle et al. - 2019 - Insensitivity of Place Cells to the Value of Spatial Goals in a Two-Choice Flexible Navigation Task.pdf}
}

@article{duvelleInsensitivityPlaceCells2019a,
  title = {Insensitivity of {{Place Cells}} to the {{Value}} of {{Spatial Goals}} in a {{Two-Choice Flexible Navigation Task}}},
  author = {Duvelle, {\'E}l{\'e}onore and Grieves, Roddy M. and Hok, Vincent and Poucet, Bruno and Arleo, Angelo and Jeffery, Kate J. and Save, Etienne},
  year = {2019},
  month = mar,
  journal = {The Journal of Neuroscience},
  volume = {39},
  number = {13},
  pages = {2522--2541},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.1578-18.2018},
  urldate = {2025-02-22},
  abstract = {Hippocampal place cells show position-specific activity thought to reflect a self-localization signal. Several reports also point to some form of goal encoding by place cells. We investigated this by asking whether they also encode the value of spatial goals, which is crucial information for optimizing goal-directed navigation. We used a continuous place navigation task in which male rats navigate to one of two (freely chosen) unmarked locations and wait, triggering the release of reward, which is then located and consumed elsewhere. This allows sampling of place fields and dissociates spatial goal from reward consumption. The two goals varied in the amount of reward provided, allowing assessment of whether the rats factored goal value into their navigational choice and of possible neural correlates of this value. Rats successfully learned the task, indicating goal localization, and they preferred higher-value goals, indicating processing of goal value. Replicating previous findings, there was goal-related activity in the out-of-field firing of CA1 place cells, with a ramping-up of firing rate during the waiting period, but no general overrepresentation of goals by place fields, an observation that we extended to CA3 place cells. Importantly, place cells were not modulated by goal value. This suggests that dorsal hippocampal place cells encode space independently of its associated value despite the effect of that value on spatial behavior. Our findings are consistent with a model of place cells in which they provide a spontaneously constructed value-free spatial representation rather than encoding other navigationally relevant but nonspatial information., SIGNIFICANCE STATEMENT We investigated whether hippocampal place cells, which compute a self-localization signal, also encode the relative value of places, which is essential information for optimal navigation. When choosing between two spatial goals of different value, rats preferred the higher-value goal. We saw out-of-field goal firing in place cells, replicating previous observations that the cells are influenced by the goal, but their activity was not modulated by the value of these goals. Our results suggest that place cells do not encode all of the navigationally relevant aspects of a place, but instead form a value-free ``map'' that links to such aspects in other parts of the brain.},
  pmcid = {PMC6435828},
  pmid = {30696727},
  file = {/Users/daniekru/Zotero/storage/8ZLNJNUB/Duvelle et al. - 2019 - Insensitivity of Place Cells to the Value of Spatial Goals in a Two-Choice Flexible Navigation Task.pdf}
}

@article{ecoffetFirstReturnThen2021,
  title = {First Return, Then Explore},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2021},
  month = feb,
  journal = {Nature},
  volume = {590},
  number = {7847},
  pages = {580--586},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03157-9},
  urldate = {2023-12-08},
  abstract = {Reinforcement learning promises to solve complex sequential-decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse1 and deceptive2 feedback. Avoiding these pitfalls requires a thorough exploration of the environment, but creating algorithms that can do~so remains one of the central challenges of the field. Here we hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (detachment) and failing to first return to a state before exploring from it (derailment). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly `remembering' promising states and returning to such states before intentionally exploring. Go-Explore solves all previously unsolved Atari games and surpasses the state of the art on all hard-exploration games1, with orders-of-magnitude improvements on the grand challenges of Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration---an insight that may prove critical to the creation of truly intelligent learning agents.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Computer science},
  file = {/Users/daniekru/Zotero/storage/ZZZUQRP5/Ecoffet et al. - 2021 - First return, then explore.pdf}
}

@article{edelmannDopaminergicInnervationModulation2018,
  title = {Dopaminergic Innervation and Modulation of Hippocampal Networks},
  author = {Edelmann, Elke and Lessmann, Volkmar},
  year = {2018},
  month = sep,
  journal = {Cell and Tissue Research},
  volume = {373},
  number = {3},
  pages = {711--727},
  issn = {1432-0878},
  doi = {10.1007/s00441-018-2800-7},
  urldate = {2023-04-12},
  abstract = {The catecholamine dopamine plays an important role in hippocampus-dependent plasticity and related learning and memory processes. Dopamine secretion in the hippocampus is activated by, e.g., salient or novel stimuli, thereby helping to establish and to stabilize hippocampus-dependent memories. Disturbed dopaminergic function in the hippocampus leads to severe pathophysiological conditions. While the role and importance of dopaminergic modulation of hippocampal networks have been unequivocally proven, there is still a lack of detailed molecular and cellular mechanistic understanding of how dopamine orchestrates these hippocampal processes. In this chapter of the special issue ``Hippocampal structure and function,'' we will discuss the current understanding of dopaminergic modulation of basal synaptic transmission and long-lasting, activity-dependent potentiation or depression.},
  langid = {english},
  keywords = {Dopamine,Dorsal,Hippocampus,Receptors,Synaptic plasticity,Ventral},
  file = {/Users/daniekru/Zotero/storage/SMCJ7I8V/Edelmann and Lessmann - 2018 - Dopaminergic innervation and modulation of hippoca.pdf}
}

@article{ehrlichMeasureComplexityNeural,
  title = {A {{Measure}} of the {{Complexity}} of {{Neural Representations}} Based on {{Partial Information Decomposition}}},
  author = {Ehrlich, David A and Schneider, Andreas C and Priesemann, Viola and Wibral, Michael and Makkeh, Abdullah},
  abstract = {In neural networks, task-relevant information is represented jointly by groups of neurons. However, the specific way in which this mutual information about the classification label is distributed among the individual neurons is not well understood: While parts of it may only be obtainable from specific single neurons, other parts are carried redundantly or synergistically by multiple neurons. We show how Partial Information Decomposition (PID), a recent extension of information theory, can disentangle these different contributions. From this, we introduce the measure of ``Representational Complexity'', which quantifies the difficulty of accessing information spread across multiple neurons. We show how this complexity is directly computable for smaller layers. For larger layers, we propose subsampling and coarse-graining procedures and prove corresponding bounds on the latter. Empirically, for quantized deep neural networks solving the MNIST and CIFAR10 tasks, we observe that representational complexity decreases both through successive hidden layers and over training, and compare the results to related measures. Overall, we propose representational complexity as a principled and interpretable summary statistic for analyzing the structure and evolution of neural representations and complex systems in general.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/XNEHMKLJ/Ehrlich et al. - A Measure of the Complexity of Neural Representati.pdf}
}

@article{elliottDissociableFunctionsMedial2000,
  title = {Dissociable {{Functions}} in the {{Medial}} and {{Lateral Orbitofrontal Cortex}}: {{Evidence}} from {{Human Neuroimaging Studies}}},
  shorttitle = {Dissociable {{Functions}} in the {{Medial}} and {{Lateral Orbitofrontal Cortex}}},
  author = {Elliott, Rebecca and Dolan, Raymond J. and Frith, Chris D.},
  year = {2000},
  month = mar,
  journal = {Cerebral Cortex},
  volume = {10},
  number = {3},
  pages = {308--317},
  issn = {1047-3211},
  doi = {10.1093/cercor/10.3.308},
  urldate = {2024-05-14},
  abstract = {Recent imaging studies show that the orbitofrontal cortex (OFC) is~activated during a wide variety of paradigms, including guessing tasks, simple delayed matching tasks and sentence completion. We suggest that, as with other regions of the prefrontal cortex, activity in the OFC is most likely to be observed when there is insufficient information available to determine the appropriate course of action. In these circumstances the OFC, rather than other prefrontal regions, is more likely to be activated when the problem of what to do next is best solved by taking into account the likely reward value of stimuli and responses, rather than their identity or location. We suggest that selection of stimuli on the basis of their familiarity and responses on the basis of a feeling of `rightness' are also examples of selection on the basis of reward value. Within the OFC, the lateral regions are more likely to be involved when the action selected requires the suppression of previously rewarded responses.},
  file = {/Users/daniekru/Zotero/storage/92CFAQC4/Elliott et al. - 2000 - Dissociable Functions in the Medial and Lateral Or.pdf;/Users/daniekru/Zotero/storage/CCN3L9YY/449600.html}
}

@misc{EmergentElasticityNeural,
  title = {Emergent Elasticity in the Neural Code for Space {\textbar} {{PNAS}}},
  urldate = {2023-11-09},
  howpublished = {https://www.pnas.org/doi/10.1073/pnas.1805959115},
  keywords = {important,to study}
}

@article{enelStableDynamicRepresentations2020,
  title = {Stable and Dynamic Representations of Value in the Prefrontal Cortex},
  author = {Enel, Pierre and Wallis, Joni D and Rich, Erin L},
  editor = {Schoenbaum, Geoffrey and Wassum, Kate M and Hunt, Laurence Tudor and Schoenbaum, Geoffrey},
  year = {2020},
  month = jul,
  journal = {eLife},
  volume = {9},
  pages = {e54313},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.54313},
  urldate = {2024-11-27},
  abstract = {Optimal decision-making requires that stimulus-value associations are kept up to date by constantly comparing the expected value of a stimulus with its experienced outcome. To do this, value information must be held in mind when a stimulus and outcome are separated in time. However, little is known about the neural mechanisms of working memory (WM) for value. Contradicting theories have suggested WM requires either persistent or transient neuronal activity, with stable or dynamic representations, respectively. To test these hypotheses, we recorded neuronal activity in the orbitofrontal and anterior cingulate cortex of two monkeys performing a valuation task. We found that features of all hypotheses were simultaneously present in prefrontal activity, and no single hypothesis was exclusively supported. Instead, mixed dynamics supported robust, time invariant value representations while also encoding the information in a temporally specific manner. We suggest that this hybrid coding is a critical mechanism supporting flexible cognitive abilities.},
  keywords = {decision-making,neural coding,orbitofrontal cortex,prefrontal cortex,value,working memory},
  file = {/Users/daniekru/Zotero/storage/VWNML2HH/Enel et al. - 2020 - Stable and dynamic representations of value in the prefrontal cortex.pdf}
}

@misc{EpisodicMemoryFormation,
  title = {Episodic {{Memory}} Formation: {{A}} Review of Complex {{Hippocampus}} Input Pathways {\textbar} {{Elsevier Enhanced Reader}}},
  shorttitle = {Episodic {{Memory}} Formation},
  doi = {10.1016/j.pnpbp.2023.110757},
  urldate = {2023-05-16},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S027858462300043X?token=33E15C2606DEB14BA0E87D55B4669AC659281E17A552E2F70CE6996EE63CBE5F3446E93DF74DE08C314FEBEAAEFBEE24\&originRegion=us-east-1\&originCreation=20230516074440},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/CVV9THPJ/Episodic Memory formation A review of complex Hip.pdf}
}

@article{epsteinCognitiveMapHumans2017,
  title = {The Cognitive Map in Humans: {{Spatial}} Navigation and Beyond},
  shorttitle = {The Cognitive Map in Humans},
  author = {Epstein, Russell A. and Patai, Eva Zita and Julian, Joshua B. and Spiers, Hugo J.},
  year = {2017},
  month = oct,
  journal = {Nature neuroscience},
  volume = {20},
  number = {11},
  pages = {1504--1513},
  issn = {1097-6256},
  doi = {10.1038/nn.4656},
  urldate = {2025-02-15},
  abstract = {The `cognitive map' hypothesis proposes that brain builds a unified representation of the spatial environment to support memory and guide future action. Forty years of electrophysiological research in rodents suggests that cognitive maps are neurally instantiated by place, grid, border, and head direction cells in the hippocampal formation and related structures. Here we review recent work that suggests a similar functional organization in the human brain and reveals novel insights into how cognitive maps are used during spatial navigation. Specifically, these studies indicate that: (i) the human hippocampus and entorhinal cortex support map-like spatial codes; (ii) posterior brain regions such as parahippocampal and retrosplenial cortices provide critical inputs that allow cognitive maps to be anchored to fixed environmental landmarks; (iii) hippocampal and entorhinal spatial codes are used in conjunction with frontal lobe mechanisms to plan routes during navigation. We also discuss how these three basic elements of cognitive map based navigation spatial coding, landmark anchoring, and route planning might be applied to non-spatial domains to provide the building blocks for many core elements of human thought.},
  pmcid = {PMC6028313},
  pmid = {29073650},
  file = {/Users/daniekru/Zotero/storage/BF8ZHIGE/Epstein et al. - 2017 - The cognitive map in humans Spatial navigation and beyond.pdf}
}

@article{epsteinNeuralSystemsLandmarkbased2014,
  title = {Neural Systems for Landmark-Based Wayfinding in Humans},
  author = {Epstein, Russell A. and Vass, Lindsay K.},
  year = {2014},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1635},
  pages = {20120533},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2012.0533},
  urldate = {2025-02-22},
  abstract = {Humans and animals use landmarks during wayfinding to determine where they are in the world and to guide their way to their destination. To implement this strategy, known as landmark-based piloting, a navigator must be able to: (i) identify individual landmarks, (ii) use these landmarks to determine their current position and heading, (iii) access long-term knowledge about the spatial relationships between locations and (iv) use this knowledge to plan a route to their navigational goal. Here, we review neuroimaging, neuropsychological and neurophysiological data that link the first three of these abilities to specific neural systems in the human brain. This evidence suggests that the parahippocampal place area is critical for landmark recognition, the retrosplenial/medial parietal region is centrally involved in localization and orientation, and both medial temporal lobe and retrosplenial/medial parietal lobe regions support long-term spatial knowledge.},
  keywords = {functional magnetic resonance imaging,hippocampus,parahippocampal cortex,parietal lobe,retrosplenial cortex,spatial navigation},
  file = {/Users/daniekru/Zotero/storage/ILBMYVNP/Epstein and Vass - 2014 - Neural systems for landmark-based wayfinding in humans.pdf}
}

@article{erdemGoalDirectedSpatialNavigation2012,
  title = {A {{Goal-Directed Spatial Navigation Model Using Forward Trajectory Planning Based}} on {{Grid Cells}}},
  author = {Erdem, U{\u g}ur Murat and Hasselmo, Michael E.},
  year = {2012},
  month = mar,
  journal = {The European journal of neuroscience},
  volume = {35},
  number = {6},
  pages = {916--931},
  issn = {0953-816X},
  doi = {10.1111/j.1460-9568.2012.08015.x},
  urldate = {2025-02-22},
  abstract = {A goal-directed navigation model is proposed based on forward linear look-ahead probe of trajectories in a network of head direction cells, grid cells, place cells, and prefrontal cortex (PFC) cells. The model allows selection of new goal-directed trajectories. In a novel environment, the virtual rat incrementally creates a map composed of place cells and PFC cells by random exploration. After exploration, the rat retrieves memory of the goal location, picks its next movement direction by forward linear look-ahead probe of trajectories in several candidate directions while stationary in one location, and finds the one activating PFC cells with the highest reward signal. Each probe direction involves activation of a static pattern of head direction cells to drive an interference model of grid cells to update their phases in a specific direction. The updating of grid cell spiking drives place cells along the probed look-ahead trajectory similar to the forward replay during waking seen in place cell recordings. Directions are probed until the look-ahead trajectory activates the reward signal and the corresponding direction is used to guide goal-finding behavior. We report simulation results in several mazes with and without barriers. Navigation with barriers requires a PFC map topology based on the temporal vicinity of visited place cells and a reward signal diffusion process. The interaction of the forward linear look-ahead trajectory probes with the reward diffusion allows discovery of never before experienced shortcuts towards a goal location.},
  pmcid = {PMC3564559},
  pmid = {22393918},
  file = {/Users/daniekru/Zotero/storage/I5FGV2II/Erdem and Hasselmo - 2012 - A Goal-Directed Spatial Navigation Model Using Forward Trajectory Planning Based on Grid Cells.pdf}
}

@misc{esnaola-acebesBumpAttractorDynamics2021,
  title = {Bump Attractor Dynamics Underlying Stimulus Integration in Perceptual Estimation Tasks},
  author = {{Esnaola-Acebes}, Jose M. and Roxin, Alex and Wimmer, Klaus},
  year = {2021},
  month = mar,
  doi = {10.1101/2021.03.15.434192},
  urldate = {2024-05-15},
  abstract = {Perceptual decision and continuous stimulus estimation tasks involve making judgments based on accumulated sensory evidence. Network models of evidence integration usually rely on competition between neural populations each encoding a discrete categorical choice. By design, these models do not maintain information of the integrated stimulus (e.g. the average stimulus direction in degrees) that is necessary for a continuous perceptual judgement. Here, we show that the continuous ring attractor network can integrate a stimulus feature such as orientation and track the stimulus average in the phase of its activity bump. We reduced the network dynamics of the ring model to a two-dimensional equation for the amplitude and the phase of the bump. Interestingly, these reduced equations are nearly identical to an optimal integration process for computing the running average of the stimulus orientation. They differ only in the intrinsic dynamics of the amplitude, which affects the temporal weighting of the sensory evidence. Whether the network shows early (primacy), uniform or late (recency) weighting depends on the relative strength of sensory stimuli compared to the amplitude of the bump and on the initial state of the network. The specific relation between the internal network dynamics and the sensory inputs can be modulated by changing a single parameter of the model, the global excitatory drive. We show that this can account for the heterogeneity of temporal weighting profiles observed in humans integrating a stream of oriented stimulus frames. Our findings point to continuous attractor dynamics as a plausible mechanism underlying stimulus integration in perceptual estimation tasks.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/4WLPW3CH/Esnaola-Acebes et al. - 2021 - Bump attractor dynamics underlying stimulus integr.pdf}
}

@misc{EvaluationBioInspiredModels,
  title = {Evaluation of {{Bio-Inspired Models}} under {{Different Learning Settings For Energy Efficiency}} in {{Network Traffic Prediction}}.},
  urldate = {2025-01-16},
  howpublished = {https://arxiv.org/html/2412.17565},
  file = {/Users/daniekru/Zotero/storage/8ME9CLWM/2412.html}
}

@incollection{faisalNoiseNeuronsOther2012,
  title = {Noise in {{Neurons}} and {{Other Constraints}}},
  booktitle = {Computational {{Systems Neurobiology}}},
  author = {Faisal, A. Aldo},
  editor = {Le Nov{\`e}re, N.},
  year = {2012},
  pages = {227--257},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-007-3858-4_8},
  urldate = {2024-12-10},
  abstract = {How do the properties of signalling molecules constrain the structure and function biological networks such as those of our brain? Here we focus on the action potential, the fundamental electrical signal of the brain, because malfunction of the action potential causes many neurological conditions. The action potential is mediated by the concerted action of voltagegated ion channels and relating the properties of these signalling molecules to the properties of neurons at the systems level is essential for biomedical brain research, as minor variations in properties of a neurons individual component, can have large, pathological effects on the physiology of the whole nervous system and the behaviour it generates. This approach is very complex and requires us to discuss computational methods that can span across many levels of biological organization, from single signalling proteins to the organization of the entire nervous system, and encompassing time scales from milliseconds to hours.Within this methodical framework, we will focus on how the properties of voltagegated ion channels relate to the functional and structural requirements of axonal signalling and the engineering design principles of neurons and their axons (nerve fibres). This is important, not only because axons are the essential wires that allow information transmission between neurons, but also because they play a crucial in neural computation itself.},
  isbn = {978-94-007-3858-4},
  langid = {english},
  keywords = {Axon Diameter,Channel Noise,Squid Giant Axon,Stochastic Simulation,Unmyelinated Axon},
  file = {/Users/daniekru/Zotero/storage/UNC8ZGRI/Faisal - 2012 - Noise in Neurons and Other Constraints.pdf}
}

@article{farzanfarCognitiveMapsSpatial2023,
  title = {From Cognitive Maps to Spatial Schemas},
  author = {Farzanfar, Delaram and Spiers, Hugo J. and Moscovitch, Morris and Rosenbaum, R. Shayna},
  year = {2023},
  month = feb,
  journal = {Nature Reviews Neuroscience},
  volume = {24},
  number = {2},
  pages = {63--79},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00655-9},
  urldate = {2025-02-17},
  abstract = {A schema refers to a structured body of prior knowledge that captures common patterns across related experiences. Schemas have been studied separately in the realms of episodic memory and spatial navigation across different species and have been grounded in theories of memory consolidation, but there has been little attempt to integrate our understanding across domains, particularly in humans. We propose that experiences during navigation with many similarly structured environments give rise to the formation of spatial schemas (for example, the expected layout of modern cities) that share properties with but are distinct from cognitive maps (for example, the memory of a modern city) and event schemas (such as expected events in a modern city) at both cognitive and neural levels. We describe earlier theoretical frameworks and empirical findings relevant to spatial schemas, along with more targeted investigations of spatial schemas in human and non-human animals. Consideration of architecture and urban analytics, including the influence of scale and regionalization, on different properties of spatial schemas may provide a powerful approach to advance our understanding of spatial schemas.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Cognitive neuroscience,Psychology},
  file = {/Users/daniekru/Zotero/storage/5QWNK3R4/Farzanfar et al. - 2023 - From cognitive maps to spatial schemas.pdf}
}

@article{fentonRemappingRevisitedHow2024,
  title = {Remapping Revisited: How the Hippocampus Represents Different Spaces},
  shorttitle = {Remapping Revisited},
  author = {Fenton, Andr{\'e} A.},
  year = {2024},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  volume = {25},
  number = {6},
  pages = {428--448},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/s41583-024-00817-x},
  urldate = {2025-02-24},
  abstract = {The representation of distinct spaces by hippocampal place cells has been linked to changes in their place fields (the locations in the environment where the place cells discharge strongly), a phenomenon that has been termed `remapping'. Remapping has been assumed to be accompanied by the reorganization of subsecond cofiring relationships among the place cells, potentially maximizing hippocampal information coding capacity. However, several observations challenge this standard view. For example, place cells exhibit mixed selectivity, encode non-positional variables, can have multiple place fields and exhibit unreliable discharge in fixed environments. Furthermore, recent evidence suggests that, when measured at subsecond timescales, the moment-to-moment cofiring of a pair of cells in one environment is remarkably similar in another environment, despite remapping. Here, I propose that remapping is a misnomer for the changes in place fields across environments and suggest instead that internally organized manifold representations of hippocampal activity are actively registered to different environments to enable navigation, promote memory and organize knowledge.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Hippocampus,Long-term memory,Navigation},
  file = {/Users/daniekru/Zotero/storage/MVASINTX/Fenton - 2024 - Remapping revisited how the hippocampus represents different spaces.pdf}
}

@inproceedings{fetteShortTermMemory2005,
  title = {Short {{Term Memory}} and {{Pattern Matching}} with {{Simple Echo State Networks}}},
  booktitle = {Artificial {{Neural Networks}}: {{Biological Inspirations}} -- {{ICANN}} 2005},
  author = {Fette, Georg and Eggert, Julian},
  editor = {Duch, W{\l}odzis{\l}aw and Kacprzyk, Janusz and Oja, Erkki and Zadro{\.z}ny, S{\l}awomir},
  year = {2005},
  pages = {13--18},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11550822_3},
  abstract = {Two recently proposed approaches to recognize temporal patterns have been proposed by J{\"a}ger with the so called Echo State Network (ESN) and by Maass with the so called Liquid State Machine (LSM). The ESN approach assumes a sort of ``black-box'' operability of the networks and claims a broad applicability to several different problems using the same principle. Here we propose a simplified version of ESNs which we call Simple Echo State Network (SESN) which exhibits good results in memory capacity and pattern matching tasks and which allows a better understanding of the capabilities and restrictions of ESNs.},
  isbn = {978-3-540-28754-4},
  langid = {english},
  keywords = {Hide Layer,Hide Unit,Input Weight,Memory Capacity,Output Weight},
  file = {/Users/daniekru/Zotero/storage/TZPPW836/Fette and Eggert - 2005 - Short Term Memory and Pattern Matching with Simple.pdf}
}

@book{finlayHowTrainYour2020,
  title = {How to Train Your Neural {{ODE}}},
  author = {Finlay, Chris and Jacobsen, J{\"o}rn-Henrik and Nurbekyan, Levon and Oberman, Adam},
  year = {2020},
  month = feb,
  abstract = {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE based generative models to the same performance as the unregularized dynamics in just over a day on one GPU, whereas unregularized dynamics can take up to 4-6 days of training time on multiple GPUs. This brings neural ODEs significantly closer to practical relevance in large-scale applications.},
  file = {/Users/daniekru/Zotero/storage/ATKLPQVX/Finlay et al. - 2020 - How to train your neural ODE.pdf}
}

@article{floreanoNeuroevolutionArchitecturesLearning2008,
  title = {Neuroevolution: From Architectures to Learning},
  shorttitle = {Neuroevolution},
  author = {Floreano, Dario and D{\"u}rr, Peter and Mattiussi, Claudio},
  year = {2008},
  month = mar,
  journal = {Evolutionary Intelligence},
  volume = {1},
  number = {1},
  pages = {47--62},
  issn = {1864-5909, 1864-5917},
  doi = {10.1007/s12065-007-0002-4},
  urldate = {2020-05-21},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/A9VTWEPK/Floreano et al. - 2008 - Neuroevolution from architectures to learning.pdf}
}

@article{frankAnatomyDecisionStriatoorbitofrontal2006,
  title = {Anatomy of a Decision: {{Striato-orbitofrontal}} Interactions in Reinforcement Learning, Decision Making, and Reversal.},
  shorttitle = {Anatomy of a Decision},
  author = {Frank, Michael J. and Claus, Eric D.},
  year = {2006},
  month = apr,
  journal = {Psychological Review},
  volume = {113},
  number = {2},
  pages = {300--326},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.113.2.300},
  urldate = {2024-05-14},
  abstract = {The authors explore the division of labor between the basal ganglia-- dopamine (BG-DA) system and the orbitofrontal cortex (OFC) in decision making. They show that a primitive neural network model of the BG-DA system slowly learns to make decisions on the basis of the relative probability of rewards but is not as sensitive to (a) recency or (b) the value of specific rewards. An augmented model that explores BG-OFC interactions is more successful at estimating the true expected value of decisions and is faster at switching behavior when reinforcement contingencies change. In the augmented model, OFC areas exert top-down control on the BG and premotor areas by representing reinforcement magnitudes in working memory. The model successfully captures patterns of behavior resulting from OFC damage in decision making, reversal learning, and devaluation paradigms and makes additional predictions for the underlying source of these deficits.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/WNJC8TGP/Frank and Claus - 2006 - Anatomy of a decision Striato-orbitofrontal inter.pdf}
}

@article{frankDynamicDopamineModulation2005,
  title = {Dynamic {{Dopamine Modulation}} in the {{Basal Ganglia}}: {{A Neurocomputational Account}} of {{Cognitive Deficits}} in {{Medicated}} and {{Nonmedicated Parkinsonism}}},
  shorttitle = {Dynamic {{Dopamine Modulation}} in the {{Basal Ganglia}}},
  author = {Frank, Michael J.},
  year = {2005},
  month = jan,
  journal = {Journal of Cognitive Neuroscience},
  volume = {17},
  number = {1},
  pages = {51--72},
  issn = {0898-929X},
  doi = {10.1162/0898929052880093},
  urldate = {2022-10-03},
  abstract = {Dopamine (DA) depletion in the basal ganglia (BG) of Parkinson's patients gives rise to both frontal-like and implicit learning impairments. Dopaminergic medication alleviates some cognitive deficits but impairs those that depend on intact areas of the BG, apparently due to DA ``overdose.'' These findings are difficult to accommodate with verbal theories of BG/DA function, owing to complexity of system dynamics: DA dynamically modulates function in the BG, which is itself a modulatory system. This article presents a neural network model that instantiates key biological properties and provides insight into the underlying role of DA in the BG during learning and execution of cognitive tasks. Specifically, the BG modulates the execution of ``actions'' (e.g., motor responses and working memory updating) being considered in different parts of the frontal cortex. Phasic changes in DA, which occur during error feedback, dynamically modulate the BG threshold for facilitating/suppressing a cortical command in response to particular stimuli. Reduced dynamic range of DA explains Parkinson and DA overdose deficits with a single underlying dysfunction, despite overall differences in raw DA levels. Simulated Parkinsonism and medication effects provide a theoretical basis for behavioral data in probabilistic classification and reversal tasks. The model also provides novel testable predictions for neuropsychological and pharmacological studies, and motivates further investigation of BG/DA interactions with the prefrontal cortex in working memory.},
  file = {/Users/daniekru/Zotero/storage/QXMXMJSP/Frank_2005_Dynamic Dopamine Modulation in the Basal Ganglia.pdf;/Users/daniekru/Zotero/storage/JQ5C37JG/Dynamic-Dopamine-Modulation-in-the-Basal-Ganglia-A.html}
}

@article{frankInteractionsFrontalCortex2001,
  title = {Interactions between Frontal Cortex and Basal Ganglia in Working Memory: {{A}} Computational Model},
  shorttitle = {Interactions between Frontal Cortex and Basal Ganglia in Working Memory},
  author = {Frank, M. J. and Loughry, B. and O'Reilly, R. C.},
  year = {2001},
  month = jun,
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  volume = {1},
  number = {2},
  pages = {137--160},
  issn = {1530-7026, 1531-135X},
  doi = {10.3758/CABN.1.2.137},
  urldate = {2024-05-14},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/V5R9YYYX/Frank et al. - 2001 - Interactions between frontal cortex and basal gang.pdf}
}

@article{fremauxReinforcementLearningUsing2013,
  title = {Reinforcement {{Learning Using}} a {{Continuous Time Actor-Critic Framework}} with {{Spiking Neurons}}},
  author = {Fr{\'e}maux, Nicolas and Sprekeler, Henning and Gerstner, Wulfram},
  year = {2013},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {4},
  pages = {e1003024},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003024},
  urldate = {2024-05-14},
  abstract = {Animals repeat rewarded behaviors, but the physiological basis of reward-based learning has only been partially elucidated. On one hand, experimental evidence shows that the neuromodulator dopamine carries information about rewards and affects synaptic plasticity. On the other hand, the theory of reinforcement learning provides a framework for reward-based learning. Recent models of reward-modulated spike-timing-dependent plasticity have made first steps towards bridging the gap between the two approaches, but faced two problems. First, reinforcement learning is typically formulated in a discrete framework, ill-adapted to the description of natural situations. Second, biologically plausible models of reward-modulated spike-timing-dependent plasticity require precise calculation of the reward prediction error, yet it remains to be shown how this can be computed by neurons. Here we propose a solution to these problems by extending the continuous temporal difference (TD) learning of Doya (2000) to the case of spiking neurons in an actor-critic network operating in continuous time, and with continuous state and action representations. In our model, the critic learns to predict expected future rewards in real time. Its activity, together with actual rewards, conditions the delivery of a neuromodulatory TD signal to itself and to the actor, which is responsible for action choice. In simulations, we show that such an architecture can solve a Morris water-maze-like navigation task, in a number of trials consistent with reported animal performance. We also use our model to solve the acrobot and the cartpole problems, two complex motor control tasks. Our model provides a plausible way of computing reward prediction error in the brain. Moreover, the analytically derived learning rule is consistent with experimental evidence for dopamine-modulated spike-timing-dependent plasticity.},
  langid = {english},
  keywords = {Action potentials,Coding mechanisms,Decision making,Dopamine,Learning,Neuronal tuning,Neurons,Synapses},
  file = {/Users/daniekru/Zotero/storage/DIKWLDBA/Frémaux et al. - 2013 - Reinforcement Learning Using a Continuous Time Act.pdf}
}

@article{friesRhythmsCognitionCommunication2015,
  title = {Rhythms for {{Cognition}}: {{Communication}} through {{Coherence}}},
  shorttitle = {Rhythms for {{Cognition}}},
  author = {Fries, Pascal},
  year = {2015},
  month = oct,
  journal = {Neuron},
  volume = {88},
  number = {1},
  pages = {220--235},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2015.09.034},
  urldate = {2020-10-30},
  abstract = {I propose that synchronization affects communication between neuronal groups. Gamma-band (30--90~Hz) synchronization modulates excitation rapidly enough that it escapes the following inhibition and activates postsynaptic neurons effectively. Synchronization also ensures that a presynaptic activation pattern arrives at postsynaptic neurons in a temporally coordinated manner. At a postsynaptic neuron, multiple presynaptic groups converge, e.g., representing different stimuli. If a stimulus is selected by attention, its neuronal representation shows stronger and higher-frequency gamma-band synchronization. Thereby, the attended stimulus representation selectively entrains postsynaptic neurons. The entrainment creates sequences of short excitation and longer inhibition that are coordinated between pre- and postsynaptic groups to transmit the attended representation and shut out competing inputs. The predominantly bottom-up-directed gamma-band influences are controlled by predominantly top-down-directed alpha-beta-band (8--20~Hz) influences. Attention itself samples stimuli at a 7--8~Hz theta rhythm. Thus, several rhythms and their interplay render neuronal communication effective, precise, and selective.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/MBK3ZVYU/Fries - 2015 - Rhythms for Cognition Communication through Coher.pdf;/Users/daniekru/Zotero/storage/8I77W37G/S0896627315008235.html}
}

@article{fristonActionUnderstandingActive2011,
  title = {Action Understanding and Active Inference},
  author = {Friston, Karl and Mattout, J{\'e}r{\'e}mie and Kilner, James},
  year = {2011},
  month = feb,
  journal = {Biological Cybernetics},
  volume = {104},
  number = {1},
  pages = {137--160},
  issn = {1432-0770},
  doi = {10.1007/s00422-011-0424-z},
  urldate = {2021-12-01},
  abstract = {We have suggested that the mirror-neuron system might be usefully understood as implementing Bayes-optimal perception of actions emitted by oneself or others. To substantiate this claim, we present neuronal simulations that show the same representations can prescribe motor behavior and encode motor intentions during action--observation. These simulations are based on the free-energy formulation of active inference, which is formally related to predictive coding. In this scheme, (generalised) states of the world are represented as trajectories. When these states include motor trajectories they implicitly entail intentions (future motor states). Optimizing the representation of these intentions enables predictive coding in a prospective sense. Crucially, the same generative models used to make predictions can be deployed to predict the actions of self or others by simply changing the bias or precision (i.e. attention) afforded to proprioceptive signals. We illustrate these points using simulations of handwriting to illustrate neuronally plausible generation and recognition of itinerant (wandering) motor trajectories. We then use the same simulations to produce synthetic electrophysiological responses to violations of intentional expectations. Our results affirm that a Bayes-optimal approach provides a principled framework, which accommodates current thinking about the mirror-neuron system. Furthermore, it endorses the general formulation of action as active inference.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZDYJIND3/Friston et al. - 2011 - Action understanding and active inference.pdf}
}

@article{fristonFreeenergyPrincipleUnified2010,
  title = {The Free-Energy Principle: A Unified Brain Theory?},
  shorttitle = {The Free-Energy Principle},
  author = {Friston, Karl},
  year = {2010},
  month = feb,
  journal = {Nature Reviews Neuroscience},
  volume = {11},
  number = {2},
  pages = {127--138},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2787},
  urldate = {2021-12-02},
  abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories --- optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/GRLAGIEV/nrn2787.pdf}
}

@article{fristonSophisticatedInference2021,
  title = {Sophisticated {{Inference}}},
  author = {Friston, Karl and Da Costa, Lancelot and Hafner, Danijar and Hesp, Casper and Parr, Thomas},
  year = {2021},
  month = mar,
  journal = {Neural Computation},
  volume = {33},
  number = {3},
  pages = {713--763},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01351},
  urldate = {2021-12-02},
  abstract = {Active inference offers a first principle account of sentient behavior, from which special and important cases---for example, reinforcement learning, active learning, Bayes optimal inference, Bayes optimal design---can be derived. Active inference finesses the exploitation-exploration dilemma in relation to prior preferences by placing information gain on the same footing as reward or value. In brief, active inference replaces value functions with functionals of (Bayesian) beliefs, in the form of an expected (variational) free energy. In this letter, we consider a sophisticated kind of active inference using a recursive form of expected free energy. Sophistication describes the degree to which an agent has beliefs about beliefs. We consider agents with beliefs about the counterfactual consequences of action for states of affairs and beliefs about those latent states. In other words, we move from simply considering beliefs about ``what would happen if I did that'' to ``what I would believe about what would happen if I did that.'' The recursive form of the free energy functional effectively implements a deep tree search over actions and outcomes in the future. Crucially, this search is over sequences of belief states as opposed to states per se. We illustrate the competence of this scheme using numerical simulations of deep decision problems.},
  file = {/Users/daniekru/Zotero/storage/LBPMMUL3/Friston et al. - 2021 - Sophisticated Inference.pdf;/Users/daniekru/Zotero/storage/T599CVPJ/Sophisticated-Inference.html}
}

@article{fuchsbergerModulationHippocampalPlasticity2022,
  title = {Modulation of Hippocampal Plasticity in Learning and Memory},
  author = {Fuchsberger, Tanja and Paulsen, Ole},
  year = {2022},
  month = aug,
  journal = {Current Opinion in Neurobiology},
  volume = {75},
  pages = {102558},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2022.102558},
  urldate = {2023-02-01},
  abstract = {Synaptic plasticity plays a central role in the study of neural mechanisms of learning and memory. Plasticity rules are not invariant over time but are under neuromodulatory control, enabling behavioral states to influence memory formation. Neuromodulation controls synaptic plasticity at network level by directing information flow, at circuit level through changes in excitation/inhibition balance, and at synaptic level through modulation of intracellular signaling cascades. Although most research has focused on modulation of principal neurons, recent progress has uncovered important roles for interneurons in not only routing information, but also setting conditions for synaptic plasticity. Moreover, astrocytes have been shown to both gate and mediate plasticity. These additional mechanisms must be considered for a comprehensive mechanistic understanding of learning and memory.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/TLKZRXE6/Fuchsberger and Paulsen - 2022 - Modulation of hippocampal plasticity in learning a.pdf;/Users/daniekru/Zotero/storage/26AVFUAK/S0959438822000526.html}
}

@article{funahashiPrefrontalContributionDecisionMaking2017,
  title = {Prefrontal {{Contribution}} to {{Decision-Making}} under {{Free-Choice Conditions}}},
  author = {Funahashi, Shintaro},
  year = {2017},
  month = jul,
  journal = {Frontiers in Neuroscience},
  volume = {11},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2017.00431},
  urldate = {2024-11-27},
  abstract = {{$<$}p{$>$}Executive function is thought to be the coordinated operation of multiple neural processes and allows to accomplish a current goal flexibly. The most important function of the prefrontal cortex is the executive function. Among a variety of executive functions in which the prefrontal cortex participates, decision-making is one of the most important. Although the prefrontal contribution to decision-making has been examined using a variety of behavioral tasks, recent studies using fMRI have shown that the prefrontal cortex participates in decision-making under free-choice conditions. Since decision-making under free-choice conditions represents the very first stage for any kind of decision-making process, it is important that we understand its neural mechanism. Although few studies have examined this issue while a monkey performed a free-choice task, those studies showed that, when the monkey made a decision to subsequently choose one particular option, prefrontal neurons showing selectivity to that option exhibited transient activation just {$<$}italic{$>$}before{$<$}/italic{$>$} presentation of the imperative cue. Further studies have suggested that this transient increase is caused by the irregular fluctuation of spontaneous firing just {$<$}italic{$>$}before{$<$}/italic{$>$} cue presentation, which enhances the response to the cue and biases the strength of the neuron's selectivity to the option. In addition, this biasing effect was observed only in neurons that exhibited sustained delay-period activity, indicating that this biasing effect not only influences the animal's decision for an upcoming choice, but also is linked to working memory mechanisms in the prefrontal cortex.{$<$}/p{$>$}},
  langid = {english},
  keywords = {choice-predictive activity,decision-making,free-choice,Prefrontal Cortex,Spontaneous fluctuation},
  file = {/Users/daniekru/Zotero/storage/T8HE6XRW/Funahashi - 2017 - Prefrontal Contribution to Decision-Making under Free-Choice Conditions.pdf}
}

@article{gallinaroSynapticWeightsThat2023,
  title = {Synaptic Weights That Correlate with Presynaptic Selectivity Increase Decoding Performance},
  author = {Gallinaro, J{\'u}lia V. and Scholl, Benjamin and Clopath, Claudia},
  year = {2023},
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {19},
  number = {8},
  pages = {e1011362},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1011362},
  urldate = {2024-12-09},
  abstract = {The activity of neurons in the visual cortex is often characterized by tuning curves, which are thought to be shaped by Hebbian plasticity during development and sensory experience. This leads to the prediction that neural circuits should be organized such that neurons with similar functional preference are connected with stronger weights. In support of this idea, previous experimental and theoretical work have provided evidence for a model of the visual cortex characterized by such functional subnetworks. A recent experimental study, however, have found that the postsynaptic preferred stimulus was defined by the total number of spines activated by a given stimulus and independent of their individual strength. While this result might seem to contradict previous literature, there are many factors that define how a given synaptic input influences postsynaptic selectivity. Here, we designed a computational model in which postsynaptic functional preference is defined by the number of inputs activated by a given stimulus. Using a plasticity rule where synaptic weights tend to correlate with presynaptic selectivity, and is independent of functional-similarity between pre- and postsynaptic activity, we find that this model can be used to decode presented stimuli in a manner that is comparable to maximum likelihood inference., Brains are composed of complex networks, with communication taking place along synaptic connections between neurons. These connections can change and adapt, a process we call ``plasticity''. However, the specific rules that dictate these changes remain largely unknown. In our visual system, which is responsible for vision and perception, it is primarily thought that connections get stronger between neurons exhibiting similar activity, also known as `Hebbian plasticity'. A recent study revealed results that seemed to contradict this idea, showing a strength in numbers of synapses rather than strength. Prompted by these findings, we developed a computational model with a hypothesis about how these changes could occur. We discovered that this new model, with a plasticity mechanism based on presynaptic activity, could capture experimental findings and lead to benefits in population decoding. Our model doesn't necessarily contradict a Hebbian model, but rather, likely co-exists with it.},
  pmcid = {PMC10434873},
  pmid = {37549193},
  file = {/Users/daniekru/Zotero/storage/Z3CGADG6/Gallinaro et al. - 2023 - Synaptic weights that correlate with presynaptic selectivity increase decoding performance.pdf}
}

@article{gallistelComputationsMetricMaps1996,
  title = {Computations on {{Metric Maps}} in {{Mammals}}: {{Getting Oriented}} and {{Choosing}} a {{Multi-Destination Route}}},
  shorttitle = {Computations on {{Metric Maps}} in {{Mammals}}},
  author = {Gallistel, C. R. and Cramer, Audrey E.},
  year = {1996},
  month = jan,
  journal = {Journal of Experimental Biology},
  volume = {199},
  number = {1},
  pages = {211--217},
  issn = {0022-0949},
  doi = {10.1242/jeb.199.1.211},
  urldate = {2025-02-22},
  abstract = {The capacity to construct a cognitive map is hypothesized to rest on two foundations: (1) dead reckoning (path integration); (2) the perception of the direction and distance of terrain features relative to the animal. A map may be constructed by combining these two sources of positional information, with the result that the positions of all terrain features are represented in the coordinate framework used for dead reckoning. When animals need to become reoriented in a mapped space, results from rats and human toddlers indicate that they focus exclusively on the shape of the perceived environment, ignoring non-geometric features such as surface colors. As a result, in a rectangular space, they are misoriented half the time even when the two ends of the space differ strikingly in their appearance. In searching for a hidden object after becoming reoriented, both kinds of subjects search on the basis of the object's mapped position in the space rather than on the basis of its relationship to a goal sign (e.g. a distinctive container or nearby marker), even though they have demonstrably noted the relationship between the goal and the goal sign. When choosing a multi-destination foraging route, vervet monkeys look at least three destinations ahead, even though they are only capable of keeping a maximum of six destinations in mind at once.},
  file = {/Users/daniekru/Zotero/storage/AH88GV6E/Computations-on-Metric-Maps-in-Mammals-Getting.html}
}

@article{gallistelComputationsMetricMaps1996a,
  title = {Computations on {{Metric Maps}} in {{Mammals}}: {{Getting Oriented}} and {{Choosing}} a {{Multi-Destination Route}}},
  shorttitle = {Computations on {{Metric Maps}} in {{Mammals}}},
  author = {Gallistel, C. R. and Cramer, Audrey E.},
  year = {1996},
  month = jan,
  journal = {Journal of Experimental Biology},
  volume = {199},
  number = {1},
  pages = {211--217},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.199.1.211},
  urldate = {2025-02-24},
  abstract = {The capacity to construct a cognitive map is hypothesized to rest on two foundations: (1) dead reckoning (path integration); (2) the perception of the direction and distance of terrain features relative to the animal. A map may be constructed by combining these two sources of positional information, with the result that the positions of all terrain features are represented in the coordinate framework used for dead reckoning. When animals need to become reoriented in a mapped space, results from rats and human toddlers indicate that they focus exclusively on the shape of the perceived environment, ignoring non-geometric features such as surface colors. As a result, in a rectangular space, they are misoriented half the time even when the two ends of the space differ strikingly in their appearance. In searching for a hidden object after becoming reoriented, both kinds of subjects search on the basis of the object's mapped position in the space rather than on the basis of its relationship to a goal sign (e.g. a distinctive container or nearby marker), even though they have demonstrably noted the relationship between the goal and the goal sign. When choosing a multidestination foraging route, vervet monkeys look at least three destinations ahead, even though they are only capable of keeping a maximum of six destinations in mind at once.},
  copyright = {http://www.biologists.com/user-licence-1-1/},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/HPSLJK6T/Gallistel and Cramer - 1996 - Computations on Metric Maps in Mammals Getting Oriented and Choosing a Multi-Destination Route.pdf}
}

@misc{garivierUpperConfidenceBoundPolicies2008,
  title = {On {{Upper-Confidence Bound Policies}} for {{Non-Stationary Bandit Problems}}},
  author = {Garivier, Aur{\'e}lien and Moulines, Eric},
  year = {2008},
  month = may,
  number = {arXiv:0805.3415},
  eprint = {0805.3415},
  primaryclass = {math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.0805.3415},
  urldate = {2024-03-25},
  abstract = {Multi-armed bandit problems are considered as a paradigm of the trade-off between exploring the environment to find profitable actions and exploiting what is already known. In the stationary case, the distributions of the rewards do not change in time, Upper-Confidence Bound (UCB) policies have been shown to be rate optimal. A challenging variant of the MABP is the non-stationary bandit problem where the gambler must decide which arm to play while facing the possibility of a changing environment. In this paper, we consider the situation where the distributions of rewards remain constant over epochs and change at unknown time instants. We analyze two algorithms: the discounted UCB and the sliding-window UCB. We establish for these two algorithms an upper-bound for the expected regret by upper-bounding the expectation of the number of times a suboptimal arm is played. For that purpose, we derive a Hoeffding type inequality for self normalized deviations with a random number of summands. We establish a lower-bound for the regret in presence of abrupt changes in the arms reward distributions. We show that the discounted UCB and the sliding-window UCB both match the lower-bound up to a logarithmic factor.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/daniekru/Zotero/storage/XPLJKI2D/Garivier and Moulines - 2008 - On Upper-Confidence Bound Policies for Non-Station.pdf;/Users/daniekru/Zotero/storage/65CD88JS/0805.html}
}

@misc{garneloConditionalNeuralProcesses2018,
  title = {Conditional {{Neural Processes}}},
  author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J. and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J. and Eslami, S. M. Ali},
  year = {2018},
  month = jul,
  number = {arXiv:1807.01613},
  eprint = {1807.01613},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.01613},
  urldate = {2022-09-08},
  abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/GRBPMWJ3/Garnelo et al. - 2018 - Conditional Neural Processes.pdf;/Users/daniekru/Zotero/storage/NW7BJ4LT/1807.html}
}

@article{garvertHippocampalSpatiopredictiveCognitive2023,
  title = {Hippocampal Spatio-Predictive Cognitive Maps Adaptively Guide Reward Generalization},
  author = {Garvert, Mona M. and Saanum, Tankred and Schulz, Eric and Schuck, Nicolas W. and Doeller, Christian F.},
  year = {2023},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {26},
  number = {4},
  pages = {615--626},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-023-01283-x},
  urldate = {2025-02-22},
  abstract = {The brain forms cognitive maps of relational knowledge---an organizing principle thought to underlie our ability to generalize and make inferences. However, how can a relevant map be selected in situations where a stimulus is embedded in multiple relational structures? Here, we find that both spatial and predictive cognitive maps influence generalization in a choice task, where spatial location determines reward magnitude. Mirroring behavior, the hippocampus not only builds a map of spatial relationships but also encodes the experienced transition structure. As the task progresses, participants' choices become more influenced by spatial relationships, reflected in a strengthening of the spatial map and a weakening of the predictive map. This change is driven by orbitofrontal cortex, which represents the degree to which an outcome is consistent with the spatial rather than the predictive map and updates hippocampal representations accordingly. Taken together, this demonstrates how hippocampal cognitive maps are used and updated flexibly for inference.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Decision,Hippocampus,Learning algorithms},
  file = {/Users/daniekru/Zotero/storage/U86ULFIX/Garvert et al. - 2023 - Hippocampal spatio-predictive cognitive maps adaptively guide reward generalization.pdf}
}

@article{gaspariniInitiationPropagationDendritic2004,
  title = {On the {{Initiation}} and {{Propagation}} of {{Dendritic Spikes}} in {{CA1 Pyramidal Neurons}}},
  author = {Gasparini, S.},
  year = {2004},
  month = dec,
  journal = {Journal of Neuroscience},
  volume = {24},
  number = {49},
  pages = {11046--11056},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2520-04.2004},
  urldate = {2020-05-22},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZPWH29YU/Gasparini - 2004 - On the Initiation and Propagation of Dendritic Spi.pdf}
}

@article{georgeClonestructuredGraphRepresentations2021,
  title = {Clone-Structured Graph Representations Enable Flexible Learning and Vicarious Evaluation of Cognitive Maps},
  author = {George, Dileep and Rikhye, Rajeev and Gothoskar, Nishad and Guntupalli, Jyothi Swaroop and Dedieu, Antoine and {L{\'a}zaro-Gredilla}, Miguel},
  year = {2021},
  month = apr,
  journal = {Nature Communications},
  volume = {12},
  doi = {10.1038/s41467-021-22559-5},
  abstract = {Cognitive maps are mental representations of spatial and conceptual relationships in an environment, and are critical for flexible behavior. To form these abstract maps, the hippocampus has to learn to separate or merge aliased observations appropriately in different contexts in a manner that enables generalization and efficient planning. Here we propose a specific higher-order graph structure, clone-structured cognitive graph (CSCG), which forms clones of an observation for different contexts as a representation that addresses these problems. CSCGs can be learned efficiently using a probabilistic sequence model that is inherently robust to uncertainty. We show that CSCGs can explain a variety of cognitive map phenomena such as discovering spatial relations from aliased sensations, transitive inference between disjoint episodes, and formation of transferable schemas. Learning different clones for different contexts explains the emergence of splitter cells observed in maze navigation and event-specific responses in lap-running experiments. Moreover, learning and inference dynamics of CSCGs offer a coherent explanation for disparate place cell remapping phenomena. By lifting aliased observations into a hidden space, CSCGs reveal latent modularity useful for hierarchical abstraction and planning. Altogether, CSCG provides a simple unifying framework for understanding hippocampal function, and could be a pathway for forming relational abstractions in artificial intelligence.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/9IZUNZ2E/George et al. - 2021 - Clone-structured graph representations enable flex.pdf}
}

@book{georgeGenerativeModelHippocampal2023,
  title = {A Generative Model of the Hippocampal Formation Trained with Theta Driven Local Learning Rules},
  author = {George, Tom and Barry, Caswell and Stachenfeld, Kimberly and Clopath, Claudia and Fukai, Tomoki},
  year = {2023},
  month = dec,
  doi = {10.1101/2023.12.12.571268},
  abstract = {Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/XTW3WVP3/George et al. - 2023 - A generative model of the hippocampal formation tr.pdf}
}

@article{gershmanMolecularMemoryCode2023,
  title = {The Molecular Memory Code and Synaptic Plasticity: {{A}} Synthesis},
  shorttitle = {The Molecular Memory Code and Synaptic Plasticity},
  author = {Gershman, Samuel J.},
  year = {2023},
  month = feb,
  journal = {Biosystems},
  volume = {224},
  pages = {104825},
  issn = {0303-2647},
  doi = {10.1016/j.biosystems.2022.104825},
  urldate = {2023-04-20},
  abstract = {The most widely accepted view of memory in the brain holds that synapses are the storage sites of memory, and that memories are formed through associative modification of synapses. This view has been challenged on conceptual and empirical grounds. As an alternative, it has been proposed that molecules within the cell body are the storage sites of memory, and that memories are formed through biochemical operations on these molecules. This paper proposes a synthesis of these two views, grounded in a computational model of memory. Synapses are conceived as storage sites for the parameters of an approximate posterior probability distribution over latent causes. Intracellular molecules are conceived as storage sites for the parameters of a generative model. The model stipulates how these two components work together as part of an integrated algorithm for learning and inference.},
  langid = {english},
  keywords = {Free energy,Inference,Learning,Memory,Synaptic plasticity,to study},
  file = {/Users/daniekru/Zotero/storage/GZB8ZJWT/Gershman - 2023 - The molecular memory code and synaptic plasticity.pdf;/Users/daniekru/Zotero/storage/M4JWP2NM/S0303264722002064.html}
}

@article{gershmanWhatDoesFree2019,
  title = {What Does the Free Energy Principle Tell Us about the Brain?},
  author = {Gershman, Samuel J.},
  year = {2019},
  month = oct,
  journal = {arXiv:1901.07945 [q-bio]},
  eprint = {1901.07945},
  primaryclass = {q-bio},
  urldate = {2020-06-08},
  abstract = {The free energy principle has been proposed as a unifying account of brain function. It is closely related, and in some cases subsumes, earlier unifying ideas such as Bayesian inference, predictive coding, and active learning. This article clarifies these connections, teasing apart distinctive and shared predictions.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/Users/daniekru/Zotero/storage/XUQUWA24/Gershman - 2019 - What does the free energy principle tell us about .pdf;/Users/daniekru/Zotero/storage/8YBEB7WL/1901.html}
}

@incollection{gerstnerChapter12Framework2001,
  title = {Chapter 12 {{A}} Framework for Spiking Neuron Models: {{The}} Spike Response Model},
  shorttitle = {Chapter 12 {{A}} Framework for Spiking Neuron Models},
  booktitle = {Handbook of {{Biological Physics}}},
  author = {Gerstner, W.},
  editor = {Moss, F. and Gielen, S.},
  year = {2001},
  month = jan,
  series = {Neuro-{{Informatics}} and {{Neural Modelling}}},
  volume = {4},
  pages = {469--516},
  publisher = {North-Holland},
  doi = {10.1016/S1383-8121(01)80015-4},
  urldate = {2024-05-13},
  abstract = {This chapter addresses both issues from the systematic point of view of a response kernel expansion. Spike generation in the Hodgkin--Huxley model can be reproduced to a high degree of accuracy by a single-variable threshold model [19]. The problem of spatial structure is studied for a multicompartmental integrate-and-fire model with a passive dendritic tree and active currents at the soma. In this case, the model dynamics can be solved and systematically reduced to a single-variable model with response kernels. The chapter reviews the standard Hodgkin--Huxley model. The four differential equations of Hodgkin and Huxley give an accurate description of neuronal spiking in the giant axon of the squid. The chapter proposes a method based on spike response kernels and provides a biologically transparent description of the essential effects during spiking. The chapter discusses the spike response model (SRM), derived from the Hodgkin--Huxley model by the SRM, can reproduce up to 90\% of the spike times of the Hodkgin--Huxley model correctly.},
  file = {/Users/daniekru/Zotero/storage/3N3QDFLA/Gerstner - 2001 - Chapter 12 A framework for spiking neuron models .pdf;/Users/daniekru/Zotero/storage/HQVYDAW9/S1383812101800154.html}
}

@article{gerstnerEligibilityTracesPlasticity2018,
  title = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}: {{Experimental Support}} of {{NeoHebbian Three-Factor Learning Rules}}},
  shorttitle = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}},
  author = {Gerstner, W. and Lehmann, M. and Liakoni, V. and Corneil, D. and Brea, J.},
  year = {2018},
  journal = {Frontiers in Neural Circuits},
  volume = {12},
  issn = {1662-5110},
  doi = {10.3389/fncir.2018.00053},
  abstract = {Most elementary behaviors such as moving the arm to grasp an object or walking into the next room to explore a museum evolve on the time scale of seconds; in contrast, neuronal action potentials occur on the time scale of a few milliseconds. Learning rules of the brain must therefore bridge the gap between these two different time scales. Modern theories of synaptic plasticity have postulated that the co-activation of pre- and postsynaptic neurons sets a flag at the synapse, called an eligibility trace, that leads to a weight change only if an additional factor is present while the flag is set. This third factor, signaling reward, punishment, surprise, or novelty, could be implemented by the phasic activity of neuromodulators or specific neuronal inputs signaling special events. While the theoretical framework has been developed over the last decades, experimental evidence in support of eligibility traces on the time scale of seconds has been collected only during the last few years. Here we review, in the context of three-factor rules of synaptic plasticity, four key experiments that support the role of synaptic eligibility traces in combination with a third factor as a biological implementation of neoHebbian three-factor learning rules. {\copyright} 2018 Gerstner, Lehmann, Liakoni, Corneil and Brea.},
  langid = {english},
  keywords = {Behavioral learning,Eligibility trace,Hebb rule,Neuromodulators,Reinforcement learning,Surprise,Synaptic plasticity,Synaptic tagging},
  file = {/Users/daniekru/Zotero/storage/9XULZDKB/Gerstner et al. - 2018 - Eligibility Traces and Plasticity on Behavioral Ti.pdf;/Users/daniekru/Zotero/storage/HH9US59R/display.html}
}

@article{gerstnerMathematicalFormulationsHebbian2002,
  title = {Mathematical Formulations of {{Hebbian}} Learning},
  author = {Gerstner, Wulfram and Kistler, Werner M.},
  year = {2002},
  month = dec,
  journal = {Biological Cybernetics},
  volume = {87},
  number = {5-6},
  pages = {404--415},
  issn = {03401200},
  doi = {10.1007/s00422-002-0353-y},
  urldate = {2022-11-18},
  abstract = {Several formulations of correlation-based Hebbian learning are reviewed. On the presynaptic side, activity is described either by a firing rate or by presynaptic spike arrival. The state of the postsynaptic neuron can be described by its membrane potential, its firing rate, or the timing of backpropagating action potentials (BPAPs). It is shown that all of the above formulations can be derived from the point of view of an expansion. In the absence of BPAPs, it is natural to correlate presynaptic spikes with the postsynaptic membrane potential. Time windows of spike-time-dependent plasticity arise naturally if the timing of postsynaptic spikes is available at the site of the synapse, as is the case in the presence of BPAPs. With an appropriate choice of parameters, Hebbian synaptic plasticity has intrinsic normalization properties that stabilizes postsynaptic firing rates and leads to subtractive weight normalization.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/JM6SZKGW/Gerstner and Kistler - 2002 - Mathematical formulations of Hebbian learning.pdf}
}

@article{giffordPersonalSocialFactors2014,
  title = {Personal and Social Factors That Influence Pro-Environmental Concern and Behaviour: {{A}} Review},
  shorttitle = {Personal and Social Factors That Influence Pro-Environmental Concern and Behaviour},
  author = {Gifford, Robert and Nilsson, Andreas},
  year = {2014},
  journal = {International Journal of Psychology},
  volume = {49},
  number = {3},
  pages = {141--157},
  issn = {1464-066X},
  doi = {10.1002/ijop.12034},
  urldate = {2018-12-16},
  abstract = {We review the personal and social influences on pro-environmental concern and behaviour, with an emphasis on recent research. The number of these influences suggests that understanding pro-environmental concern and behaviour is far more complex than previously thought. The influences are grouped into 18 personal and social factors. The personal factors include childhood experience, knowledge and education, personality and self-construal, sense of control, values, political and world views, goals, felt responsibility, cognitive biases, place attachment, age, gender and chosen activities. The social factors include religion, urban--rural differences, norms, social class, proximity to problematic environmental sites and cultural and ethnic variations We also recognize that pro-environmental behaviour often is undertaken based on none of the above influences, but because individuals have non-environmental goals such as to save money or to improve their health. Finally, environmental outcomes that are a result of these influences undoubtedly are determined by combinations of the 18 categories. Therefore, a primary goal of researchers now should be to learn more about how these many influences moderate and mediate one another to determine pro-environmental behaviour.},
  copyright = {{\copyright} 2014 International Union of Psychological Science},
  langid = {english},
  keywords = {Personal factors,Pro-environmental behaviour,Pro-environmental concern,Review,Social factors},
  file = {/Users/daniekru/Zotero/storage/YJWNPWKR/ijop.html}
}

@article{gillnerNavigationAcquisitionSpatial1998,
  title = {Navigation and {{Acquisition}} of {{Spatial Knowledge}} in a {{Virtual Maze}}},
  author = {Gillner, Sabine and Mallot, Hanspeter A.},
  year = {1998},
  month = jul,
  journal = {Journal of Cognitive Neuroscience},
  volume = {10},
  number = {4},
  pages = {445--463},
  issn = {0898-929X},
  doi = {10.1162/089892998562861},
  urldate = {2025-02-23},
  abstract = {Spatial behavior in humans and animals includes a wide variety of behavioral competences and makes use of a large number of sensory cues. Here we studied the ability of human subjects to search locations, to find shortcuts and novel paths, to estimate distances between remembered places, and to draw sketch maps of the explored environment; these competences are related to goal-independent memory of space, or cognitive maps. Information on spatial relations was restricted to two types: a visual motion sequence generated by simulated movements in a virtual maze and the subject's own movement decisions defining the path through the maze. Visual information was local (i.e., no global landmarks or compass information was provided). Other position and movement information (vestibular or proprioceptive) was excluded. The amount of visual information provided was varied over four experimental conditions. The results indicate that human subjects are able to learn a virtual maze from sequences of local views and movements. The information acquired is local, consisting of recognized positions and movement decisions associated to them. Although simple associations of this type can be shown to be present in some subjects, more complete configurational knowledge is acquired as well. The results are discussed in a view-based framework of navigation and the representation of spatial knowledge by means of a view graph.},
  file = {/Users/daniekru/Zotero/storage/QAULRUM3/Gillner and Mallot - 1998 - Navigation and Acquisition of Spatial Knowledge in a Virtual Maze.pdf;/Users/daniekru/Zotero/storage/XQA7ZU5I/Navigation-and-Acquisition-of-Spatial-Knowledge-in.html}
}

@article{ginosarAreGridCells2023,
  title = {Are Grid Cells Used for Navigation? {{On}} Local Metrics, Subjective Spaces, and Black Holes},
  shorttitle = {Are Grid Cells Used for Navigation?},
  author = {Ginosar, Gily and Aljadeff, Johnatan and Las, Liora and Derdikman, Dori and Ulanovsky, Nachum},
  year = {2023},
  month = jun,
  journal = {Neuron},
  volume = {111},
  number = {12},
  pages = {1858--1875},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2023.03.027},
  urldate = {2023-10-24},
  abstract = {The symmetric, lattice-like spatial pattern of grid-cell activity is thought to provide a neuronal global metric for space. This view is compatible with grid cells recorded in empty boxes but inconsistent with data from more naturalistic settings. We review evidence arguing against the global-metric notion, including the distortion and disintegration of the grid pattern in complex and three-dimensional environments. We argue that deviations from lattice symmetry are key for understanding grid-cell function. We propose three possible functions for grid cells, which treat real-world grid distortions as a feature rather than a bug. First, grid cells may constitute a local metric for proximal space rather than a global metric for all space. Second, grid cells could form a metric for subjective action-relevant space rather than physical space. Third, distortions may represent salient locations. Finally, we discuss mechanisms that can underlie these functions. These ideas may transform our thinking about grid cells.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/4AIW2UZ9/Ginosar et al. - 2023 - Are grid cells used for navigation On local metri.pdf;/Users/daniekru/Zotero/storage/ZHP4X8RI/S0896627323002234.html}
}

@article{gittinsBanditProcessesDynamic1979,
  title = {Bandit {{Processes}} and {{Dynamic Allocation Indices}}},
  author = {Gittins, J. C.},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {41},
  number = {2},
  eprint = {2985029},
  eprinttype = {jstor},
  pages = {148--177},
  abstract = {The paperaimsto givea unifiedaccountofthecentracl onceptsin recentworkon banditprocessesand dynamicallocationindices;to showhow thesereducesome previouslyintractablperoblemsto theproblemof calculatingsuchindices;and to describehow thesecalculationsmay be carriedout. Applicationsto stochastic schedulings,equentialclinicaltrialsand a class of searchproblemsare discussed.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FYGDZ4DT/Gittins - 1979 - Bandit Processes and Dynamic Allocation Indices.pdf}
}

@article{goldingDendriticCalciumSpike1999,
  title = {Dendritic {{Calcium Spike Initiation}} and {{Repolarization Are Controlled}} by {{Distinct Potassium Channel Subtypes}} in {{CA1 Pyramidal Neurons}}},
  author = {Golding, Nace L. and Jung, Hae-yoon and Mickus, Timothy and Spruston, Nelson},
  year = {1999},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {19},
  number = {20},
  pages = {8789--8798},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.19-20-08789.1999},
  urldate = {2023-02-07},
  abstract = {In CA1 pyramidal neurons of the hippocampus, calcium-dependent spikes occur in vivo during specific behavioral states and may be enhanced during epileptiform activity. However, the mechanisms that control calcium spike initiation and repolarization are poorly understood. Using dendritic and somatic patch-pipette recordings, we show that calcium spikes are initiated in the apical dendrites of CA1 pyramidal neurons and drive bursts of sodium-dependent action potentials at the soma. Initiation of calcium spikes at the soma was suppressed in part by potassium channels activated by sodium-dependent action potentials. Low-threshold, putative D-type potassium channels [blocked by 100 {$\mu$}m 4-aminopyridine (4-AP) and 0.5--1 {$\mu$}m {$\alpha$}-dendrotoxin ({$\alpha$}-DTX)] played a prominent role in setting a high threshold for somatic calcium spikes, thus restricting initiation to the dendrites. DTX- and 4-AP-sensitive channels were activated during sodium-dependent action potentials and mediated a large component of their afterhyperpolarization. Once initiated, repetitive firing of calcium spikes was limited by activation of putative BK-type calcium-activated potassium channels (blocked by 250 {$\mu$}m tetraethylammonium chloride, 70 nm charybdotoxin, or 100 nmiberiotoxin). Thus, the concerted action of calcium- and voltage-activated potassium channels serves to focus spatially and temporally the membrane depolarization and calcium influx generated by calcium spikes during strong, synchronous network excitation.},
  chapter = {ARTICLE},
  copyright = {Copyright {\copyright} 1999 Society for Neuroscience},
  langid = {english},
  pmid = {10516298},
  keywords = {4-AP,BK channels,bursting,calcium spike,dendrotoxin,hippocampus,TEA},
  file = {/Users/daniekru/Zotero/storage/YC6V7ZHV/Golding et al. - 1999 - Dendritic Calcium Spike Initiation and Repolarizat.pdf}
}

@article{goldingDendriticSpikesMechanism2002,
  title = {Dendritic Spikes as a Mechanism for Cooperative Long-Term Potentiation},
  author = {Golding, Nace L. and Staff, Nathan P. and Spruston, Nelson},
  year = {2002},
  month = jul,
  journal = {Nature},
  volume = {418},
  number = {6895},
  pages = {326--331},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature00854},
  urldate = {2020-05-22},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/EWJZNG8Z/Golding et al. - 2002 - Dendritic spikes as a mechanism for cooperative lo.pdf}
}

@incollection{goldman-rakicCorticalDopamineSystem1997,
  title = {The {{Cortical Dopamine System}}: {{Role}} in {{Memory}} and {{Cognition}}},
  shorttitle = {The {{Cortical Dopamine System}}},
  booktitle = {Advances in {{Pharmacology}}},
  author = {{Goldman-Rakic}, Patricia S.},
  editor = {Goldstein, David S. and Eisenhofer, Graeme and McCarty, Richard},
  year = {1997},
  month = jan,
  volume = {42},
  pages = {707--711},
  publisher = {Academic Press},
  doi = {10.1016/S1054-3589(08)60846-7},
  urldate = {2022-10-27},
  abstract = {There are three distinct possible cellular mechanisms that have so far been identified for dopamine modulation of working memory function in the prefrontal cortex. These are: direct synaptic modulation of receptors on the distal dendrites and spines of pyramidal neurons, direct nonsynaptic modulation of pyramidal neurons, and indirect synaptic modulation of pyramidal neurons via feedforward inhibition from GABAergic interneurons. As the majority of the dopamine synapses appear to be formed on pyramidal neurons, dopamine axons are thus placed in direct contact with the major projection neurons of the prefrontal cortex. Nonsynaptic neurotransmission may be the more pervasive means of altering pyramidal cell activity, because numerous dopamine varicosities are observed in nonsynaptic relationship to cortical elements. Members of the D1 family of dopamine receptors have been found to be particularly prominent in the prefrontal cortex of primates, and both D1 and D5 receptor proteins have been localized to the distal dendrites and spines of pyramidal cells. A third mechanism of dopamine action is indirect, appearing to involve feedforward inhibition on pyramidal neurons from nonpyramidal neurons. The indirect action of dopamine on this circuit derives from the identification of dopamine synapses on nonpyramidal GABAergic neurons in the prefrontal cortex and the finding that the D4 member of the D2 family of dopamine receptors is localized postsynaptically on a subset of GABA interneurons. These findings provide suggestive evidence that feedforward inhibition may play a role in the construction of a memory field in prefrontal neurons. The different effects of dopamine may depend on the subtype of interneuron engaged, and future studies may indicate the basis for the differential modulation of interneurons by dopamine.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/2SQWYDB6/Goldman-Rakic - 1997 - The Cortical Dopamine System Role in Memory and C.pdf;/Users/daniekru/Zotero/storage/QY7EW8CL/S1054358908608467.html}
}

@article{golledgeCognitiveMapsSpatial2000,
  title = {Cognitive {{Maps}}, {{Spatial Abilities}}, and {{Human Wayfinding}}},
  author = {Golledge, Reginald and Jacobson, Dan and Kitchin, Rob and Blades, Mark},
  year = {2000},
  month = dec,
  journal = {GEOGRAPHICAL REVIEW OF JAPAN SERIES B},
  volume = {73},
  pages = {93--104},
  doi = {10.4157/grj1984b.73.93},
  abstract = {In this paper we discuss the relations between cognitive maps, spatial abilities and human wayfinding, particularly in the context of traveling without the use of sight. Initially we discuss the nature of cognitive maps and the process of cognitive mapping as mechanisms for developing person to object (egocentric) and object to object (allocentric) internal representations. Imperfections in encoding either relations can introduce imperfections in representations of environments in memory. This, together with individual differences in human spatial abilities, can result in data manipulations that produce error. When information stored in long term memory is brought into working memory for purposes of decision making and choice behavior (as in route selection), the result may be the selection of an inefficient or incorrect path. We explore the connection between environmental learning and cognitive maps in the context of learning a route in two different cultural environments-Belfast (Northern Ireland) and Santa Barbara (California). Blind, vision impaired and sighted volunteers traveled and learned routes of approximately the same length (1.2 miles) in their respective urban environments. An initial trial was experimenter guided: three following trials were regarded as "test" trials where the participants learned the route and performed route fixing tasks including pointing between designated places, verbally describing the route after each completion, and building a model of the route using metallics strips on a magnetic board. Results indicated that by the end of the third test trial, and using the reinforcing strategies, the results of the blind or vision impaired participants could not be statistically differentiated from those of the sighted participants. This indicated that the wayfinding abilities of the three groups were equivalent in this experiments and suggested that spatial abilities were potentially the same in each group but that lack of sight interfered with putting knowledge into action.},
  file = {/Users/daniekru/Zotero/storage/2E9QAU62/Golledge et al. - 2000 - Cognitive Maps, Spatial Abilities, and Human Wayfinding.pdf}
}

@article{gordleevaModelingWorkingMemory2021,
  title = {Modeling {{Working Memory}} in a {{Spiking Neuron Network Accompanied}} by {{Astrocytes}}},
  author = {Gordleeva, Susanna Yu and Tsybina, Yuliya A. and Krivonosov, Mikhail I. and Ivanchenko, Mikhail V. and Zaikin, Alexey A. and Kazantsev, Victor B. and Gorban, Alexander N.},
  year = {2021},
  month = mar,
  journal = {Frontiers in Cellular Neuroscience},
  volume = {15},
  publisher = {Frontiers},
  issn = {1662-5102},
  doi = {10.3389/fncel.2021.631485},
  urldate = {2024-05-03},
  abstract = {We propose a novel biologically plausible computational model of working memory (WM) implemented by the spiking neuron network (SNN) interacting with a network of astrocytes. SNN is modelled by the synaptically coupled Izhikevich neurons with a non-specific architecture connection topology. Astrocytes generating calcium signals are connected by local gap junction diffusive couplings and interact with neurons by chemicals diffused in the extracellular space. Calcium elevations occur in response to the increased concentration of the neurotransmitter released by spiking neurons when a group of them fire coherently. In turn, gliotransmitters are released by activated astrocytes modulating the strengths of synaptic connections in the corresponding neuronal group. Input information is encoded as two-dimensional patterns of short applied current pulses stimulating neurons. The output is taken from frequencies of transient discharges of corresponding neurons. We show how a set of information patterns with quite significant overlapping areas can be uploaded into the neuron-astrocyte network and stored for several seconds. Information retrieval is organised by the application of a cue pattern representing the one from the memory set distorted by noise. We found that successful retrieval with the level of the correlation between the recalled pattern and ideal pattern exceeding 90\% is possible for multi-item WM task. Having analysed the dynamical mechanism of WM formation, we discovered that astrocytes operating at a time scale of a dozen of seconds can successfully store traces of neuronal activations corresponding to information patterns. In the retrieval stage, the astrocytic network selectively modulates synaptic connections in SNN leading to the successful recall. Information and dynamical characteristics of the proposed WM model agrees with classical concepts and other WM models.},
  langid = {english},
  keywords = {astrocyte,delayed activity,neuron-astrocyte interaction,Spiking Neural network,working memory},
  file = {/Users/daniekru/Zotero/storage/CLGHGHEZ/Gordleeva et al. - 2021 - Modeling Working Memory in a Spiking Neuron Networ.pdf}
}

@incollection{gotoPrefrontalCorticalSynaptic2007,
  title = {Prefrontal {{Cortical Synaptic Plasticity}}: {{The Roles}} of {{Dopamine}} and {{Implication}} for {{Schizophrenia}}},
  shorttitle = {Prefrontal {{Cortical Synaptic Plasticity}}},
  booktitle = {Monoaminergic {{Modulation}} of {{Cortical Excitability}}},
  author = {Goto, Yukiori and Otani, Satoru},
  editor = {Tseng, Kuei-Yuan and Atzori, Marco},
  year = {2007},
  pages = {165--174},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-0-387-72256-6_10},
  urldate = {2024-04-26},
  abstract = {The prefrontal cortex (PFC) is central in mediating executive functions in goaldirected behavior, for which proper dopamine (DA) actions of information processing modulation is essential in this area. It is now evident that, as in the case of the hippocampus, the PFC undergoes neuronal adaptation processes in its networks with induction of synaptic plasticity such as long-term potentiation (LTP) and short-term potentiation (STP). A prominent characteristic of synaptic plasticity in the PFC is that its induction mechanisms involve DA as an essential modulatory molecule. As such, DA-dependent plastic changes occurring in PFC network have important roles for PFC-mediated cognitive functions. Nevertheless, little attempt has been made to characterize the nature of PFC neuronal adaptation by synaptic plasticity, given that the PFC is thought to be the area of temporary storage and manipulation of information, known as working memory. However, accumulating evidences now indicate that the functions of the PFC cannot be fully explained just as the region of an online representation and handling of information. Importance of DA-dependent synaptic plasticity is further encouraged by possible disruption of synaptic plasticity mechanism in the PFC in psychiatric disorders such as schizophrenia, drug addiction, and depression.},
  isbn = {978-0-387-72256-6},
  langid = {english},
  keywords = {Prefrontal Cortex,Synaptic Plasticity,Tetanic Stimulation,Trace Fear Conditioning,Ventral Tegmental Area},
  file = {/Users/daniekru/Zotero/storage/5PNVFBP5/Goto and Otani - 2007 - Prefrontal Cortical Synaptic Plasticity The Roles.pdf}
}

@article{goyalRecurrentIndependentMechanisms2020,
  title = {Recurrent {{Independent Mechanisms}}},
  author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = nov,
  journal = {arXiv:1909.10893 [cs, stat]},
  eprint = {1909.10893},
  primaryclass = {cs, stat},
  urldate = {2021-12-02},
  abstract = {Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/BJV6PV6U/Goyal et al. - 2020 - Recurrent Independent Mechanisms.pdf;/Users/daniekru/Zotero/storage/GQJM6TJW/1909.html}
}

@article{grienbergerEntorhinalCortexDirects2022,
  title = {Entorhinal Cortex Directs Learning-Related Changes in {{CA1}} Representations},
  author = {Grienberger, Christine and Magee, Jeffrey C.},
  year = {2022},
  month = nov,
  journal = {Nature},
  volume = {611},
  number = {7936},
  pages = {554--562},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-022-05378-6},
  urldate = {2025-01-20},
  abstract = {Abstract                            Learning-related changes in brain activity are thought to underlie adaptive behaviours               1,2               . For instance, the learning of a reward site by rodents requires the development of an over-representation of that location in the hippocampus               3--6               . How this learning-related change occurs remains unknown. Here we recorded hippocampal CA1 population activity as mice learned a reward location on a linear treadmill. Physiological and pharmacological evidence suggests that the adaptive over-representation required behavioural timescale synaptic plasticity (BTSP)               7               . BTSP is known to be driven by dendritic voltage signals that we proposed were initiated by input from entorhinal cortex layer 3 (EC3). Accordingly, the CA1 over-representation was largely removed by optogenetic inhibition of EC3 activity. Recordings from EC3 neurons revealed an activity pattern that could provide an instructive signal directing BTSP to generate the over-representation. Consistent with this function, our observations show that exposure to a second environment possessing a prominent reward-predictive cue resulted in both EC3 activity and CA1 place field density that were more elevated at the cue than at the reward. These data indicate that learning-related changes in the hippocampus are produced by synaptic plasticity directed by an instructive signal from the EC3 that seems to be specifically adapted to the behaviourally relevant features of the environment.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/VI5FQBX6/Grienberger and Magee - 2022 - Entorhinal cortex directs learning-related changes in CA1 representations.pdf}
}

@article{guerguievDeepLearningSegregated2017,
  title = {Towards Deep Learning with Segregated Dendrites},
  author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  editor = {Latham, Peter},
  year = {2017},
  month = dec,
  journal = {eLife},
  volume = {6},
  pages = {e22901},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.22901},
  urldate = {2023-01-10},
  abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations---the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons.},
  keywords = {credit assignment,deep learning,dendritic morphology,feedback alignment,neocortex,target propagation,to study},
  file = {/Users/daniekru/Zotero/storage/UXVQAR9Z/Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites.pdf}
}

@article{guptaEmbodiedIntelligenceLearning2021,
  title = {Embodied Intelligence via Learning and Evolution},
  author = {Gupta, Agrim and Savarese, Silvio and Ganguli, Surya and {Fei-Fei}, Li},
  year = {2021},
  month = oct,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5721},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25874-z},
  urldate = {2024-01-19},
  abstract = {The intertwined processes of learning and evolution in complex environmental niches have resulted in a remarkable diversity of morphological forms. Moreover, many aspects of animal intelligence are deeply embodied in these evolved morphologies. However, the principles governing relations between environmental complexity, evolved morphology, and the learnability of intelligent control, remain elusive, because performing large-scale in silico experiments on evolution and learning is challenging. Here, we introduce Deep Evolutionary Reinforcement Learning (DERL): a computational framework which can evolve diverse agent morphologies to learn challenging locomotion and manipulation tasks in complex environments. Leveraging DERL we demonstrate several relations between environmental complexity, morphological intelligence and the learnability of control. First, environmental complexity fosters the evolution of morphological intelligence as quantified by the ability of a morphology to facilitate the learning of novel tasks. Second, we demonstrate a morphological Baldwin effect i.e., in our simulations evolution rapidly selects morphologies that learn faster, thereby enabling behaviors learned late in the lifetime of early ancestors to be expressed early in the descendants lifetime. Third, we suggest a mechanistic basis for the above relationships through the evolution of morphologies that are more physically stable and energy efficient, and can therefore facilitate learning and control.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science},
  file = {/Users/daniekru/Zotero/storage/RBCFTE7Q/Gupta et al. - 2021 - Embodied intelligence via learning and evolution.pdf}
}

@article{gurneyNewFrameworkCorticoStriatal2015,
  title = {A {{New Framework}} for {{Cortico-Striatal Plasticity}}: {{Behavioural Theory Meets In Vitro Data}} at the {{Reinforcement-Action Interface}}},
  shorttitle = {A {{New Framework}} for {{Cortico-Striatal Plasticity}}},
  author = {Gurney, Kevin N. and Humphries, Mark D. and Redgrave, Peter},
  editor = {Dayan, Peter},
  year = {2015},
  month = jan,
  journal = {PLoS Biology},
  volume = {13},
  number = {1},
  pages = {e1002034},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002034},
  urldate = {2024-05-06},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/Y7GF8RN2/Gurney et al. - 2015 - A New Framework for Cortico-Striatal Plasticity B.pdf}
}

@misc{guruRampingActivityMidbrain2020,
  title = {Ramping Activity in Midbrain Dopamine Neurons Signifies the Use of a Cognitive Map},
  author = {Guru, Akash and Seo, Changwoo and Post, Ryan J. and Kullakanda, Durga S. and Schaffer, Julia A. and Warden, Melissa R.},
  year = {2020},
  month = may,
  primaryclass = {New Results},
  pages = {2020.05.21.108886},
  publisher = {bioRxiv},
  doi = {10.1101/2020.05.21.108886},
  urldate = {2025-02-17},
  abstract = {Journeys to novel and familiar destinations employ different navigational strategies. The first drive to a new restaurant relies on map-based planning, but after repeated trips the drive is automatic and guided by local environmental cues1,2. Ventral striatal dopamine rises during navigation toward goals and reflects the spatial proximity and value of goals3, but the impact of experience, the neural mechanisms, and the functional significance of dopamine ramps are unknown4,5. Here, we used fiber photometry6--8 to record the evolution of activity in midbrain dopamine neurons as mice learned a variety of reward-seeking tasks, starting recordings before training had commenced and continuing daily for weeks. When mice navigated through space toward a goal, robust ramping activity in dopamine neurons appeared immediately -- after the first rewarded trial on the first training day in completely na{\"i}ve animals. In this task spatial cues were available to guide behavior, and although ramps were strong at first, they gradually faded away as training progressed. If instead mice learned to run a fixed distance on a stationary wheel for reward, a task that required an internal model of progress toward the goal, strong dopamine ramps persisted indefinitely. In a passive task in which a visible cue and reward moved together toward the mouse, ramps appeared and then faded over several days, but in an otherwise identical task with a stationary cue and reward ramps never appeared. Our findings provide strong evidence that ramping activity in midbrain dopamine neurons reflects the use of a cognitive map9,10 -- an internal model of the distance already covered and the remaining distance until the goal is reached. We hypothesize that dopamine ramps may be used to reinforce locations on the way to newly-discovered rewards in order to build a graded ventral striatal value landscape for guiding routine spatial behavior.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/KFMANJCK/Guru et al. - 2020 - Ramping activity in midbrain dopamine neurons signifies the use of a cognitive map.pdf}
}

@misc{guruRampingActivityMidbrain2020a,
  title = {Ramping Activity in Midbrain Dopamine Neurons Signifies the Use of a Cognitive Map},
  author = {Guru, Akash and Seo, Changwoo and Post, Ryan J. and Kullakanda, Durga S. and Schaffer, Julia A. and Warden, Melissa R.},
  year = {2020},
  month = may,
  primaryclass = {New Results},
  pages = {2020.05.21.108886},
  publisher = {bioRxiv},
  doi = {10.1101/2020.05.21.108886},
  urldate = {2025-02-17},
  abstract = {Journeys to novel and familiar destinations employ different navigational strategies. The first drive to a new restaurant relies on map-based planning, but after repeated trips the drive is automatic and guided by local environmental cues1,2. Ventral striatal dopamine rises during navigation toward goals and reflects the spatial proximity and value of goals3, but the impact of experience, the neural mechanisms, and the functional significance of dopamine ramps are unknown4,5. Here, we used fiber photometry6--8 to record the evolution of activity in midbrain dopamine neurons as mice learned a variety of reward-seeking tasks, starting recordings before training had commenced and continuing daily for weeks. When mice navigated through space toward a goal, robust ramping activity in dopamine neurons appeared immediately -- after the first rewarded trial on the first training day in completely na{\"i}ve animals. In this task spatial cues were available to guide behavior, and although ramps were strong at first, they gradually faded away as training progressed. If instead mice learned to run a fixed distance on a stationary wheel for reward, a task that required an internal model of progress toward the goal, strong dopamine ramps persisted indefinitely. In a passive task in which a visible cue and reward moved together toward the mouse, ramps appeared and then faded over several days, but in an otherwise identical task with a stationary cue and reward ramps never appeared. Our findings provide strong evidence that ramping activity in midbrain dopamine neurons reflects the use of a cognitive map9,10 -- an internal model of the distance already covered and the remaining distance until the goal is reached. We hypothesize that dopamine ramps may be used to reinforce locations on the way to newly-discovered rewards in order to build a graded ventral striatal value landscape for guiding routine spatial behavior.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ECE2Y25F/Guru et al. - 2020 - Ramping activity in midbrain dopamine neurons signifies the use of a cognitive map.pdf}
}

@article{haamCholinergicModulationHippocampal2017,
  title = {Cholinergic Modulation of the Hippocampal Region and Memory Function},
  author = {Haam, Juhee and Yakel, Jerrel L.},
  year = {2017},
  journal = {Journal of Neurochemistry},
  volume = {142},
  number = {S2},
  pages = {111--121},
  issn = {1471-4159},
  doi = {10.1111/jnc.14052},
  urldate = {2024-03-16},
  abstract = {Acetylcholine (ACh) plays an important role in memory function and has been implicated in aging-related dementia, in which the impairment of hippocampus-dependent learning strongly manifests. Cholinergic neurons densely innervate the hippocampus, mediating the formation of episodic as well as semantic memory. Here, we will review recent findings on acetylcholine's modulation of memory function, with a particular focus on hippocampus-dependent learning, and the circuits involved. In addition, we will discuss the complexity of ACh actions in memory function to better understand the physiological role of ACh in memory. This is an article for the special issue XVth International Symposium on Cholinergic Mechanisms.},
  copyright = {{\copyright} 2017 International Society for Neurochemistry},
  langid = {english},
  keywords = {ACh,AD,cholinergic,hippocampus,learning,memory,muscarinic,nicotinic,to study},
  file = {/Users/daniekru/Zotero/storage/568SWDQZ/Haam and Yakel - 2017 - Cholinergic modulation of the hippocampal region a.pdf;/Users/daniekru/Zotero/storage/I632LMUZ/jnc.html}
}

@article{halvagalCombinationHebbianPredictive2023,
  title = {The Combination of {{Hebbian}} and Predictive Plasticity Learns Invariant Object Representations in Deep Sensory Networks},
  author = {Halvagal, Manu Srinath and Zenke, Friedemann},
  year = {2023},
  month = oct,
  journal = {Nature Neuroscience},
  pages = {1--10},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-023-01460-y},
  urldate = {2023-10-17},
  abstract = {Recognition of objects from sensory stimuli is essential for survival. To that end, sensory networks in the brain must form object representations invariant to stimulus changes, such as size, orientation and context. Although Hebbian plasticity is known to shape sensory networks, it fails to create invariant object representations in computational models, raising the question of how the brain achieves such processing. In the present study, we show that combining Hebbian plasticity with a predictive form of plasticity leads to invariant representations in deep neural network models. We derive a local learning rule that generalizes to spiking neural networks and naturally accounts for several experimentally observed properties of synaptic plasticity, including metaplasticity and spike-timing-dependent plasticity. Finally, our model accurately captures neuronal selectivity changes observed in the primate inferotemporal cortex in response to altered visual experience. Thus, we provide a plausible normative theory emphasizing the importance of predictive plasticity mechanisms for successful representational learning.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Learning algorithms,Network models},
  file = {/Users/daniekru/Zotero/storage/L8HLP3S8/Halvagal and Zenke - 2023 - The combination of Hebbian and predictive plastici.pdf}
}

@article{hargreavesMajorDissociationMedial2005,
  title = {Major {{Dissociation Between Medial}} and {{Lateral Entorhinal Input}} to {{Dorsal Hippocampus}}},
  author = {Hargreaves, Eric L. and Rao, Geeta and Lee, Inah and Knierim, James J.},
  year = {2005},
  month = jun,
  journal = {Science},
  volume = {308},
  number = {5729},
  pages = {1792--1794},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1110449},
  urldate = {2021-03-05},
  abstract = {Hippocampal place cells are a model system of how the brain constructs cognitive representations and of how these representations support complex behavior, learning, and memory. There is, however, a lack of detailed knowledge about the properties of hippocampal afferents. We recorded multiple single units from the hippocampus and the medial and lateral entorhinal areas of behaving rats. Although many medial entorhinal neurons had highly specific place fields, lateral entorhinal neurons displayed weak spatial specificity. This finding demonstrates a fundamental dissociation between the information conveyed to the hippocampus by its major input streams, with spatial information represented by the medial and nonspatial information represented by the lateral entorhinal cortex. A brain center known to store short-term memories integrates spatial and nonspatial information received separately from adjacent cortices. A brain center known to store short-term memories integrates spatial and nonspatial information received separately from adjacent cortices.},
  chapter = {Report},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {15961670},
  file = {/Users/daniekru/Zotero/storage/T9VNVGLG/Hargreaves et al. - 2005 - Major Dissociation Between Medial and Lateral Ento.pdf;/Users/daniekru/Zotero/storage/DR45M7L4/1792.html}
}

@article{harleAlteredStatisticalLearning2015,
  title = {Altered {{Statistical Learning}} and {{Decision-Making}} in {{Methamphetamine Dependence}}: {{Evidence}} from a {{Two-Armed Bandit Task}}},
  shorttitle = {Altered {{Statistical Learning}} and {{Decision-Making}} in {{Methamphetamine Dependence}}},
  author = {Harl{\'e}, Katia M. and Zhang, Shunan and Schiff, Max and Mackey, Scott and Paulus, Martin P. and Yu, Angela J.},
  year = {2015},
  month = dec,
  journal = {Frontiers in Psychology},
  volume = {6},
  pages = {1910},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.01910},
  urldate = {2024-04-29},
  abstract = {Understanding how humans weigh long-term and short-term goals is important for both basic cognitive science and clinical neuroscience, as substance users need to balance the appeal of an immediate high vs. the long-term goal of sobriety. We use a computational model to identify learning and decision-making abnormalities in methamphetamine-dependent individuals (MDI, n = 16) vs. healthy control subjects (HCS, n = 16), in a two-armed bandit task. In this task, subjects repeatedly choose between two arms with fixed but unknown reward rates. Each choice not only yields potential immediate reward but also information useful for long-term reward accumulation, thus pitting exploration against exploitation. We formalize the task as comprising a learning component, the updating of estimated reward rates based on ongoing observations, and a decision-making component, the choice among options based on current beliefs and uncertainties about reward rates. We model the learning component as iterative Bayesian inference (the Dynamic Belief Model), and the decision component using five competing decision policies: Win-stay/Lose-shift (WSLS), {$\varepsilon$}-Greedy, {$\tau$}-Switch, Softmax, Knowledge Gradient. HCS and MDI significantly differ in how they learn about reward rates and use them to make decisions. HCS learn from past observations but weigh recent data more, and their decision policy is best fit as Softmax. MDI are more likely to follow the simple learning-independent policy of WSLS, and among MDI best fit by Softmax, they have more pessimistic prior beliefs about reward rates and are less likely to choose the option estimated to be most rewarding. Neurally, MDI's tendency to avoid the most rewarding option is associated with a lower gray matter volume of the thalamic dorsal lateral nucleus. More broadly, our work illustrates the ability of our computational framework to help reveal subtle learning and decision-making abnormalities in substance use.},
  pmcid = {PMC4683191},
  pmid = {26733906},
  file = {/Users/daniekru/Zotero/storage/WH6J7IYU/Harlé et al. - 2015 - Altered Statistical Learning and Decision-Making i.pdf}
}

@article{hassabisNeuroscienceInspiredArtificialIntelligence2017,
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  year = {2017},
  month = jul,
  journal = {Neuron},
  volume = {95},
  number = {2},
  pages = {245--258},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.06.011},
  urldate = {2025-01-16},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/56AHC2CG/Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf}
}

@article{hasselmoOverviewComputationalModels2020,
  title = {Overview of Computational Models of Hippocampus and Related Structures: {{Introduction}} to the Special Issue},
  shorttitle = {Overview of Computational Models of Hippocampus and Related Structures},
  author = {Hasselmo, Michael E. and Alexander, Andrew S. and Dannenberg, Holger and Newman, Ehren L.},
  year = {2020},
  journal = {Hippocampus},
  volume = {30},
  number = {4},
  pages = {295--301},
  issn = {1098-1063},
  doi = {10.1002/hipo.23201},
  urldate = {2023-04-03},
  abstract = {Extensive computational modeling has focused on the hippocampal formation and associated cortical structures. This overview describes some of the factors that have motivated the strong focus on these structures, including major experimental findings and their impact on computational models. This overview provides a framework for describing the topics addressed by individual articles in this special issue of the journal Hippocampus.},
  langid = {english},
  keywords = {entorhinal cortex,episodic memory,grid cells,hippocampus,place cells,spatial navigation,theta rhythm,unit recording},
  file = {/Users/daniekru/Zotero/storage/VCSSWASG/Hasselmo et al. - 2020 - Overview of computational models of hippocampus an.pdf;/Users/daniekru/Zotero/storage/RJ7GKQHE/hipo.html}
}

@article{hasselmoRoleAcetylcholineLearning2006,
  title = {The Role of Acetylcholine in Learning and Memory},
  author = {Hasselmo, Michael E},
  year = {2006},
  month = dec,
  journal = {Current Opinion in Neurobiology},
  series = {Motor Systems / {{Neurobiology}} of Behaviour},
  volume = {16},
  number = {6},
  pages = {710--715},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2006.09.002},
  urldate = {2023-04-03},
  abstract = {Pharmacological data clearly indicate that both muscarinic and nicotinic acetylcholine receptors have a role in the encoding of new memories. Localized lesions and antagonist infusions demonstrate the anatomical locus of these cholinergic effects, and computational modeling links the function of cholinergic modulation to specific cellular effects within these regions. Acetylcholine has been shown to increase the strength of afferent input relative to feedback, to contribute to theta rhythm oscillations, activate intrinsic mechanisms for persistent spiking, and increase the modification of synapses. These effects might enhance different types of encoding in different cortical structures. In particular, the effects in entorhinal and perirhinal cortex and hippocampus might be important for encoding new episodic memories.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/CWAQ8NGH/Hasselmo - 2006 - The role of acetylcholine in learning and memory.pdf;/Users/daniekru/Zotero/storage/PMUTNTLJ/S095943880600122X.html}
}

@inproceedings{hauserPrinciplesRiemannianGeometry2017,
  title = {Principles of {{Riemannian Geometry}} in {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hauser, Michael and Ray, Asok},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-11},
  abstract = {This study deals with neural networks in the sense of geometric transformations acting on the coordinate representation of the underlying data manifold which the data is sampled from. It forms part of an attempt to construct a formalized general theory of neural networks in the setting of Riemannian geometry. From this perspective, the following theoretical results are developed and proven for feedforward networks. First it is shown that residual neural networks are finite difference approximations to dynamical systems of first order differential equations, as opposed to ordinary networks that are static. This implies that the network is learning systems of differential equations governing the coordinate transformations that represent the data. Second it is shown that a closed form solution of the metric tensor on the underlying data manifold can be found by backpropagating the coordinate representations learned by the neural network itself. This is formulated in a formal abstract sense as a sequence of Lie group actions on the metric fibre space in the principal and associated bundles on the data manifold. Toy experiments were run to confirm parts of the proposed theory, as well as to provide intuitions as to how neural networks operate on data.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/ZBMWECKL/Hauser and Ray - 2017 - Principles of Riemannian Geometry in Neural Networ.pdf}
}

@article{hayesReplayDeepLearning2021,
  title = {Replay in {{Deep Learning}}: {{Current Approaches}} and {{Missing Biological Elements}}},
  shorttitle = {Replay in {{Deep Learning}}},
  author = {Hayes, Tyler L. and Krishnan, Giri P. and Bazhenov, Maxim and Siegelmann, Hava T. and Sejnowski, Terrence J. and Kanan, Christopher},
  year = {2021},
  month = oct,
  journal = {Neural Computation},
  volume = {33},
  number = {11},
  pages = {2908--2950},
  issn = {1530-888X},
  doi = {10.1162/neco_a_01433},
  abstract = {Replay is the reactivation of one or more neural patterns that are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated in deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this letter, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be used to improve artificial neural networks.},
  langid = {english},
  pmcid = {PMC9074752},
  pmid = {34474476},
  keywords = {Algorithms,Animals,Deep Learning,Hippocampus,Neural Networks Computer,Reinforcement Psychology,Sleep},
  file = {/Users/daniekru/Zotero/storage/CBPPV8NJ/Hayes et al. - 2021 - Replay in Deep Learning Current Approaches and Mi.pdf}
}

@article{henriksenSpatialRepresentationProximodistal2010,
  title = {Spatial {{Representation}} along the {{Proximodistal Axis}} of {{CA1}}},
  author = {Henriksen, Espen J. and Colgin, Laura L. and Barnes, Carol A. and Witter, Menno P. and Moser, May-Britt and Moser, Edvard I.},
  year = {2010},
  month = oct,
  journal = {Neuron},
  volume = {68},
  number = {1},
  pages = {127--137},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2010.08.042},
  urldate = {2023-03-13},
  abstract = {CA1 cells receive direct input from space-responsive cells in medial entorhinal cortex (MEC), such as grid~cells, as well as more nonspatial cells in lateral entorhinal cortex (LEC). Because MEC projects preferentially to the proximal part of the CA1, bordering CA2, whereas LEC innervates only the distal part, bordering subiculum, we asked if spatial tuning is graded along the transverse axis of CA1. Tetrodes were implanted along the entire proximodistal axis of dorsal CA1 in rats. Data were recorded in cylinders large enough to elicit firing at more than one location in many neurons. Distal CA1 cells showed more dispersed firing and had a larger number of firing fields than proximal cells. Phase-locking of spikes to MEC theta oscillations was weaker in distal CA1 than in proximal CA1. The findings suggest that spatial firing in CA1 is organized transversally, with the strongest spatial modulation occurring in the MEC-associated proximal part.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/H5JPUNUM/Henriksen et al. - 2010 - Spatial Representation along the Proximodistal Axi.pdf;/Users/daniekru/Zotero/storage/ZVZM4KJ4/S0896627310006859.html}
}

@article{herdNeuralNetworkModel2014,
  title = {A Neural Network Model of Individual Differences in Task Switching Abilities},
  author = {Herd, Seth A. and O'Reilly, Randall C. and Hazy, Tom E. and Chatham, Christopher H. and Brant, Angela M. and Friedman, Naomi P.},
  year = {2014},
  month = sep,
  journal = {Neuropsychologia},
  volume = {62},
  pages = {375--389},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2014.04.014},
  urldate = {2024-05-14},
  abstract = {We use a biologically grounded neural network model to investigate the brain mechanisms underlying individual differences specific to the selection and instantiation of representations that exert cognitive control in task switching. Existing computational models of task switching do not focus on individual differences and so cannot explain why task switching abilities are separable from other executive function (EF) abilities (such as response inhibition). We explore hypotheses regarding neural mechanisms underlying the ``Shifting-Specific'' and ``Common EF'' components of EF proposed in the Unity/Diversity model (Miyake \& Friedman, 2012) and similar components in related theoretical frameworks. We do so by adapting a well-developed neural network model of working memory (Prefrontal cortex, Basal ganglia Working Memory or PBWM; Hazy, Frank, \& O'Reilly, 2007) to task switching and the Stroop task, and comparing its behavior on those tasks under a variety of individual difference manipulations. Results are consistent with the hypotheses that variation specific to task switching (i.e., Shifting-Specific) may be related to uncontrolled, automatic persistence of goal representations, whereas variation general to multiple EFs (i.e., Common EF) may be related to the strength of PFC representations and their effect on processing in the remainder of the cognitive system. Moreover, increasing signal to noise ratio in PFC, theoretically tied to levels of tonic dopamine and a genetic polymorphism in the COMT gene, reduced Stroop interference but increased switch costs. This stability--flexibility tradeoff provides an explanation for why these two EF components sometimes show opposing correlations with other variables such as attention problems and self-restraint.},
  keywords = {Computational model,Executive control,Genetics,Set shifting},
  file = {/Users/daniekru/Zotero/storage/R7P3GEDX/Herd et al. - 2014 - A neural network model of individual differences i.pdf;/Users/daniekru/Zotero/storage/XR5Q2HMI/S0028393214001365.html}
}

@article{hintonForwardForwardAlgorithmPreliminary,
  title = {The {{Forward-Forward Algorithm}}: {{Some Preliminary Investigations}}},
  author = {Hinton, Geoffrey},
  abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth serious investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes can be separated in time, the negative passes can be done offline, which makes the learning much simpler in the positive pass and allows video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/VHIPRMUY/Hinton - The Forward-Forward Algorithm Some Preliminary In.pdf}
}

@article{hoffmanFusionsConsciousness2023,
  title = {Fusions of {{Consciousness}}},
  author = {Hoffman, Donald D. and Prakash, Chetan and Prentner, Robert},
  year = {2023},
  month = jan,
  journal = {Entropy},
  volume = {25},
  number = {1},
  pages = {129},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e25010129},
  urldate = {2023-01-27},
  abstract = {What are conscious experiences? Can they combine to form new experiences? What are conscious subjects? Can they combine to form new subjects? Most attempts to answer these questions assume that spacetime, and some of its particles, are fundamental. However, physicists tell us that spacetime cannot be fundamental. Spacetime, they say, is doomed. We heed the physicists, and drop the assumption that spacetime is fundamental. We assume instead that subjects and experiences are entities beyond spacetime, not within spacetime. We make this precise in a mathematical theory of conscious agents, whose dynamics are described by Markov chains. We show how (1) agents combine into more complex agents, (2) agents fuse into simpler agents, and (3) qualia fuse to create new qualia. The possible dynamics of n agents form an n(n-1)-dimensional polytope with nn vertices---the Markov polytopeMn. The total fusions of n agents and qualia form an (n-1)-dimensional simplex---the fusion simplexFn. To project the Markovian dynamics of conscious agents onto scattering processes in spacetime, we define a new map from Markov chains to decorated permutations. Such permutations---along with helicities, or masses and spins---invariantly encode all physical information used to compute scattering amplitudes. We propose that spacetime and scattering processes are a data structure that codes for interactions of conscious agents: a particle in spacetime is a projection of the Markovian dynamics of a communicating class of conscious agents.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {amplituhedron,combination problem,conscious agents,consciousness,decorated permutations,fusion simplex,hard problem of consciousness,interface theory of perception,Markov chains,Markov polytopes,panpsychism,positive Grassmannian,qualia,subjective experience},
  file = {/Users/daniekru/Zotero/storage/RWAFZDRV/Hoffman et al. - 2023 - Fusions of Consciousness.pdf}
}

@article{horneNeuralNetworksRobotics1990,
  title = {Neural Networks in Robotics: {{A}} Survey},
  shorttitle = {Neural Networks in Robotics},
  author = {Horne, Bill and Jamshidi, M. and Vadiee, Nader},
  year = {1990},
  journal = {Journal of Intelligent and Robotic Systems},
  volume = {3},
  number = {1},
  pages = {51--66},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/BF00368972},
  urldate = {2020-05-24},
  langid = {english}
}

@article{houstonMatchingBehavioursRewards2021,
  title = {Matching {{Behaviours}} and {{Rewards}}},
  author = {Houston, Alasdair I. and Trimmer, Pete C. and McNamara, John M.},
  year = {2021},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {25},
  number = {5},
  pages = {403--415},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2021.01.011},
  urldate = {2024-05-07},
  langid = {english},
  pmid = {33612384},
  keywords = {bout structure,input matching,matching law,optimal behaviour,probability matching,switching},
  file = {/Users/daniekru/Zotero/storage/YS788STY/Houston et al. - 2021 - Matching Behaviours and Rewards.pdf}
}

@article{huFeedbackGraphMotifs2018,
  title = {Feedback through Graph Motifs Relates Structure and Function in Complex Networks},
  author = {Hu, Yu and Brunton, Steven L. and Cain, Nicholas and Mihalas, Stefan and Kutz, J. Nathan and {Shea-Brown}, Eric},
  year = {2018},
  month = dec,
  journal = {Physical Review E},
  volume = {98},
  number = {6},
  pages = {062312},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.98.062312},
  urldate = {2022-11-16},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/CX9UV5UH/Hu et al. - 2018 - Feedback through graph motifs relates structure an.pdf}
}

@article{hulmeEmergingRolesMetaplasticity2013,
  title = {Emerging Roles of Metaplasticity in Behaviour and Disease},
  author = {Hulme, Sarah R. and Jones, Owen D. and Abraham, Wickliffe C.},
  year = {2013},
  month = jun,
  journal = {Trends in Neurosciences},
  volume = {36},
  number = {6},
  pages = {353--362},
  publisher = {Elsevier},
  issn = {0166-2236, 1878-108X},
  doi = {10.1016/j.tins.2013.03.007},
  urldate = {2024-12-12},
  langid = {english},
  pmid = {23602195},
  keywords = {fEPSP,field excitatory postsynaptic potentials.}
}

@misc{huSystematicAssessmentSyntactic2020,
  title = {A {{Systematic Assessment}} of {{Syntactic Generalization}} in {{Neural Language Models}}},
  author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P.},
  year = {2020},
  month = may,
  number = {arXiv:2005.03692},
  eprint = {2005.03692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.03692},
  urldate = {2024-05-06},
  abstract = {While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M--40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/daniekru/Zotero/storage/Y6FLEADB/Hu et al. - 2020 - A Systematic Assessment of Syntactic Generalizatio.pdf;/Users/daniekru/Zotero/storage/NH32UN3L/2005.html}
}

@article{iariaCognitiveStrategiesDependent2003,
  title = {Cognitive {{Strategies Dependent}} on the {{Hippocampus}} and {{Caudate Nucleus}} in {{Human Navigation}}: {{Variability}} and {{Change}} with {{Practice}}},
  shorttitle = {Cognitive {{Strategies Dependent}} on the {{Hippocampus}} and {{Caudate Nucleus}} in {{Human Navigation}}},
  author = {Iaria, Giuseppe and Petrides, Michael and Dagher, Alain and Pike, Bruce and Bohbot, V{\'e}ronique D.},
  year = {2003},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {23},
  number = {13},
  pages = {5945--5952},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.23-13-05945.2003},
  urldate = {2025-02-22},
  abstract = {The human brain activity related to strategies for navigating in space and how it changes with practice was investigated with functional magnetic resonance imaging. Subjects used two different strategies to solve a place-learning task in a computer-generated virtual environment. One-half of the subjects used spatial landmarks to navigate in the early phase of training, and these subjects showed increased activation of the right hippocampus. The other half used a nonspatial strategy and showed, with practice, sustained increased activity within the caudate nucleus during navigation. Activation common to both groups was observed in the posterior parietal and frontal cortex. These results provide the first evidence for spontaneous variability and shift in neural mechanisms during navigation in humans.},
  chapter = {Behavioral/Systems/Cognitive},
  copyright = {Copyright {\copyright} 2003 Society for Neuroscience 0270-6474/03/235945-08.00/0},
  langid = {english},
  pmid = {12843299},
  keywords = {basal ganglia,place learning,spatial memory,striatum,topographical amnesia,virtual environment},
  file = {/Users/daniekru/Zotero/storage/TYLB4Z55/Iaria et al. - 2003 - Cognitive Strategies Dependent on the Hippocampus and Caudate Nucleus in Human Navigation Variabili.pdf}
}

@article{igarashiFunctionalDiversityTransverse2014,
  title = {Functional Diversity along the Transverse Axis of Hippocampal Area {{CA1}}},
  author = {Igarashi, Kei M. and Ito, Hiroshi T. and Moser, Edvard I. and Moser, May-Britt},
  year = {2014},
  month = aug,
  journal = {FEBS Letters},
  series = {Paris},
  volume = {588},
  number = {15},
  pages = {2470--2476},
  issn = {0014-5793},
  doi = {10.1016/j.febslet.2014.06.004},
  urldate = {2023-03-13},
  abstract = {Decades of neuroscience research have shed light on the hippocampus as a key structure for the formation of episodic memory. The hippocampus is divided into distinct subfields -- CA1, CA2 and CA3. While accumulating evidence points to cellular and synaptic heterogeneity within each subfield, this heterogeneity has not received much attention in computational and behavioural studies and subfields have until recently been considered functionally uniform. However, a couple of recent studies have demonstrated prominent functional differences along the proximodistal axis of the CA1 subfield. Here, we review anatomical and physiological differences that might give rise to heterogeneity along the proximodistal axis of CA1 as well as the functional implications of such heterogeneity. We suggest that such heterogeneity in CA1 operates dynamically in the sense that the CA1 network alternates, on a subsecond scale, between a state where the network is primarily responsive to functionally segregated direct inputs from entorhinal cortex and a state where cells predominantly are controlled by more integrated inputs from CA3.},
  langid = {english},
  keywords = {CA1,Direct pathway,Entorhinal cortex,Hippocampus,Transverse axis},
  file = {/Users/daniekru/Zotero/storage/5AJZPYSD/Igarashi et al. - 2014 - Functional diversity along the transverse axis of .pdf;/Users/daniekru/Zotero/storage/NKRQG9MQ/S001457931400444X.html}
}

@article{igelCovarianceMatrixAdaptation2007,
  title = {Covariance {{Matrix Adaptation}} for {{Multi-objective Optimization}}},
  author = {Igel, Christian and Hansen, Nikolaus and Roth, Stefan},
  year = {2007},
  month = mar,
  journal = {Evolutionary Computation},
  volume = {15},
  number = {1},
  pages = {1--28},
  issn = {1063-6560},
  doi = {10.1162/evco.2007.15.1.1},
  urldate = {2024-12-05},
  abstract = {The covariancematrix adaptation evolution strategy (CMA-ES) is one of themost powerful evolutionary algorithms for real-valued single-objective optimization. In this paper, we develop a variant of the CMA-ES for multi-objective optimization (MOO). We first introduce a single-objective, elitist CMA-ES using plus-selection and step size control based on a success rule. This algorithm is compared to the standard CMA-ES. The elitist CMA-ES turns out to be slightly faster on unimodal functions, but is more prone to getting stuck in sub-optimal local minima. In the new multi-objective CMAES (MO-CMA-ES) a population of individuals that adapt their search strategy as in the elitist CMA-ES is maintained. These are subject to multi-objective selection. The selection is based on non-dominated sorting using either the crowding-distance or the contributing hypervolume as second sorting criterion. Both the elitist single-objective CMA-ES and the MO-CMA-ES inherit important invariance properties, in particular invariance against rotation of the search space, from the original CMA-ES. The benefits of the new MO-CMA-ES in comparison to the well-known NSGA-II and to NSDE, a multi-objective differential evolution algorithm, are experimentally shown.},
  file = {/Users/daniekru/Zotero/storage/AY6BZSM6/Covariance-Matrix-Adaptation-for-Multi-objective.html}
}

@article{iigayaAdaptiveLearningDecisionmaking2016,
  title = {Adaptive Learning and Decision-Making under Uncertainty by Metaplastic Synapses Guided by a Surprise Detection System},
  author = {Iigaya, Kiyohito},
  editor = {Uchida, Naoshige},
  year = {2016},
  month = aug,
  journal = {eLife},
  volume = {5},
  pages = {e18073},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.18073},
  urldate = {2024-12-02},
  abstract = {Recent experiments have shown that animals and humans have a remarkable ability to adapt their learning rate according to the volatility of the environment. Yet the neural mechanism responsible for such adaptive learning has remained unclear. To fill this gap, we investigated a biophysically inspired, metaplastic synaptic model within the context of a well-studied decision-making network, in which synapses can change their rate of plasticity in addition to their efficacy according to a reward-based learning rule. We found that our model, which assumes that synaptic plasticity is guided by a novel surprise detection system, captures a wide range of key experimental findings and performs as well as a Bayes optimal model, with remarkably little parameter tuning. Our results further demonstrate the computational power of synaptic plasticity, and provide insights into the circuit-level computation which underlies adaptive decision-making.},
  keywords = {decision-making,learning,memory consolidation,plasticity,surprise,synapse},
  file = {/Users/daniekru/Zotero/storage/XSMBK22P/Iigaya - 2016 - Adaptive learning and decision-making under uncertainty by metaplastic synapses guided by a surprise.pdf}
}

@article{inglisModulationDopamineAdaptive2021,
  title = {Modulation of {{Dopamine}} for {{Adaptive Learning}}: {{A Neurocomputational Model}}},
  shorttitle = {Modulation of {{Dopamine}} for {{Adaptive Learning}}},
  author = {Inglis, Jeffrey B. and Valentin, Vivian V. and Ashby, F. Gregory},
  year = {2021},
  month = mar,
  journal = {Computational brain \& behavior},
  volume = {4},
  number = {1},
  pages = {34--52},
  issn = {2522-087X},
  doi = {10.1007/s42113-020-00083-x},
  urldate = {2024-12-02},
  abstract = {There have been many proposals that learning rates in the brain are adaptive, in the sense that they increase or decrease depending on environmental conditions. The majority of these models are abstract and make no attempt to describe the neural circuitry that implements the proposed computations. This article describes a biologically detailed computational model that overcomes this shortcoming. Specifically, we propose a neural circuit that implements adaptive learning rates by modulating the gain on the dopamine response to reward prediction errors, and we model activity within this circuit at the level of spiking neurons. The model generates a dopamine signal that depends on the size of the tonically active dopamine neuron population and the phasic spike rate. The model was tested successfully against results from two single-neuron recording studies and a fast-scan cyclic voltammetry study. We conclude by discussing the general applicability of the model to dopamine mediated tasks that transcend the experimental phenomena it was initially designed to address.},
  pmcid = {PMC8210637},
  pmid = {34151186},
  file = {/Users/daniekru/Zotero/storage/9ZNF7VY3/Inglis et al. - 2021 - Modulation of Dopamine for Adaptive Learning A Neurocomputational Model.pdf}
}

@article{ishikawaSpatialKnowledgeAcquisition2006,
  title = {Spatial Knowledge Acquisition from Direct Experience in the Environment: {{Individual}} Differences in the Development of Metric Knowledge and the Integration of Separately Learned Places},
  shorttitle = {Spatial Knowledge Acquisition from Direct Experience in the Environment},
  author = {Ishikawa, Toru and Montello, Daniel R.},
  year = {2006},
  month = mar,
  journal = {Cognitive Psychology},
  volume = {52},
  number = {2},
  pages = {93--129},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2005.08.003},
  urldate = {2025-02-22},
  abstract = {Existing frameworks for explaining spatial knowledge acquisition in a new environment propose either stage-like or continuous development. To examine the spatial microgenesis of individuals, a longitudinal study was conducted. Twenty-four college students were individually driven along two routes in a previously unfamiliar neighborhood over 10 weekly sessions. Starting Session 4, they were also driven along a short connecting route. After each session, participants estimated spatial properties of the routes. Some participants' knowledge improved fairly continuously over the sessions, but most participants either manifested accurate metric knowledge from the first session or never manifested accurate metric knowledge. Results are discussed in light of these large individual differences, particularly with respect to the accuracy and development of integrated configurational knowledge.},
  keywords = {Cognitive maps,Individual differences,Large-scale spaces,Spatial knowledge,Spatial microgenesis},
  file = {/Users/daniekru/Zotero/storage/5TDXKFFL/S0010028505000733.html}
}

@article{isomuraActiveInferenceLeads2022,
  title = {Active Inference Leads to {{Bayesian}} Neurophysiology},
  author = {Isomura, Takuya},
  year = {2022},
  month = feb,
  journal = {Neuroscience Research},
  series = {Constructive {{Understanding}} of {{Multi-scale Dynamism}} of {{Neuropsychiatric Disorders}}},
  volume = {175},
  pages = {38--45},
  issn = {0168-0102},
  doi = {10.1016/j.neures.2021.12.003},
  urldate = {2023-06-21},
  abstract = {The neuronal substrates that implement the free-energy principle and ensuing active inference at the neuron and synapse level have not been fully elucidated. This Review considers possible neuronal substrates underlying the principle. First, the foundations of the free-energy principle are introduced, and then its ability to empirically explain various brain functions and psychological and biological phenomena in terms of Bayesian inference is described. Mathematically, the dynamics of neural activity and plasticity that minimise a cost function can be cast as performing Bayesian inference that minimises variational free energy. This equivalence licenses the adoption of the free-energy principle as a universal characterisation of neural networks. Further, the neural network structure itself represents a generative model under which an agent operates. A virtue of this perspective is that it enables the formal association of neural network properties with prior beliefs that regulate inference and learning. The possible neuronal substrates that implement prior and posterior beliefs and how to empirically examine the theory are discussed. This perspective renders brain activity explainable, leading to a deeper understanding of the neuronal mechanisms underlying basic psychology and psychiatric disorders in terms of an implicit generative model.},
  langid = {english},
  keywords = {Active inference,Computational psychiatry,Free-energy principle,Hebbian plasticity,Neuromodulation,Predictive coding,to study,Variational Bayesian inference},
  file = {/Users/daniekru/Zotero/storage/DNE6RLQC/Isomura - 2022 - Active inference leads to Bayesian neurophysiology.pdf;/Users/daniekru/Zotero/storage/UM39BLDJ/S0168010221002595.html}
}

@article{itoFunctionalDivisionHippocampal2012,
  title = {Functional Division of Hippocampal Area {{CA1}} via Modulatory Gating of Entorhinal Cortical Inputs},
  author = {Ito, Hiroshi T. and Schuman, Erin M.},
  year = {2012},
  journal = {Hippocampus},
  volume = {22},
  number = {2},
  pages = {372--387},
  issn = {1098-1063},
  doi = {10.1002/hipo.20909},
  urldate = {2023-03-09},
  abstract = {The hippocampus receives two streams of information, spatial and nonspatial, via major afferent inputs from the medial (MEC) and lateral entorhinal cortexes (LEC). The MEC and LEC projections in the temporoammonic pathway are topographically organized along the transverse-axis of area CA1. The potential for functional segregation of area CA1, however, remains relatively unexplored. Here, we demonstrated differential novelty-induced c-Fos expression along the transverse-axis of area CA1 corresponding to topographic projections of MEC and LEC inputs. We found that, while novel place exposure induced a uniform c-Fos expression along the transverse-axis of area CA1, novel object exposure primarily activated the distal half of CA1 neurons. In hippocampal slices, we observed distinct presynaptic properties between LEC and MEC terminals, and application of either DA or NE produced a largely selective influence on one set of inputs (LEC). Finally, we demonstrated that differential c-Fos expression along the transverse axis of area CA1 was largely abolished by an antagonist of neuromodulatory receptors, clozapine. Our results suggest that neuromodulators can control topographic TA projections allowing the hippocampus to differentially encode new information along the transverse axis of area CA1. {\copyright} 2011 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {dopamine,medial/lateral entorhinal cortex,norepinephrine,spatial/nonspatial information,temporoammonic pathway},
  file = {/Users/daniekru/Zotero/storage/JCDU5UWR/Ito and Schuman - 2012 - Functional division of hippocampal area CA1 via mo.pdf;/Users/daniekru/Zotero/storage/CA2DTPCL/hipo.html}
}

@article{izhikevichSolvingDistalReward2007,
  title = {Solving the {{Distal Reward Problem}} through {{Linkage}} of {{STDP}} and {{Dopamine Signaling}}},
  author = {Izhikevich, Eugene M.},
  year = {2007},
  month = oct,
  journal = {Cerebral Cortex},
  volume = {17},
  number = {10},
  pages = {2443--2452},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhl152},
  urldate = {2022-10-27},
  abstract = {In Pavlovian and instrumental conditioning, reward typically comes seconds after reward-triggering actions, creating an explanatory conundrum known as ``distal reward problem'': How does the brain know what firing patterns of what neurons are responsible for the reward if 1) the patterns are no longer there when the reward arrives and 2) all neurons and synapses are active during the waiting period to the reward? Here, we show how the conundrum is resolved by a model network of cortical spiking neurons with spike-timing--dependent plasticity (STDP) modulated by dopamine (DA). Although STDP is triggered by nearly coincident firing patterns on a millisecond timescale, slow kinetics of subsequent synaptic plasticity is sensitive to changes in the extracellular DA concentration during the critical period of a few seconds. Random firings during the waiting period to the reward do not affect STDP and hence make the network insensitive to the ongoing activity---the key feature that distinguishes our approach from previous theoretical studies, which implicitly assume that the network be quiet during the waiting period or that the patterns be preserved until the reward arrives. This study emphasizes the importance of precise firing patterns in brain dynamics and suggests how a global diffusive reinforcement signal in the form of extracellular DA can selectively influence the right synapses at the right time.},
  file = {/Users/daniekru/Zotero/storage/ZKSRGENZ/Izhikevich - 2007 - Solving the Distal Reward Problem through Linkage .pdf;/Users/daniekru/Zotero/storage/CX2LYHY6/314939.html}
}

@article{jacksonReversalThetaRhythm2014,
  title = {Reversal of Theta Rhythm Flow through Intact Hippocampal Circuits},
  author = {Jackson, Jesse and Amilhon, B{\'e}n{\'e}dicte and Goutagny, Romain and Bott, Jean-Bastien and Manseau, Fr{\'e}d{\'e}ric and Kortleven, Christian and Bressler, Steven L. and Williams, Sylvain},
  year = {2014},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {17},
  number = {10},
  pages = {1362--1370},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn.3803},
  urldate = {2023-03-16},
  abstract = {Theta oscillations are thought to propagate unidirectionally along the hippocampal circuitry, from CA3 to CA1 and the subiculum. In this paper, Jackson and colleagues demonstrate that, in the intact rat hippocampus, theta activity can also flow in reverse from subiculum to CA3, and find that this phenomenon depends on long-range GABAergic inhibition.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Hippocampus,Learning and memory},
  file = {/Users/daniekru/Zotero/storage/Q8I8CSZU/Jackson et al. - 2014 - Reversal of theta rhythm flow through intact hippo.pdf}
}

@article{jaksicComprehensiveReviewBioInspired2023,
  title = {A {{Comprehensive Review}} of {{Bio-Inspired Optimization Algorithms Including Applications}} in {{Microelectronics}} and {{Nanophotonics}}},
  author = {Jak{\v s}i{\'c}, Zoran and Devi, Swagata and Jak{\v s}i{\'c}, Olga and Guha, Koushik},
  year = {2023},
  month = jul,
  journal = {Biomimetics},
  volume = {8},
  number = {3},
  pages = {278},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-7673},
  doi = {10.3390/biomimetics8030278},
  urldate = {2025-01-16},
  abstract = {The application of artificial intelligence in everyday life is becoming all-pervasive and unavoidable. Within that vast field, a special place belongs to biomimetic/bio-inspired algorithms for multiparameter optimization, which find their use in a large number of areas. Novel methods and advances are being published at an accelerated pace. Because of that, in spite of the fact that there are a lot of surveys and reviews in the field, they quickly become dated. Thus, it is of importance to keep pace with the current developments. In this review, we first consider a possible classification of bio-inspired multiparameter optimization methods because papers dedicated to that area are relatively scarce and often contradictory. We proceed by describing in some detail some more prominent approaches, as well as those most recently published. Finally, we consider the use of biomimetic algorithms in two related wide fields, namely microelectronics (including circuit design optimization) and nanophotonics (including inverse design of structures such as photonic crystals, nanoplasmonic configurations and metamaterials). We attempted to keep this broad survey self-contained so it can be of use not only to scholars in the related fields, but also to all those interested in the latest developments in this attractive area.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,bio-inspired computation,deep learning,genetic algorithms,metaheuristic algorithms,metasurfaces,microelectronics,multiparameter optimization,nanoelectronics,nanophotonics},
  file = {/Users/daniekru/Zotero/storage/7ZEH4Q52/Jakšić et al. - 2023 - A Comprehensive Review of Bio-Inspired Optimization Algorithms Including Applications in Microelectr.pdf}
}

@article{jayPlasticityHippocampalPrefrontal2004,
  title = {Plasticity at Hippocampal to Prefrontal Cortex Synapses Is Impaired by Loss of Dopamine and Stress: Importance for Psychiatric Diseases},
  shorttitle = {Plasticity at Hippocampal to Prefrontal Cortex Synapses Is Impaired by Loss of Dopamine and Stress},
  author = {Jay, Th{\'e}r{\`e}se M. and Rocher, Cyril and Hotte, Ma{\"i}te and Naudon, Laurent and Gurden, Hirac and Spedding, Michael},
  year = {2004},
  journal = {Neurotoxicity Research},
  volume = {6},
  number = {3},
  pages = {233--244},
  issn = {1029-8428},
  doi = {10.1007/BF03033225},
  abstract = {The direct hippocampal to prefrontal cortex pathway and its changes in synaptic plasticity is a useful framework for investigating the functional operations of hippocampal-prefrontal cortex communication in cognitive functions. Synapses on this pathway are modifiable and synaptic strength can be turned up or down depending on specific patterns of activity in the pathway. The objective of this review will be to summarize the different studies carried out on this topic including very recent data and to underline the importance of animal models for the development of new and effective medications in psychiatric diseases. We have shown that long-term potentiation (LTP) of hippocampal-prefrontal synapses is driven by the level of mesocortical dopaminergic (DA) activity and more recently that stress is also an environmental determinant of LTP at these cortical synapses. Stimulation of the ventral tegmental area at a frequency known to evoke DA overflow in the prefrontal cortex produces a long-lasting enhancement of the magnitude of hippocampal-prefrontal cortex LTP whereas a depletion of cortical DA levels generates a dramatic decrease in this LTP. Moreover, hippocampal stimulation induces a transient but significant increase in DA release in the prefrontal cortex and an optimal level of D1 receptor activation is essential for LTP expression. We recently investigated the impact of stress on hippocampal-prefrontal LTP and demonstrated that exposure to an acute stress causes a remarkable and long-lasting inhibition of LTP. Furthermore, we demonstrated that tianeptine, an antidepressant which has a unique mode of action, and clozapine an atypical antipsychotic when administered at doses normally used in human testing are able to reverse the impairment in LTP. Stressful life events have a substantial causal association with psychiatric disorders like schizophrenia and depression and recent imaging studies have shown an important role of the limbic-cortical circuit in the pathophysiology of these illnesses. Therefore, we proposed that agents capable of reversing the impairment of plasticity at hippocampal to prefrontal cortex synapses have the potential of becoming new therapeutic classes of antidepressant or antipsychotic drugs.},
  langid = {english},
  pmid = {15325962},
  keywords = {Animals,Dopamine,Hippocampus,Humans,Neuronal Plasticity,Prefrontal Cortex,Psychotropic Drugs,Receptors Dopamine D1,Stress Physiological,Synapses,Synaptic Transmission}
}

@article{jefferyIntegrationSensoryInputs2007,
  title = {Integration of the Sensory Inputs to Place Cells: {{What}}, Where, Why, and How?},
  shorttitle = {Integration of the Sensory Inputs to Place Cells},
  author = {Jeffery, Kathryn J.},
  year = {2007},
  journal = {Hippocampus},
  volume = {17},
  number = {9},
  pages = {775--785},
  issn = {1098-1063},
  doi = {10.1002/hipo.20322},
  urldate = {2023-02-17},
  abstract = {The hippocampal place cells are a highly multimodal class of neurons, receiving information from many different sensory sources to correctly localize their firing to restricted regions of an environment. Evidence suggests that the sensory information is processed upstream of the hippocampus, to extract both angular and linear metric information, and also contextual information. These various kinds of information need to be integrated for coherent firing fields to be generated, and the present article reviews recent evidence concerning how this occurs. It is concluded that there is a functional dissociation of the cortical inputs, with one class of incoming information comprising purely metric information concerning distance and orientation, probably routed via the grid cells and head direction cells. The other class of information is much more heterogeneous and serves, at least in part, to contextualize the spatial inputs so as to provide a unique representation of the place the animal is in. Evidence from remapping studies suggests that the metric and contextual inputs interact upstream of the place cells, perhaps in entorhinal cortex. A full understanding of the generation of the hippocampal place representation will require elucidation of the representational functions of the afferent cortical areas. {\copyright} 2007 Wiley-Liss, Inc.},
  langid = {english},
  keywords = {context,grid cells,place cells,sensory integration,space},
  file = {/Users/daniekru/Zotero/storage/77P35F6Z/Jeffery - 2007 - Integration of the sensory inputs to place cells .pdf;/Users/daniekru/Zotero/storage/2WYQ9VHD/hipo.html}
}

@article{jepkoechEffectAdaptiveLearning2021,
  title = {The {{Effect}} of {{Adaptive Learning Rate}} on the {{Accuracy}} of {{Neural Networks}}},
  author = {Jepkoech, Jennifer and Mugo, David Muchangi and Kenduiywo, Benson K. and Too, Edna Chebet},
  year = {2021},
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {12},
  number = {8},
  publisher = {{Science and Information (SAI) Organization Limited}},
  address = {West Yorkshire, United Kingdom},
  issn = {2158107X},
  doi = {10.14569/IJACSA.2021.0120885},
  urldate = {2024-12-09},
  abstract = {Learning rates in gradient descent algorithms have significant effects especially on the accuracy of a Capsule Neural Network (CNN). Choosing an appropriate learning rate is still an issue to date. Many developers still have a problem in selecting a learning rate for CNN leading to low accuracies in classification. This gap motivated this study to assess the effect of learning rate on the accuracy of a developed (CNN). There are no predefined learning rates in CNN and therefore it is hard for researchers to know what learning rate will give good results. This work, therefore, focused on assessing the effect of learning rate on the accuracy of a CNN by using different learning rates and observing the best performance. The contribution of this work is to give an appropriate learning rate for CNNs to improve accuracy during classification. This work has assessed the effect of different learning rates and came up with the most appropriate learning rate for CNN plant leaf disease classification. Part of the images used in this work was from the PlantVillage dataset while others were from the Nepal database. The images were pre-processed then subjected to the original CNN model for classification. When the learning rate was 0.0001, the best performance was 99.4\% on testing and 100\% on training. When the learning rate was 0.00001, the highest performance was 97\% on testing and 99.9\% on training. The lowest performance observed was 81\% accuracy on testing and 99\% on training when the learning rate was 0.001. This work observed that CNN was able to achieve the highest accuracy with a learning rate of 0.0001. The best Convolutional Neural Network accuracy observed was 98\% on testing and 100\% on training when the learning rate was 0.0001.},
  copyright = {{\copyright} 2021. This work is licensed under https://creativecommons.org/licenses/by/4.0/  (the ``License'').  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  keywords = {Accuracy,Accuracy and precision,Adaptive learning,Algorithms,Artificial neural networks,Capsule neural network,Classification,CNN,ConvNet,Convolutional neural network,Digital image processing,gradient descent,Gradient descent,Image classification,learning rate,Learning rate,Machine learning,Nepal,Nervous system,Neural network,Neural networks,Plant diseases,Training},
  file = {/Users/daniekru/Zotero/storage/PDIFPEPC/Jepkoech et al. - 2021 - The Effect of Adaptive Learning Rate on the Accuracy of Neural Networks.pdf}
}

@article{jerjianSelfmotionPerceptionSequential2023,
  title = {Self-Motion Perception and Sequential Decision-Making: Where Are We Heading?},
  shorttitle = {Self-Motion Perception and Sequential Decision-Making},
  author = {Jerjian, Steven J. and Harsch, Devin R. and Fetsch, Christopher R.},
  year = {2023},
  month = aug,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {378},
  number = {1886},
  pages = {20220333},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2022.0333},
  urldate = {2025-02-18},
  abstract = {To navigate and guide adaptive behaviour in a dynamic environment, animals must accurately estimate their own motion relative to the external world. This is a fundamentally multisensory process involving integration of visual, vestibular and kinesthetic inputs. Ideal observer models, paired with careful neurophysiological investigation, helped to reveal how visual and vestibular signals are combined to support perception of linear self-motion direction, or heading. Recent work has extended these findings by emphasizing the dimension of time, both with regard to stimulus dynamics and the trade-off between speed and accuracy. Both time and certainty---i.e. the degree of confidence in a multisensory decision---are essential to the ecological goals of the system: terminating a decision process is necessary for timely action, and predicting one's accuracy is critical for making multiple decisions in a sequence, as in navigation. Here, we summarize a leading model for multisensory decision-making, then show how the model can be extended to study confidence in heading discrimination. Lastly, we preview ongoing efforts to bridge self-motion perception and navigation per se, including closed-loop virtual reality and active self-motion. The design of unconstrained, ethologically inspired tasks, accompanied by large-scale neural recordings, raise promise for a deeper understanding of spatial perception and decision-making in the behaving animal. This article is part of the theme issue `Decision and control processes in multisensory perception'.},
  keywords = {navigation,self-motion,vestibular,visual},
  file = {/Users/daniekru/Zotero/storage/3DYF2ZPB/Jerjian et al. - 2023 - Self-motion perception and sequential decision-making where are we heading.pdf}
}

@article{jiangFewShotLearningSpiking2021,
  title = {Few-{{Shot Learning}} in {{Spiking Neural Networks}} by {{Multi-Timescale Optimization}}},
  author = {Jiang, Runhao and Zhang, Jie and Yan, Rui and Tang, Huajin},
  year = {2021},
  month = aug,
  journal = {Neural Computation},
  volume = {33},
  number = {9},
  pages = {2439--2472},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01423},
  urldate = {2022-10-27},
  abstract = {Learning new concepts rapidly from a few examples is an open issue in spike-based machine learning. This few-shot learning imposes substantial challenges to the current learning methodologies of spiking neuron networks (SNNs) due to the lack of task-related priori knowledge. The recent learning-to-learn (L2L) approach allows SNNs to acquire priori knowledge through example-level learning and task-level optimization. However, existing L2L-based frameworks do not target the neural dynamics (i.e., neuronal and synaptic parameter changes) on different timescales. This diversity of temporal dynamics is an important attribute in spike-based learning, which facilitates the networks to rapidly acquire knowledge from very few examples and gradually integrate this knowledge. In this work, we consider the neural dynamics on various timescales and provide a multi-timescale optimization (MTSO) framework for SNNs. This framework introduces an adaptive-gated LSTM to accommodate two different timescales of neural dynamics: short-term learning and long-term evolution. Short-term learning is a fast knowledge acquisition process achieved by a novel surrogate gradient online learning (SGOL) algorithm, where the LSTM guides gradient updating of SNN on a short timescale through an adaptive learning rate and weight decay gating. The long-term evolution aims to slowly integrate acquired knowledge and form a priori, which can be achieved by optimizing the LSTM guidance process to tune SNN parameters on a long timescale. Experimental results demonstrate that the collaborative optimization of multi-timescale neural dynamics can make SNNs achieve promising performance for the few-shot learning tasks.},
  file = {/Users/daniekru/Zotero/storage/B8APF7JI/Few-Shot-Learning-in-Spiking-Neural-Networks-by.html}
}

@phdthesis{jimenez-romeroHeterosynapticSpikingNeural2017,
  title = {A {{Heterosynaptic Spiking Neural System}} for the {{Development}} of {{Autonomous Agents}}},
  author = {{Jimenez-Romero}, Cristian},
  year = {2017},
  month = feb,
  urldate = {2023-06-12},
  abstract = {Artificial neural systems for computation were first proposed three quarters of a century ago and the concepts developed by the pioneers still shape the field today. The first generation of neural systems was developed in the nineteen forties in the context of analogue electronics and the theoretical research in logic and mathematics that led to the first digital computers in nineteen forties and fifties. The second generation of neural systems implemented on digital computers was born in the nineteen fifties and great progress was made in the subsequent half century with neural networks being applied to many problems in pattern recognition and machine learning. Through this history there has been an interplay between biologically inspired neural systems and their implementation by engineers on digital machines. This thesis concerns the third generation of neural networks, Spiking Neural Networks, which is making possible the creation of new kinds of brain inspired computing architectures that offer the potential to increase the level of realism and sophistication in terms of autonomous machine behaviour and cognitive computing. This thesis presents the development and demonstration of a new theoretical architecture for third generation neural systems, the Integrate-and-Fire based Spiking Neural Model with extended Neuro-modulated Spike Timing Dependent Plasticity capabilities. This proposed architecture overcomes the limitation of the homosynaptic architecture underlying existing implementations of spiking neural networks that it lacks a natural spike timing dependent plasticity regulation mechanism, and this results in `run away' dynamics. To overcome this ad hoc procedures have been implemented to overcome the `run away' dynamics that emerge from the use of spike timing dependent plasticity among other hebbian-based plasticity rules. The new heterosynaptic architecture presented, explicitly abstracts the modulation of complex biochemical mechanisms into a simplified mechanism that is suitable for the engineering of artificial systems with low computational complexity. Neurons work by receiving input signals from other neurons through synapses. The difference between homosynaptic and heterosynaptic plasticity is that, in the former the change in the properties of a synapse (e.g. synaptic efficacy) depends on the point to point activity in either of the sending and receiving neurons, in contrast for heterosynaptic plasticity the change in the properties of a synapse can be elicited by neurons that are not necessary presynaptic or postsynaptic to the synapse in question. The new architecture is tested by a number of implementations in simulated and real environments. This includes experiments with a simulation environment implemented in Netlogo, and an implementation using Lego Mindstorms as the physical robot platform. These experiments demonstrate the problems with the traditional Spike timing dependent plasticity homosynaptic architecture and how the new heterosynaptic approach can overcome them. It is concluded that the new theoretical architecture provides a natural, theoretically sound, and practical new direction for research into the role of modulatory neural systems applied to spiking neural networks.},
  langid = {english},
  school = {The Open University},
  file = {/Users/daniekru/Zotero/storage/87X7SFMF/Jimenez-Romero - 2017 - A Heterosynaptic Spiking Neural System for the Dev.pdf;/Users/daniekru/Zotero/storage/B5RUYPE4/48888.html}
}

@inproceedings{jordanEvaluatingPerformanceReinforcement2020,
  title = {Evaluating the {{Performance}} of {{Reinforcement Learning Algorithms}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Jordan, Scott and Chandak, Yash and Cohen, Daniel and Zhang, Mengxue and Thomas, Philip},
  year = {2020},
  month = nov,
  pages = {4962--4973},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-10-27},
  abstract = {Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difficult to replicate. In this work, we argue that the inconsistency of performance stems from the use of flawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/LTDGBVRV/Jordan et al. - 2020 - Evaluating the Performance of Reinforcement Learni.pdf;/Users/daniekru/Zotero/storage/MDM6HRSC/Jordan et al. - 2020 - Evaluating the Performance of Reinforcement Learni.pdf}
}

@misc{JupyterLab,
  title = {{{JupyterLab}}},
  urldate = {2025-01-29},
  howpublished = {http://localhost:8889/lab},
  file = {/Users/daniekru/Zotero/storage/H5DSSWSD/lab.html}
}

@misc{JupyterLaba,
  title = {{{JupyterLab}}},
  urldate = {2025-03-01},
  howpublished = {http://localhost:8889/lab},
  file = {/Users/daniekru/Zotero/storage/Q7AQ5ITC/lab.html}
}

@misc{kannanUnsupervisedSpikingNeural2023,
  title = {Unsupervised {{Spiking Neural Network Model}} of {{Prefrontal Cortex}} to Study {{Task Switching}} with {{Synaptic}} Deficiency},
  author = {Kannan, Ashwin Viswanathan and Mylavarapu, Goutam and Thomas, Johnson P.},
  year = {2023},
  month = may,
  number = {arXiv:2305.14394},
  eprint = {2305.14394},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14394},
  urldate = {2024-05-03},
  abstract = {In this study, we build a computational model of Prefrontal Cortex (PFC) using Spiking Neural Networks (SNN) to understand how neurons adapt and respond to tasks switched under short and longer duration of stimulus changes. We also explore behavioral deficits arising out of the PFC lesions by simulating lesioned states in our Spiking architecture model. Although there are some computational models of the PFC, SNN's have not been used to model them. In this study, we use SNN's having parameters close to biologically plausible values and train the model using unsupervised Spike Timing Dependent Plasticity (STDP) learning rule. Our model is based on connectionist architectures and exhibits neural phenomena like sustained activity which helps in generating short-term or working memory. We use these features to simulate lesions by deactivating synaptic pathways and record the weight adjustments of learned patterns and capture the accuracy of learning tasks in such conditions. All our experiments are trained and recorded using a real-world Fashion MNIST (FMNIST) dataset and through this work, we bridge the gap between bio-realistic models and those that perform well in pattern recognition tasks},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/daniekru/Zotero/storage/DJTHQ7S7/Kannan et al. - 2023 - Unsupervised Spiking Neural Network Model of Prefr.pdf;/Users/daniekru/Zotero/storage/MH6K7SL2/2305.html}
}

@article{karlebachModellingAnalysisGene2008,
  title = {Modelling and Analysis of Gene Regulatory Networks},
  author = {Karlebach, Guy and Shamir, Ron},
  year = {2008},
  month = oct,
  journal = {Nature Reviews Molecular Cell Biology},
  volume = {9},
  number = {10},
  pages = {770--780},
  publisher = {Nature Publishing Group},
  issn = {1471-0080},
  doi = {10.1038/nrm2503},
  urldate = {2022-09-06},
  abstract = {Gene regulatory networks control many cellular processes such as cell cycle, cell differentiation, metabolism and signal transduction. Computational methods, both for supporting the development of network models and for the analysis of their functionality, have already proved to be a valuable research tool.},
  copyright = {2008 Nature Publishing Group},
  langid = {english},
  keywords = {Biochemistry,Cancer Research,Cell Biology,Developmental Biology,general,Life Sciences,Stem Cells},
  file = {/Users/daniekru/Zotero/storage/HG822QU4/Karlebach and Shamir - 2008 - Modelling and analysis of gene regulatory networks.pdf;/Users/daniekru/Zotero/storage/FEXLCXPR/nrm2503.html}
}

@inproceedings{karniDevelopmentAutonomousDownscaled2019,
  title = {Development {{Of Autonomous Downscaled Model Car Using Neural Networks And Machine Learning}}},
  booktitle = {2019 3rd {{International Conference}} on {{Computing Methodologies}} and {{Communication}} ({{ICCMC}})},
  author = {Karni, Uvais and Ramachandran, S. Shreyas and Sivaraman, K. and Veeraraghavan, A. K.},
  year = {2019},
  month = mar,
  pages = {1089--1094},
  publisher = {IEEE},
  address = {Erode, India},
  doi = {10.1109/ICCMC.2019.8819720},
  urldate = {2020-05-24},
  isbn = {978-1-5386-7808-4}
}

@article{kashtanTopologicalGeneralizationsNetwork2004,
  title = {Topological Generalizations of Network Motifs},
  author = {Kashtan, N. and Itzkovitz, S. and Milo, R. and Alon, U.},
  year = {2004},
  month = sep,
  journal = {Physical Review E},
  volume = {70},
  number = {3},
  pages = {031909},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.70.031909},
  urldate = {2022-11-17},
  abstract = {Biological and technological networks contain patterns, termed network motifs, which occur far more often than in randomized networks. Network motifs were suggested to be elementary building blocks that carry out key functions in the network. It is of interest to understand how network motifs combine to form larger structures. To address this, we present a systematic approach to define ``motif generalizations'': families of motifs of different sizes that share a common architectural theme. To define motif generalizations, we first define ``roles'' in a subgraph according to structural equivalence. For example, the feedforward loop triad---a motif in transcription, neuronal, and some electronic networks---has three roles: an input node, an output node, and an internal node. The roles are used to define possible generalizations of the motif. The feedforward loop can have three simple generalizations, based on replicating each of the three roles and their connections. We present algorithms for efficiently detecting motif generalizations. We find that the transcription networks of bacteria and yeast display only one of the three generalizations, the multi-output feedforward generalization. In contrast, the neuronal network of C. elegans mainly displays the multi-input generalization. Forward-logic electronic circuits display a multi-input, multi-output hybrid. Thus, networks which share a common motif can have very different generalizations of that motif. Using mathematical modeling, we describe the information processing functions of the different motif generalizations in transcription, neuronal, and electronic networks.},
  file = {/Users/daniekru/Zotero/storage/QUK6T8YE/Kashtan et al. - 2004 - Topological generalizations of network motifs.pdf;/Users/daniekru/Zotero/storage/F86S8KR6/PhysRevE.70.html}
}

@article{kastellakisLinkingMemoriesTime2016,
  title = {Linking {{Memories}} across {{Time}} via {{Neuronal}} and {{Dendritic Overlaps}} in {{Model Neurons}} with {{Active Dendrites}}},
  author = {Kastellakis, George and Silva, Alcino J. and Poirazi, Panayiota},
  year = {2016},
  month = nov,
  journal = {Cell Reports},
  volume = {17},
  number = {6},
  pages = {1491--1504},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2016.10.015},
  urldate = {2023-06-20},
  abstract = {Memories are believed to be stored in distributed~neuronal assemblies through activity-induced changes in synaptic and intrinsic properties. However, the specific mechanisms by which different memories become associated or linked remain a mystery. Here, we develop a simplified, biophysically inspired network model that incorporates multiple plasticity processes and explains linking of information at three different levels: (1) learning of a single associative memory, (2) rescuing of a weak memory when paired with a strong one, and (3) linking of multiple memories across time. By dissecting synaptic from intrinsic plasticity and neuron-wide from dendritically restricted protein capture, the model reveals a simple, unifying principle: linked memories share synaptic clusters within the dendrites of overlapping populations of neurons. The model generates numerous experimentally testable predictions regarding the cellular and sub-cellular properties of~memory engrams as well as their spatiotemporal interactions.},
  langid = {english},
  keywords = {computational model,information binding,intrinsic excitability,memory allocation,non-linear dendrites,plasticity,simplified neurons,synaptic clustering,synaptic tagging and capture},
  file = {/Users/daniekru/Zotero/storage/4R435585/Kastellakis et al. - 2016 - Linking Memories across Time via Neuronal and Dend.pdf;/Users/daniekru/Zotero/storage/TKXRTP85/supplementary_mat.pdf;/Users/daniekru/Zotero/storage/VN4RWBW6/S2211124716314012.html}
}

@misc{kaufmannThompsonSamplingAsymptotically2012,
  title = {Thompson {{Sampling}}: {{An Asymptotically Optimal Finite Time Analysis}}},
  shorttitle = {Thompson {{Sampling}}},
  author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'e}mi},
  year = {2012},
  month = jul,
  number = {arXiv:1205.4217},
  eprint = {1205.4217},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-23},
  abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/ZL83R67T/Kaufmann et al. - 2012 - Thompson Sampling An Asymptotically Optimal Finit.pdf}
}

@article{kaufmanRoleLocusCoeruleus2020,
  title = {A {{Role}} for the {{Locus Coeruleus}} in {{Hippocampal CA1 Place Cell Reorganization}} during {{Spatial Reward Learning}}},
  author = {Kaufman, Alexandra Mansell and Geiller, Tristan and Losonczy, Attila},
  year = {2020},
  month = mar,
  journal = {Neuron},
  volume = {105},
  number = {6},
  pages = {1018-1026.e4},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.12.029},
  urldate = {2023-02-07},
  abstract = {During spatial learning, hippocampal (HPC) place maps reorganize to represent new goal locations, but little is known about the circuit mechanisms facilitating these changes. Here, we examined how neuromodulation via locus coeruleus (LC) projections to HPC area CA1 (LC-CA1) regulates the overrepresentation of CA1 place cells near rewarded locations. Using two-photon calcium imaging, we monitored the activity of LC-CA1 fibers in the mouse dorsal HPC. We find that the LC-CA1 projection signals the translocation of a reward, predicting behavioral performance on a goal-oriented spatial learning task. An optogenetic stimulation mimicking this LC-CA1 activity induces place cell reorganization around a familiar reward, while its inhibition decreases the degree of overrepresentation around a translocated reward. Our results show that LC acts in conjunction with other factors to induce goal-directed reorganization of HPC representations and provide a better understanding of the role of neuromodulatory actions on HPC place map plasticity.},
  langid = {english},
  keywords = {hippocampus,imaging,locus coeruleus,noradrenaline,optogenetics,place cell,reward learning},
  file = {/Users/daniekru/Zotero/storage/4ZHW2K7S/Kaufman et al. - 2020 - A Role for the Locus Coeruleus in Hippocampal CA1 .pdf;/Users/daniekru/Zotero/storage/H35AEUC3/S0896627319310955.html}
}

@article{kazaninaNeuralIngredientsLanguage2023,
  title = {The Neural Ingredients for a Language of Thought Are Available},
  author = {Kazanina, Nina and Poeppel, David},
  year = {2023},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {27},
  number = {11},
  pages = {996--1007},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2023.07.012},
  urldate = {2024-05-08},
  abstract = {The classical notion of a `language of thought' (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.},
  keywords = {compositionality,computational theory of mind,language-of-thought,spatial navigation,symbolic representation},
  file = {/Users/daniekru/Zotero/storage/YRPZG2W5/Kazanina and Poeppel - 2023 - The neural ingredients for a language of thought a.pdf}
}

@article{kazaninaNeuralIngredientsLanguage2023a,
  title = {The Neural Ingredients for a Language of Thought Are Available},
  author = {Kazanina, Nina and Poeppel, David},
  year = {2023},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {27},
  number = {11},
  pages = {996--1007},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2023.07.012},
  urldate = {2024-05-08},
  abstract = {The classical notion of a `language of thought' (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.},
  keywords = {compositionality,computational theory of mind,language-of-thought,spatial navigation,symbolic representation},
  file = {/Users/daniekru/Zotero/storage/6VUKZGKB/Kazanina and Poeppel - 2023 - The neural ingredients for a language of thought a.pdf;/Users/daniekru/Zotero/storage/ISQ43J2F/S1364661323001936.html}
}

@article{kempadooDopamineReleaseLocus2016,
  title = {Dopamine Release from the Locus Coeruleus to the Dorsal Hippocampus Promotes Spatial Learning and Memory},
  author = {Kempadoo, Kimberly A. and Mosharov, Eugene V. and Choi, Se Joon and Sulzer, David and Kandel, Eric R.},
  year = {2016},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {51},
  pages = {14835--14840},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1616515114},
  urldate = {2023-05-02},
  abstract = {Significance             Successful completion of daily activities relies on the ability to select the relevant features of the environment to pay attention to and remember. Disruptions of these processes can lead to disorders, such as attention-deficit hyperactivity disorder and age-related memory loss. To devise therapeutic strategies, we must understand the neural circuits underlying normal cognition. One important pathway is the signaling of dopamine, a reinforcement-related neurotransmitter, in the hippocampus, a spatial learning and memory center. Surprisingly, the brain region supplying dopamine to the dorsal hippocampus is unclear. This study provides direct evidence that the noradrenergic locus coeruleus coreleases dopamine in the dorsal hippocampus and provides insight into dopamine function in selective attention and spatial learning and memory.           ,              Dopamine neurotransmission in the dorsal hippocampus is critical for a range of functions from spatial learning and synaptic plasticity to the deficits underlying psychiatric disorders such as attention-deficit hyperactivity disorder. The ventral tegmental area (VTA) is the presumed source of dopamine in the dorsal hippocampus. However, there is a surprising scarcity of VTA dopamine axons in the dorsal hippocampus despite the dense network of dopamine receptors. We have explored this apparent paradox using optogenetic, biochemical, and behavioral approaches and found that dopaminergic axons and subsequent dopamine release in the dorsal hippocampus originate from neurons of the locus coeruleus (LC). Photostimulation of LC axons produced an increase in dopamine release in the dorsal hippocampus as revealed by high-performance liquid chromatography. Furthermore, optogenetically induced release of dopamine from the LC into the dorsal hippocampus enhanced selective attention and spatial object recognition via the dopamine D1/D5 receptor. These results suggest that spatial learning and memory are energized by the release of dopamine in the dorsal hippocampus from noradrenergic neurons of the LC. The present findings are critical for identifying the neural circuits that enable proper attention selection and successful learning and memory.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/V3P4BSNR/Kempadoo et al. - 2016 - Dopamine release from the locus coeruleus to the d.pdf}
}

@article{kennedySynapticSignalingLearning2016,
  title = {Synaptic {{Signaling}} in {{Learning}} and {{Memory}}},
  author = {Kennedy, Mary B.},
  year = {2016},
  month = feb,
  journal = {Cold Spring Harbor Perspectives in Biology},
  volume = {8},
  number = {2},
  pages = {a016824},
  issn = {1943-0264},
  doi = {10.1101/cshperspect.a016824},
  urldate = {2024-12-12},
  abstract = {Learning and memory require the formation of new neural networks in the brain. A key mechanism underlying this process is synaptic plasticity at excitatory synapses, which connect neurons into networks. Excitatory synaptic transmission happens when glutamate, the excitatory neurotransmitter, activates receptors on the postsynaptic neuron. Synaptic plasticity is a higher-level process in which the strength of excitatory synapses is altered in response to the pattern of activity at the synapse. It is initiated in the postsynaptic compartment, where the precise pattern of influx of calcium through activated glutamate receptors leads either to the addition of new receptors and enlargement of the synapse (long-term potentiation) or the removal of receptors and shrinkage of the synapse (long-term depression). Calcium/calmodulin-regulated enzymes and small GTPases collaborate to control this highly tuned mechanism., The {$\sim$}86 billion neurons of the human brain make trillions of synaptic connections. The unique plasticity of excitatory glutamatergic synapses is an essential mechanism of memory formation.},
  pmcid = {PMC4743082},
  pmid = {24379319},
  file = {/Users/daniekru/Zotero/storage/64R6AGRR/Kennedy - 2016 - Synaptic Signaling in Learning and Memory.pdf}
}

@article{kennerleyDecisionMakingReward2011a,
  title = {Decision {{Making}} and {{Reward}} in {{Frontal Cortex}}},
  author = {Kennerley, Steven W. and Walton, Mark E.},
  year = {2011},
  month = jun,
  journal = {Behavioral Neuroscience},
  volume = {125},
  number = {3},
  pages = {297--317},
  issn = {0735-7044},
  doi = {10.1037/a0023575},
  urldate = {2024-11-27},
  abstract = {Patients with damage to the prefrontal cortex (PFC)---especially the ventral and medial parts of PFC---often show a marked inability to make choices that meet their needs and goals. These decision-making impairments often reflect both a deficit in learning concerning the consequences of a choice, as well as deficits in the ability to adapt future choices based on experienced value of the current choice. Thus, areas of PFC must support some value computations that are necessary for optimal choice. However, recent frameworks of decision making have highlighted that optimal and adaptive decision making does not simply rest on a single computation, but a number of different value computations may be necessary. Using this framework as a guide, we summarize evidence from both lesion studies and single-neuron physiology for the representation of different value computations across PFC areas.},
  pmcid = {PMC3129331},
  pmid = {21534649},
  file = {/Users/daniekru/Zotero/storage/WW4F669B/Kennerley and Walton - 2011 - Decision Making and Reward in Frontal Cortex.pdf}
}

@incollection{khamassiChapter22Medial2013,
  title = {Chapter 22 - {{Medial}} Prefrontal Cortex and the Adaptive Regulation of Reinforcement Learning Parameters},
  booktitle = {Progress in {{Brain Research}}},
  author = {Khamassi, Mehdi and Enel, Pierre and Dominey, Peter Ford and Procyk, Emmanuel},
  editor = {Pammi, V. S. Chandrasekhar and Srinivasan, Narayanan},
  year = {2013},
  month = jan,
  series = {Decision {{Making}}},
  volume = {202},
  pages = {441--464},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-444-62604-2.00022-8},
  urldate = {2024-12-10},
  abstract = {Converging evidence suggest that the medial prefrontal cortex (MPFC) is involved in feedback categorization, performance monitoring, and task monitoring, and may contribute to the online regulation of reinforcement learning (RL) parameters that would affect decision-making processes in the lateral prefrontal cortex (LPFC). Previous neurophysiological experiments have shown MPFC activities encoding error likelihood, uncertainty, reward volatility, as well as neural responses categorizing different types of feedback, for instance, distinguishing between choice errors and execution errors. Rushworth and colleagues have proposed that the involvement of MPFC in tracking the volatility of the task could contribute to the regulation of one of RL parameters called the learning rate. We extend this hypothesis by proposing that MPFC could contribute to the regulation of other RL parameters such as the exploration rate and default action values in case of task shifts. Here, we analyze the sensitivity to RL parameters of behavioral performance in two monkey decision-making tasks, one with a deterministic reward schedule and the other with a stochastic one. We show that there exist optimal parameter values specific to each of these tasks, that need to be found for optimal performance and that are usually hand-tuned in computational models. In contrast, automatic online regulation of these parameters using some heuristics can help producing a good, although non-optimal, behavioral performance in each task. We finally describe our computational model of MPFC--LPFC interaction used for online regulation of the exploration rate and its application to a human--robot interaction scenario. There, unexpected uncertainties are produced by the human introducing cued task changes or by cheating. The model enables the robot to autonomously learn to reset exploration in response to such uncertain cues and events. The combined results provide concrete evidence specifying how prefrontal cortical subregions may cooperate to regulate RL parameters. It also shows how such neurophysiologically inspired mechanisms can control advanced robots in the real world. Finally, the model's learning mechanisms that were challenged in the last robotic scenario provide testable predictions on the way monkeys may learn the structure of the task during the pretraining phase of the previous laboratory experiments.},
  keywords = {computational modeling,decision making,medial prefrontal cortex,metalearning,neurorobotics,reinforcement learning},
  file = {/Users/daniekru/Zotero/storage/6G7K6A66/Khamassi et al. - 2013 - Chapter 22 - Medial prefrontal cortex and the adaptive regulation of reinforcement learning paramete.pdf;/Users/daniekru/Zotero/storage/KRWI3AVB/B9780444626042000228.html}
}

@misc{khonaEmergenceRobustGlobal2023,
  title = {Emergence of Robust Global Modules from Local Interactions and Smooth Gradients},
  author = {Khona, Mikail and Chandra, Sarthak and Fiete, Ila},
  year = {2023},
  month = aug,
  primaryclass = {New Results},
  pages = {2021.10.28.466284},
  publisher = {bioRxiv},
  doi = {10.1101/2021.10.28.466284},
  urldate = {2023-10-27},
  abstract = {Modular structure and function are ubiquitous in biology, from the scale of ecosystems to the organization of animal bodies and brains. However, the mechanisms of modularity emergence over development remain unclear. Here we introduce the principle of peak selection, a process in which two local interactions self-organize discontinuous module boundaries from a smooth global gradient, unifying the positional hypothesis and the Turing pattern formation hypothesis for morphogenesis. Applied to the brain's grid cell networks, peak selection results in the spontaneous emergence of functionally distinct modules with discretely spaced spatial periods. Applied to ecological systems, a generalization of the process results in discrete systems-level niches. The dynamics exhibits emergent self-scaling to variations in system size and ``topological robustness'' [1] that renders module emergence and module properties insensitive to most parameters. Peak selection substantially ameliorates the fine-tuning requirement of continuous attractor dynamics even within single modules. It makes a detail-independent prediction that grid module period ratios should approximate adjacent integer ratios, furnishing the most accurate match to data to date, with additional predictions to connect physiology, connectomics, and transcriptomics data. In sum, our results indicate that local competitive interactions combined with low-information global gradients can lead to robust global module emergence.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/TVQA8H8C/Khona et al. - 2023 - Emergence of robust global modules from local inte.pdf}
}

@article{kimSimpleFrameworkConstructing2019,
  title = {Simple Framework for Constructing Functional Spiking Recurrent Neural Networks},
  author = {Kim, Robert and Li, Yinghao and Sejnowski, Terrence J.},
  year = {2019},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {45},
  pages = {22811--22820},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1905926116},
  urldate = {2020-05-27},
  abstract = {Cortical microcircuits exhibit complex recurrent architectures that possess dynamically rich properties. The neurons that make up these microcircuits communicate mainly via discrete spikes, and it is not clear how spikes give rise to dynamics that can be used to perform computationally challenging tasks. In contrast, continuous models of rate-coding neurons can be trained to perform complex tasks. Here, we present a simple framework to construct biologically realistic spiking recurrent neural networks (RNNs) capable of learning a wide range of tasks. Our framework involves training a continuous-variable rate RNN with important biophysical constraints and transferring the learned dynamics and constraints to a spiking RNN in a one-to-one manner. The proposed framework introduces only 1 additional parameter to establish the equivalence between rate and spiking RNN models. We also study other model parameters related to the rate and spiking networks to optimize the one-to-one mapping. By establishing a close relationship between rate and spiking models, we demonstrate that spiking RNNs could be constructed to achieve similar performance as their counterpart continuous rate networks.},
  chapter = {Biological Sciences},
  copyright = {{\copyright} 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {31636215},
  keywords = {rate neural networks,recurrent neural networks,spiking neural networks},
  file = {/Users/daniekru/Zotero/storage/RJV95RT8/Kim et al. - 2019 - Simple framework for constructing functional spiki.pdf;/Users/daniekru/Zotero/storage/HKBEZ573/22811.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2024-12-09},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/2JZF4MHY/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/daniekru/Zotero/storage/KT9KGGYT/1412.html}
}

@article{kloostermanElectrophysiologicalCharacterizationInterlaminar2003,
  title = {Electrophysiological Characterization of Interlaminar Entorhinal Connections: An Essential Link for Re-Entrance in the Hippocampal--Entorhinal System},
  shorttitle = {Electrophysiological Characterization of Interlaminar Entorhinal Connections},
  author = {Kloosterman, Fabian and Van Haeften, Theo and Witter, Menno P. and {Lopes da Silva}, Fernando H.},
  year = {2003},
  journal = {European Journal of Neuroscience},
  volume = {18},
  number = {11},
  pages = {3037--3052},
  issn = {1460-9568},
  doi = {10.1111/j.1460-9568.2003.03046.x},
  urldate = {2023-03-13},
  abstract = {The hippocampal formation communicates with the neocortex mainly through the adjacent entorhinal cortex. Neurons projecting to the hippocampal formation are found in the superficial layers of the entorhinal cortex and are largely segregated from the neurons receiving hippocampal output, which are located in deep entorhinal layers. We studied the communication between deep and superficial entorhinal layers in the anaesthetized rat using field potential recordings, current source density analysis and single unit measurements. We found that subiculum stimulation was able to excite entorhinal neurons in deep layers. This response was followed by current sinks in superficial layers. Both responses were subject to frequency dependent facilitation, but not depression. Selective blockade of deep layer responses also abolished subsequent superficial layer responses. This clearly demonstrates a functional deep-to-superficial layer communication in the entorhinal cortex, which can be triggered by hippocampal output. This pathway may provide a means by which processed hippocampal output is integrated or compared with new incoming information in superficial entorhinal layers, and it constitutes an important link in the process of re-entrance of activity in the hippocampal--entorhinal network, which may be important for consolidation of memories or retaining information for short periods.},
  langid = {english},
  keywords = {current source density analysis,entorhinal cortex,in vivo,memory,rat,subiculum},
  file = {/Users/daniekru/Zotero/storage/93CVL86W/Kloosterman et al. - 2003 - Electrophysiological characterization of interlami.pdf;/Users/daniekru/Zotero/storage/4JDS9LCY/j.1460-9568.2003.03046.html}
}

@book{konradkording2022,
  author = {{Konrad, Kording} and {Ma, Wei Ji} and {Goldreich, Daniel}},
  year = {2022},
  publisher = {MIT press},
  file = {/Users/daniekru/Zotero/storage/H2G6L8Z5/Bayesian_models_of_perception_and_action_v3.pdf}
}

@article{krishnanRewardExpectationExtinction2022,
  title = {Reward Expectation Extinction Restructures and Degrades {{CA1}} Spatial Maps through Loss of a Dopaminergic Reward Proximity Signal},
  author = {Krishnan, Seetha and Heer, Chad and Cherian, Chery and Sheffield, Mark E. J.},
  year = {2022},
  month = nov,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {6662},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-34465-5},
  urldate = {2025-02-22},
  abstract = {Hippocampal place cells support reward-related spatial memories by forming a cognitive map that over-represents reward locations. The strength of these memories is modulated by the extent of reward expectation during encoding. However, the circuit mechanisms underlying this modulation are unclear. Here we find that when reward expectation is extinguished in mice, they remain engaged with their environment, yet place cell over-representation of rewards vanishes, place field remapping throughout the environment increases, and place field trial-to-trial reliability decreases. Interestingly, Ventral Tegmental Area (VTA) dopaminergic axons in CA1 exhibit a ramping reward-proximity signal that depends on reward expectation and inhibiting VTA dopaminergic neurons largely replicates the effects of extinguishing reward expectation. We conclude that changing reward expectation restructures CA1 cognitive maps and determines map reliability by modulating the dopaminergic VTA-CA1 reward-proximity signal. Thus, internal states of high reward expectation enhance encoding of spatial memories by reinforcing hippocampal cognitive maps associated with reward.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Hippocampus,Reward,Spatial memory},
  file = {/Users/daniekru/Zotero/storage/VQYNKWZ7/Krishnan et al. - 2022 - Reward expectation extinction restructures and degrades CA1 spatial maps through loss of a dopaminer.pdf}
}

@article{kropffSpeedCellsMedial2015,
  title = {Speed Cells in the Medial Entorhinal Cortex},
  author = {Kropff, Emilio and Carmichael, James E. and Moser, May-Britt and Moser, Edvard I.},
  year = {2015},
  month = jul,
  journal = {Nature},
  volume = {523},
  number = {7561},
  pages = {419--424},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14622},
  urldate = {2025-02-22},
  abstract = {Grid cells in the medial entorhinal cortex have spatial firing fields that repeat periodically in a hexagonal pattern. When animals move, activity is translated between grid cells in accordance with the animal's displacement in the environment. For this translation to occur, grid cells must have continuous access to information about instantaneous running speed. However, a powerful entorhinal speed signal has not been identified. Here we show that running speed is represented in the firing rate of a ubiquitous but functionally dedicated population of entorhinal neurons distinct from other cell populations of the local circuit, such as grid, head-direction and border cells. These `speed cells' are characterized by a context-invariant positive, linear response to running speed, and share with grid cells a prospective bias of {$\sim$}50--80 ms. Our observations point to speed cells as a key component of the dynamic representation of self-location in the medial entorhinal cortex.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Network models},
  file = {/Users/daniekru/Zotero/storage/KMKEDU64/Kropff et al. - 2015 - Speed cells in the medial entorhinal cortex.pdf}
}

@article{kubieSpatialFrequenciesGrid2015,
  title = {Do the Spatial Frequencies of Grid Cells Mold the Firing Fields of Place Cells?},
  author = {Kubie, John L. and Fox, Steven E.},
  year = {2015},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {13},
  pages = {3860--3861},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1503155112},
  urldate = {2023-11-03},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/5KK7JG5W/Kubie and Fox - 2015 - Do the spatial frequencies of grid cells mold the .pdf}
}

@article{kubieSpatialFrequenciesGrid2015a,
  title = {Do the Spatial Frequencies of Grid Cells Mold the Firing Fields of Place Cells?},
  author = {Kubie, John L. and Fox, Steven E.},
  year = {2015},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {13},
  pages = {3860--3861},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1503155112},
  urldate = {2023-11-09},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/7854D6NP/Kubie and Fox - 2015 - Do the spatial frequencies of grid cells mold the .pdf}
}

@article{kudithipudiBiologicalUnderpinningsLifelong2022,
  title = {Biological Underpinnings for Lifelong Learning Machines},
  author = {Kudithipudi, Dhireesha and {Aguilar-Simon}, Mario and Babb, Jonathan and Bazhenov, Maxim and Blackiston, Douglas and Bongard, Josh and Brna, Andrew P. and Chakravarthi Raja, Suraj and Cheney, Nick and Clune, Jeff and Daram, Anurag and Fusi, Stefano and Helfer, Peter and Kay, Leslie and Ketz, Nicholas and Kira, Zsolt and Kolouri, Soheil and Krichmar, Jeffrey L. and Kriegman, Sam and Levin, Michael and Madireddy, Sandeep and Manicka, Santosh and Marjaninejad, Ali and McNaughton, Bruce and Miikkulainen, Risto and Navratilova, Zaneta and Pandit, Tej and Parker, Alice and Pilly, Praveen K. and Risi, Sebastian and Sejnowski, Terrence J. and Soltoggio, Andrea and Soures, Nicholas and Tolias, Andreas S. and {Urbina-Mel{\'e}ndez}, Dar{\'i}o and {Valero-Cuevas}, Francisco J. and {van de Ven}, Gido M. and Vogelstein, Joshua T. and Wang, Felix and Weiss, Ron and {Yanguas-Gil}, Angel and Zou, Xinyun and Siegelmann, Hava},
  year = {2022},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {196--210},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00452-0},
  urldate = {2022-09-21},
  abstract = {Biological organisms learn from interactions with their environment throughout their lifetime. For artificial systems to successfully act and adapt in the real world, it is desirable to similarly be able to learn on a continual basis. This challenge is known as lifelong learning, and remains to a large extent unsolved. In this Perspective article, we identify a set of key capabilities that artificial systems will need to achieve lifelong learning. We describe a number of biological mechanisms, both neuronal and non-neuronal, that help explain how organisms solve these challenges, and present examples of biologically inspired models and biologically plausible mechanisms that have been applied to artificial systems in the quest towards development of lifelong learning machines. We discuss opportunities to further our understanding and advance the state of the art in lifelong learning, aiming to bridge the gap between natural and artificial intelligence.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Intelligence,Learning algorithms},
  file = {/Users/daniekru/Zotero/storage/QECJ6HNY/Kudithipudi et al. - 2022 - Biological underpinnings for lifelong learning mac.pdf;/Users/daniekru/Zotero/storage/M6T4ZSMJ/s42256-022-00452-0.html}
}

@article{kumaranWhatLearningSystems2016,
  title = {What {{Learning Systems}} Do {{Intelligent Agents Need}}? {{Complementary Learning Systems Theory Updated}}},
  shorttitle = {What {{Learning Systems}} Do {{Intelligent Agents Need}}?},
  author = {Kumaran, Dharshan and Hassabis, Demis and McClelland, James L.},
  year = {2016},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  volume = {20},
  number = {7},
  pages = {512--534},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2016.05.004},
  urldate = {2024-04-30},
  langid = {english},
  pmid = {27315762},
  keywords = {artificial intelligence,hippocampus,learning,memory,to study},
  file = {/Users/daniekru/Zotero/storage/NT99Q6AH/Kumaran et al. - 2016 - What Learning Systems do Intelligent Agents Need .pdf}
}

@article{kuoLearningHypothesisSpatial,
  title = {The {{Learning Hypothesis}} on {{Spatial Receptive Field Remapping}}},
  author = {Kuo, Henry and Masset, Paul and Bordelon, Blake and Pehlevan, Cengiz},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/LQHTHLDP/Kuo et al. - The Learning Hypothesis on Spatial Receptive Field Remapping.pdf}
}

@book{lairdSoarCognitiveArchitecture2012,
  title = {The {{Soar Cognitive Architecture}}},
  author = {Laird, John E.},
  year = {2012},
  month = apr,
  publisher = {MIT Press},
  abstract = {The definitive presentation of Soar, one AI's most enduring architectures, offering comprehensive descriptions of fundamental aspects and new components. In development for thirty years, Soar is a general cognitive architecture that integrates knowledge-intensive reasoning, reactive execution, hierarchical reasoning, planning, and learning from experience, with the goal of creating a general computational system that has the same cognitive abilities as humans. In contrast, most AI systems are designed to solve only one type of problem, such as playing chess, searching the Internet, or scheduling aircraft departures. Soar is both a software system for agent development and a theory of what computational structures are necessary to support human-level agents. Over the years, both software system and theory have evolved. This book offers the definitive presentation of Soar from theoretical and practical perspectives, providing comprehensive descriptions of fundamental aspects and new components.The current version of Soar features major extensions, adding reinforcement learning, semantic memory, episodic memory, mental imagery, and an appraisal-based model of emotion. This book describes details of Soar's component memories and processes and offers demonstrations of individual components, components working in combination, and real-world applications. Beyond these functional considerations, the book also proposes requirements for general cognitive architectures and explicitly evaluates how well Soar meets those requirements.},
  googlebooks = {Z9bxCwAAQBAJ},
  isbn = {978-0-262-30035-3},
  langid = {english},
  keywords = {Computers / Intelligence (AI) & Semantics}
}

@misc{langeDiscoveringEvolutionStrategies2023,
  title = {Discovering {{Evolution Strategies}} via {{Meta-Black-Box Optimization}}},
  author = {Lange, Robert Tjarko and Schaul, Tom and Chen, Yutian and Zahavy, Tom and Dallibard, Valentin and Lu, Chris and Singh, Satinder and Flennerhag, Sebastian},
  year = {2023},
  month = mar,
  number = {arXiv:2211.11260},
  eprint = {2211.11260},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.11260},
  urldate = {2023-11-29},
  abstract = {Optimizing functions without access to gradients is the remit of black-box methods such as evolution strategies. While highly general, their learning dynamics are often times heuristic and inflexible - exactly the limitations that meta-learning can address. Hence, we propose to discover effective update rules for evolution strategies via meta-learning. Concretely, our approach employs a search strategy parametrized by a self-attention-based architecture, which guarantees the update rule is invariant to the ordering of the candidate solutions. We show that meta-evolving this system on a small set of representative low-dimensional analytic optimization problems is sufficient to discover new evolution strategies capable of generalizing to unseen optimization problems, population sizes and optimization horizons. Furthermore, the same learned evolution strategy can outperform established neuroevolution baselines on supervised and continuous control tasks. As additional contributions, we ablate the individual neural network components of our method; reverse engineer the learned strategy into an explicit heuristic form, which remains highly competitive; and show that it is possible to self-referentially train an evolution strategy from scratch, with the learned update rule used to drive the outer meta-learning loop.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/daniekru/Zotero/storage/7XXKX3EI/Lange et al. - 2023 - Discovering Evolution Strategies via Meta-Black-Bo.pdf;/Users/daniekru/Zotero/storage/Y76GVRWG/2211.html}
}

@article{lansdellNeuralSpikingCausal2023,
  title = {Neural Spiking for Causal Inference and Learning},
  author = {Lansdell, Benjamin James and Kording, Konrad Paul},
  year = {2023},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {19},
  number = {4},
  pages = {e1011005},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1011005},
  urldate = {2023-04-19},
  abstract = {When a neuron is driven beyond its threshold, it spikes. The fact that it does not communicate its continuous membrane potential is usually seen as a computational liability. Here we show that this spiking mechanism allows neurons to produce an unbiased estimate of their causal influence, and a way of approximating gradient descent-based learning. Importantly, neither activity of upstream neurons, which act as confounders, nor downstream non-linearities bias the results. We show how spiking enables neurons to solve causal estimation problems and that local plasticity can approximate gradient descent using spike discontinuity learning.},
  langid = {english},
  keywords = {Action potentials,Artificial neural networks,Learning,Network analysis,Neural networks,Neuronal plasticity,Neurons,Sensory perception,to study},
  file = {/Users/daniekru/Zotero/storage/27F6YK3R/Lansdell and Kording - 2023 - Neural spiking for causal inference and learning.pdf}
}

@article{laptevNeuralDynamicsIndicate2019,
  title = {Neural {{Dynamics Indicate Parallel Integration}} of {{Environmental}} and {{Self-Motion Information}} by {{Place}} and {{Grid Cells}}},
  author = {Laptev, Dmitri and Burgess, Neil},
  year = {2019},
  journal = {Frontiers in Neural Circuits},
  volume = {13},
  issn = {1662-5110},
  urldate = {2023-11-03},
  abstract = {Place cells and grid cells in the hippocampal formation are thought to integrate sensory and self-motion information into a representation of estimated spatial location, but the precise mechanism is unknown. We simulated a parallel attractor system in which place cells form an attractor network driven by environmental inputs and grid cells form an attractor network performing path integration driven by self-motion, with inter-connections between them allowing both types of input to influence firing in both ensembles. We show that such a system is needed to explain the spatial patterns and temporal dynamics of place cell firing when rats run on a linear track in which the familiar correspondence between environmental and self-motion inputs is changed. In contrast, the alternative architecture of a single recurrent network of place cells (performing path integration and receiving environmental inputs) cannot reproduce the place cell firing dynamics. These results support the hypothesis that grid and place cells provide two different but complementary attractor representations (based on self-motion and environmental sensory inputs, respectively). Our results also indicate the specific neural mechanism and main predictors of hippocampal map realignment and make predictions for future studies.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/ETZ4P754/Laptev and Burgess - 2019 - Neural Dynamics Indicate Parallel Integration of E.pdf}
}

@article{laptevNeuralDynamicsIndicate2019a,
  title = {Neural {{Dynamics Indicate Parallel Integration}} of {{Environmental}} and {{Self-Motion Information}} by {{Place}} and {{Grid Cells}}},
  author = {Laptev, Dmitri and Burgess, Neil},
  year = {2019},
  journal = {Frontiers in Neural Circuits},
  volume = {13},
  issn = {1662-5110},
  urldate = {2023-11-09},
  abstract = {Place cells and grid cells in the hippocampal formation are thought to integrate sensory and self-motion information into a representation of estimated spatial location, but the precise mechanism is unknown. We simulated a parallel attractor system in which place cells form an attractor network driven by environmental inputs and grid cells form an attractor network performing path integration driven by self-motion, with inter-connections between them allowing both types of input to influence firing in both ensembles. We show that such a system is needed to explain the spatial patterns and temporal dynamics of place cell firing when rats run on a linear track in which the familiar correspondence between environmental and self-motion inputs is changed. In contrast, the alternative architecture of a single recurrent network of place cells (performing path integration and receiving environmental inputs) cannot reproduce the place cell firing dynamics. These results support the hypothesis that grid and place cells provide two different but complementary attractor representations (based on self-motion and environmental sensory inputs, respectively). Our results also indicate the specific neural mechanism and main predictors of hippocampal map realignment and make predictions for future studies.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/2Y5TCDBR/Laptev and Burgess - 2019 - Neural Dynamics Indicate Parallel Integration of E.pdf}
}

@article{laraRolePrefrontalCortex2015,
  title = {The {{Role}} of {{Prefrontal Cortex}} in {{Working Memory}}: {{A Mini Review}}},
  shorttitle = {The {{Role}} of {{Prefrontal Cortex}} in {{Working Memory}}},
  author = {Lara, Antonio H. and Wallis, Jonathan D.},
  year = {2015},
  month = dec,
  journal = {Frontiers in Systems Neuroscience},
  volume = {9},
  publisher = {Frontiers},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2015.00173},
  urldate = {2024-05-11},
  abstract = {A prominent account of prefrontal cortex (PFC) function is that single neurons within the PFC maintain representations of task-relevant stimuli in working memory. Evidence for this view comes from studies in which subjects hold a stimulus across a delay lasting up to several seconds. Persistent elevated activity in the PFC has been observed in animal models as well as in humans performing these tasks. This persistent activity has been interpreted as evidence for the encoding of the stimulus itself in working memory. However, recent findings have posed a challenge to this notion. A number of recent studies have examined neural data from the PFC and posterior sensory areas, both at the single neuron level in primates, and at a larger scale in humans, and have failed to find encoding of stimulus information in the PFC during tasks with a substantial working memory component. Strong stimulus related information, however, was seen in posterior sensory areas. These results suggest that delay period activity in the PFC might be better understood not as a signature of memory storage per se, but as a top down signal that influences posterior sensory areas where the actual working memory representations are maintained.},
  langid = {english},
  keywords = {Attention,Executive Function,frontoparietal network,Prefrontal Cortex,working memory},
  file = {/Users/daniekru/Zotero/storage/PKTASAWQ/Lara and Wallis - 2015 - The Role of Prefrontal Cortex in Working Memory A.pdf}
}

@article{larkumAreDendritesConceptually2022,
  title = {Are {{Dendrites Conceptually Useful}}?},
  author = {Larkum, Matthew E.},
  year = {2022},
  month = may,
  journal = {Neuroscience},
  series = {Dendritic Contributions to Biological and Artificial Computations},
  volume = {489},
  pages = {4--14},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2022.03.008},
  urldate = {2022-09-15},
  abstract = {This article presents the argument that, while understanding the brain will require a multi-level approach, there is nevertheless something fundamental about understanding the components of the brain. I argue here that the standard description of neurons is not merely too simplistic, but also misses the true nature of how they operate at the computational level. In particular, the humble point neuron, devoid of dendrites with their powerful computational properties, prevents conceptual progress at higher levels of understanding.},
  langid = {english},
  keywords = {integrate-and-fire,intrinsic excitability,levels of understanding,networks,NMDA spikes,point neurons},
  file = {/Users/daniekru/Zotero/storage/ZYFCQXIR/Larkum - 2022 - Are Dendrites Conceptually Useful.pdf;/Users/daniekru/Zotero/storage/YADRSYTT/S0306452222001208.html}
}

@article{larsenSynapsetypespecificPlasticityLocal2015,
  title = {Synapse-Type-Specific Plasticity in Local Circuits},
  author = {Larsen, Rylan S and Sj{\"o}str{\"o}m, P Jesper},
  year = {2015},
  month = dec,
  journal = {Current opinion in neurobiology},
  volume = {35},
  pages = {127--135},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2015.08.001},
  urldate = {2024-12-10},
  abstract = {Neuroscientists spent decades debating whether synaptic plasticity was presynaptically or postsynaptically expressed. It was eventually concluded that plasticity depends on many factors, including cell type. More recently, it has become increasingly clear that plasticity is regulated at an even finer grained level; it is specific to the synapse type, a concept we denote synapse-type-specific plasticity (STSP). Here, we review recent developments in the field of STSP, discussing both long-term and short-term variants and with particular emphasis on neocortical function. As there are dozens of neocortical cell types, there is a multiplicity of forms of STSP, the vast majority of which have never been explored. We argue that to understand the brain and synaptic diseases, we have to grapple with STSP.},
  pmcid = {PMC5280068},
  pmid = {26310110},
  file = {/Users/daniekru/Zotero/storage/TBKZ7FZ5/Larsen and Sjöström - 2015 - Synapse-type-specific plasticity in local circuits.pdf}
}

@article{laskowskiRoleMedialPrefrontal2016,
  title = {The Role of the Medial Prefrontal Cortex in Updating Reward Value and Avoiding Perseveration},
  author = {Laskowski, C. S. and Williams, R. J. and Martens, K. M. and Gruber, A. J. and Fisher, K. G. and Euston, D. R.},
  year = {2016},
  month = jun,
  journal = {Behavioural Brain Research},
  volume = {306},
  pages = {52--63},
  issn = {0166-4328},
  doi = {10.1016/j.bbr.2016.03.007},
  urldate = {2024-04-29},
  abstract = {The medial prefrontal cortex (mPFC) plays a major role in goal-directed behaviours, but it is unclear whether it plays a role in breaking away from a high-value reward in order to explore for better options. To address this question, we designed a novel 3-arm Bandit Task in which rats were required to choose one of three potential reward arms, each of which was associated with a different amount of food reward and time-out punishment. After a variable number of choice trials the reward locations were shuffled and animals had to disengage from the now devalued arm and explore the other options in order to optimise payout. Lesion and control groups' behaviours on the task were then analysed by fitting data with a reinforcement learning model. As expected, lesioned animals obtained less reward overall due to an inability to flexibly adapt their behaviours after a change in reward location. However, modelling results showed that lesioned animals were no more likely to explore than control animals. We also discovered that all animals showed a strong preference for certain maze arms, at the expense of reward. This tendency was exacerbated in the lesioned animals, with the strongest effects seen in a subset of animals with damage to dorsal mPFC. The results confirm a role for mPFC in goal-directed behaviours but suggest that rats rely on other areas to resolve the explore-exploit dilemma.},
  keywords = {Decision making,Exploration,Prefrontal cortex,Reinforcement learning,Reward,Value},
  file = {/Users/daniekru/Zotero/storage/X52KZDDB/S0166432816301322.html}
}

@article{leeBraininspiredPredictiveCoding2022,
  title = {Brain-Inspired {{Predictive Coding Improves}} the {{Performance}} of {{Machine Challenging Tasks}}},
  author = {Lee, Jangho and Jo, Jeonghee and Lee, Byounghwa and Lee, Jung-Hoon and Yoon, Sungroh},
  year = {2022},
  journal = {Frontiers in Computational Neuroscience},
  volume = {16},
  pages = {1062678},
  issn = {1662-5188},
  doi = {10.3389/fncom.2022.1062678},
  abstract = {Backpropagation has been regarded as the most favorable algorithm for training artificial neural networks. However, it has been criticized for its biological implausibility because its learning mechanism contradicts the human brain. Although backpropagation has achieved super-human performance in various machine learning applications, it often shows limited performance in specific tasks. We collectively referred to such tasks as machine-challenging tasks (MCTs) and aimed to investigate methods to enhance machine learning for MCTs. Specifically, we start with a natural question: Can a learning mechanism that mimics the human brain lead to the improvement of MCT performances? We hypothesized that a learning mechanism replicating the human brain is effective for tasks where machine intelligence is difficult. Multiple experiments corresponding to specific types of MCTs where machine intelligence has room to improve performance were performed using predictive coding, a more biologically plausible learning algorithm than backpropagation. This study regarded incremental learning, long-tailed, and few-shot recognition as representative MCTs. With extensive experiments, we examined the effectiveness of predictive coding that robustly outperformed backpropagation-trained networks for the MCTs. We demonstrated that predictive coding-based incremental learning alleviates the effect of catastrophic forgetting. Next, predictive coding-based learning mitigates the classification bias in long-tailed recognition. Finally, we verified that the network trained with predictive coding could correctly predict corresponding targets with few samples. We analyzed the experimental result by drawing analogies between the properties of predictive coding networks and those of the human brain and discussing the potential of predictive coding networks in general machine learning.},
  langid = {english},
  pmcid = {PMC9709416},
  pmid = {36465966},
  keywords = {backpropagation,biologically plausible learning,brain-inspired learning,deep learning,predictive coding},
  file = {/Users/daniekru/Zotero/storage/Z7MDZAVM/Lee et al. - 2022 - Brain-inspired Predictive Coding Improves the Performance of Machine Challenging Tasks.pdf}
}

@article{leeComparisonPopulationCoherence2004,
  title = {Comparison of Population Coherence of Place Cells in Hippocampal Subfields {{CA1}} and {{CA3}}},
  author = {Lee, Inah and Yoganarasimha, D. and Rao, Geeta and Knierim, James J.},
  year = {2004},
  month = jul,
  journal = {Nature},
  volume = {430},
  number = {6998},
  pages = {456--459},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature02739},
  urldate = {2023-03-06},
  abstract = {The hippocampus, a critical brain structure for navigation, context-dependent learning and episodic memory1,2,3, is composed of anatomically heterogeneous subregions. These regions differ in their anatomical inputs as well as in their internal circuitry4. A major feature of the CA3 region is its recurrent collateral circuitry, by which the CA3 pyramidal cells make excitatory synaptic contacts on each other4,5. In contrast, pyramidal cells in the CA1 region are not extensively interconnected4. Although these differences have inspired numerous theoretical models of differential processing capacities of these two regions6,7,8,9,10,11,12,13, there have been few reports of robust differences in the firing properties of CA1 and CA3 neurons in behaving animals. The most extensively studied of these properties is the spatially selective firing of hippocampal `place cells'1,14. Here we report that in a dynamically changing environment, in which familiar landmarks on the behavioural track and along the wall are rotated relative to each other15,16, the population representation of the environment is more coherent between the original and cue-altered environments in CA3 than in CA1. These results demonstrate a functional heterogeneity between the place cells of CA3 and CA1 at the level of neural population representations.},
  copyright = {2004 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/daniekru/Zotero/storage/68XML4U4/Lee et al. - 2004 - Comparison of population coherence of place cells .pdf}
}

@inproceedings{leeRegularizedAutoencodersIsometric2021,
  title = {Regularized {{Autoencoders}} for {{Isometric Representation Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lee, Yonghyeon and Yoon, Sangwoong and Son, MinJun and Park, Frank C.},
  year = {2021},
  month = oct,
  urldate = {2023-10-17},
  abstract = {The recent success of autoencoders for representation learning can be traced in large part to the addition of a regularization term. Such regularized autoencoders ``constrain" the representation so as to prevent overfitting to the data while producing a parsimonious generative model. A regularized autoencoder should in principle learn not only the data manifold, but also a set of geometry-preserving coordinates for the latent representation space; by geometry-preserving we mean that the latent space representation should attempt to preserve actual distances and angles on the data manifold. In this paper we first formulate a hierarchy for geometry-preserving mappings (isometry, conformal mapping of degree \$k\$, area-preserving mappings). We then show that a conformal regularization term of degree zero -- i.e., one that attempts to preserve angles and relative distances, instead of angles and exact distances -- produces data representations that are superior to other existing methods. Applying our algorithm to an unsupervised information retrieval task for CelebA data with 40 annotations, we achieve 79{\textbackslash}\% precision at five retrieved images, an improvement of more than 10{\textbackslash}\% compared to recent related work. Code is available at https://github.com/Gabe-YHLee/IRVAE-public.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/8URHU49E/Lee et al. - 2021 - Regularized Autoencoders for Isometric Representat.pdf}
}

@article{lehrCA2SocialMemory2021,
  title = {{{CA2}} beyond Social Memory: {{Evidence}} for a Fundamental Role in Hippocampal Information Processing},
  shorttitle = {{{CA2}} beyond Social Memory},
  author = {Lehr, Andrew B. and Kumar, Arvind and Tetzlaff, Christian and Hafting, Torkel and Fyhn, Marianne and St{\"o}ber, Tristan M.},
  year = {2021},
  month = jul,
  journal = {Neuroscience \& Biobehavioral Reviews},
  volume = {126},
  pages = {398--412},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2021.03.020},
  urldate = {2023-03-20},
  abstract = {Hippocampal region CA2 has received increased attention due to its importance in social recognition memory. While its specific function remains to be identified, there are indications that CA2 plays a major role in a variety of situations, widely extending beyond social memory. In this targeted review, we highlight lines of research which have begun to converge on a more fundamental role for CA2 in hippocampus-dependent memory processing. We discuss recent proposals that speak to the computations CA2 may perform within the hippocampal circuit.},
  langid = {english},
  keywords = {CA2,Hippocampus,Learning and memory,Social memory},
  file = {/Users/daniekru/Zotero/storage/F887JI4E/Lehr et al. - 2021 - CA2 beyond social memory Evidence for a fundament.pdf;/Users/daniekru/Zotero/storage/D9XNGPTR/S0149763421001354.html}
}

@article{liBrainInspiredComputingSystematic2024,
  title = {Brain-{{Inspired Computing}}: {{A Systematic Survey}} and {{Future Trends}}},
  shorttitle = {Brain-{{Inspired Computing}}},
  author = {Li, Guoqi and Deng, Lei and Tang, Huajin and Pan, Gang and Tian, Yonghong and Roy, Kaushik and Maass, Wolfgang},
  year = {2024},
  month = jun,
  journal = {Proceedings of the IEEE},
  volume = {112},
  number = {6},
  pages = {544--584},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2024.3429360},
  urldate = {2025-01-16},
  abstract = {Brain-inspired computing (BIC) is an emerging research field that aims to build fundamental theories, models, hardware architectures, and application systems toward more general artificial intelligence (AI) by learning from the information processing mechanisms or structures/functions of biological nervous systems. It is regarded as one of the most promising research directions for future intelligent computing in the post-Moore era. In the past few years, various new schemes in this field have sprung up to explore more general AI. These works are quite divergent in the aspects of modeling/algorithm, software tool, hardware platform, and benchmark data since BIC is an interdisciplinary field that consists of many different domains, including computational neuroscience, AI, computer science, statistical physics, material science, and microelectronics. This situation greatly impedes researchers from obtaining a clear picture and getting started in the right way. Hence, there is an urgent requirement to do a comprehensive survey in this field to help correctly recognize and analyze such bewildering methodologies. What are the key issues to enhance the development of BIC? What roles do the current mainstream technologies play in the general framework of BIC? Which techniques are truly useful in real-world applications? These questions largely remain open. To address the above issues, in this survey, we first clarify the biggest challenge of BIC: how can AI models benefit from the recent advancements in computational neuroscience? With this challenge in mind, we will focus on discussing the concept of BIC and summarize four components of BIC infrastructure development: 1) modeling/algorithm; 2) hardware platform; 3) software tool; and 4) benchmark data. For each component, we will summarize its recent progress, main challenges to resolve, and future trends. Based on these studies, we present a general framework for the real-world applications of BIC systems, which is promising to benefit both AI and brain science. Finally, we claim that it is extremely important to build a research ecology to promote prosperity continuously in this field.},
  keywords = {Artificial intelligence,Benchmark datasets,Benchmark testing,Bio-inspired computing,Biological system modeling,Brain modeling,brain-inspired computing (BIC),Computational modeling,Computer architecture,computing architecture,Neural networks,neuromorphic chips,Neuromorphic engineering,neuromorphic sensors,Sensors,software tool,Software tools,spiking neural networks (SNNs),Surveys},
  file = {/Users/daniekru/Zotero/storage/LBIH6CJN/Li et al. - 2024 - Brain-Inspired Computing A Systematic Survey and Future Trends.pdf;/Users/daniekru/Zotero/storage/ET6PVJE4/10636118.html}
}

@article{lillicrapRandomSynapticFeedback2016,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  year = {2016},
  month = nov,
  journal = {Nature Communications},
  volume = {7},
  number = {1},
  pages = {13276},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/ncomms13276},
  urldate = {2023-05-04},
  abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Learning algorithms,to study},
  file = {/Users/daniekru/Zotero/storage/AX7X2VS2/Lillicrap et al. - 2016 - Random synaptic feedback weights support error bac.pdf}
}

@article{liMechanismsMemorysupportingNeuronal2024,
  title = {Mechanisms of Memory-Supporting Neuronal Dynamics in Hippocampal Area {{CA3}}},
  author = {Li, Yiding and Briguglio, John J. and Romani, Sandro and Magee, Jeffrey C.},
  year = {2024},
  month = nov,
  journal = {Cell},
  volume = {187},
  number = {24},
  pages = {6804-6819.e21},
  publisher = {Elsevier},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2024.09.041},
  urldate = {2024-12-01},
  langid = {english},
  pmid = {39454575},
  keywords = {attractor dynamics,BTSP,CA3,hippocampus,memory,place cell,synaptic plasticity,to study},
  file = {/Users/daniekru/Zotero/storage/3ACSWY5U/Li et al. - 2024 - Mechanisms of memory-supporting neuronal dynamics in hippocampal area CA3.pdf}
}

@book{liModelingPlaceCells2019,
  title = {Modeling Place Cells and Grid Cells in Multi-Compartment Environments: Hippocampal-Entorhinal Loop as a Multisensory Integration Circuit},
  shorttitle = {Modeling Place Cells and Grid Cells in Multi-Compartment Environments},
  author = {Li, Tianyi and Arleo, Angelo and Sheynikhovich, Denis},
  year = {2019},
  month = apr,
  doi = {10.1101/602235},
  abstract = {Hippocampal place cells and entorhinal grid cells are thought to form a representation of space by integrating internal and external sensory cues. Experimental studies show that different subsets of place cells are controlled by vision, self-motion or a combination of both. Moreover, recent studies in environments with a high degree of visual aliasing suggest that a continuous interaction between place cells and grid cells can result in a deformation of hexagonal grids or in a progressive loss of visual cue control. The computational nature of such a bidirectional interaction remains unclear. In this work we present a neural network model of a dynamic loop between place cells and grid cells. The model is tested in two recent experimental paradigms involving double-room environments that provide conflicting evidence about visual cue control over self-motion-based spatial codes. Analysis of the model behavior in the two experiments suggests that the strength of hippocampal-entorhinal dynamical loop is the key parameter governing differential cue control in multi-compartment environments. Construction of spatial representations in visually identical environments requires weak visual cue control, while synaptic plasticity is regulated by the mismatch between visual- and self-motion representations. More generally our results suggest a functional segregation between plastic and dynamic processes in hippocampal processing.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/7X8KUL6A/Li et al. - 2019 - Modeling place cells and grid cells in multi-compa.pdf}
}

@inproceedings{linerImprovingNeuralNetwork2021,
  title = {Improving {{Neural Network Learning Through Dual Variable Learning Rates}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Liner, Elizabeth and Miikkulainen, Risto},
  year = {2021},
  month = jul,
  pages = {1--7},
  issn = {2161-4407},
  doi = {10.1109/IJCNN52387.2021.9533487},
  urldate = {2024-12-02},
  abstract = {This paper introduces and evaluates a novel training method for neural networks: Dual Variable Learning Rates (DVLR). Building on insights from behavioral psychology, the dual learning rates are used to emphasize correct and incorrect responses differently, thereby making the feedback to the network more specific. Further, the learning rates are varied as a function of the network's performance, thereby making it more efficient. DVLR was implemented on three types of networks: feedforward, convolutional, and residual, and two domains: MNIST and CIFAR-10. The results suggest a consistently improved accuracy, demonstrating that DVLR is a promising, psychologically motivated technique for training neural network models.},
  keywords = {Buildings,Machine learning,Neural networks,Psychology,Residual neural networks,Schedules,Training},
  file = {/Users/daniekru/Zotero/storage/H3JIEI3K/Liner and Miikkulainen - 2021 - Improving Neural Network Learning Through Dual Variable Learning Rates.pdf;/Users/daniekru/Zotero/storage/AXR9KM29/9533487.html}
}

@inproceedings{linskerApplicationPrincipleMaximum1988,
  title = {An {{Application}} of the {{Principle}} of {{Maximum Information Preservation}} to {{Linear Systems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Linsker, Ralph},
  year = {1988},
  volume = {1},
  publisher = {Morgan-Kaufmann},
  urldate = {2022-11-18},
  abstract = {This paper addresses the problem of determining the weights for a  set  of  linear  filters  (model  "cells")  so  as  to  maximize  the  ensemble-averaged information that the cells' output values jointly  convey about their input values,  given  the  statistical properties of  the ensemble of input vectors.  The quantity that is maximized is the  Shannon  information  rate,  or  equivalently  the  average  mutual  information between input and output.  Several models for the role  of processing noise are analyzed, and the biological motivation for  considering them is described.  For simple models in which nearby  input  signal  values  (in  space  or  time)  are  correlated,  the  cells  resulting  from  this  optimization  process  include  center-surround  cells and cells sensitive to temporal variations in input signal.},
  keywords = {optional},
  file = {/Users/daniekru/Zotero/storage/H78SRLHB/Linsker - 1988 - An Application of the Principle of Maximum Informa.pdf}
}

@article{linskerLocalLearningRule1997,
  title = {A {{Local Learning Rule That Enables Information Maximization}} for {{Arbitrary Input Distributions}}},
  author = {Linsker, Ralph},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1661--1665},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1661},
  urldate = {2022-11-18},
  abstract = {This note presents a local learning rule that enables a network to maximize the mutual information between input and output vectors. The network's output units may be nonlinear, and the distribution of input vectors is arbitrary. The local algorithm also serves to compute the inverse C               -1               of an arbitrary square connection weight matrix.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Downloads/linsker1997.pdf}
}

@misc{liRapidMemoryEncoding2023,
  title = {Rapid Memory Encoding in a Recurrent Network Model with Behavioral Time Scale Synaptic Plasticity {\textbar} {{bioRxiv}}},
  author = {Li, Pan Le and Roxin, Alex},
  year = {2023},
  urldate = {2023-05-02},
  howpublished = {https://www.biorxiv.org/content/10.1101/2023.05.02.539020v1},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/747TLWXK/Rapid memory encoding in a recurrent network model.pdf;/Users/daniekru/Zotero/storage/NNFXDFNU/2023.05.02.html}
}

@article{liRapidMemoryEncoding2023a,
  title = {Rapid Memory Encoding in a Recurrent Network Model with Behavioral Time Scale Synaptic Plasticity},
  author = {Li, Pan Ye and Roxin, Alex},
  year = {2023},
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {19},
  number = {8},
  pages = {e1011139},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1011139},
  urldate = {2024-04-30},
  abstract = {Episodic memories are formed after a single exposure to novel stimuli. The plasticity mechanisms underlying such fast learning still remain largely unknown. Recently, it was shown that cells in area CA1 of the hippocampus of mice could form or shift their place fields after a single traversal of a virtual linear track. In-vivo intracellular recordings in CA1 cells revealed that previously silent inputs from CA3 could be switched on when they occurred within a few seconds of a dendritic plateau potential (PP) in the post-synaptic cell, a phenomenon dubbed Behavioral Time-scale Plasticity (BTSP). A recently developed computational framework for BTSP in which the dynamics of synaptic traces related to the pre-synaptic activity and post-synaptic PP are explicitly modelled, can account for experimental findings. Here we show that this model of plasticity can be further simplified to a 1D map which describes changes to the synaptic weights after a single trial. We use a temporally symmetric version of this map to study the storage of a large number of spatial memories in a recurrent network, such as CA3. Specifically, the simplicity of the map allows us to calculate the correlation of the synaptic weight matrix with any given past environment analytically. We show that the calculated memory trace can be used to predict the emergence and stability of bump attractors in a high dimensional neural network model endowed with BTSP.},
  langid = {english},
  keywords = {Biophysics,Memory,Network analysis,Neural networks,Neuronal plasticity,Neurons,Synapses,Synaptic plasticity,to study},
  file = {/Users/daniekru/Zotero/storage/HZ294VD5/Li and Roxin - 2023 - Rapid memory encoding in a recurrent network model.pdf}
}

@article{lismanHippocampalVTALoopControlling2005,
  title = {The {{Hippocampal-VTA Loop}}: {{Controlling}} the {{Entry}} of {{Information}} into {{Long-Term Memory}}},
  shorttitle = {The {{Hippocampal-VTA Loop}}},
  author = {Lisman, John E. and Grace, Anthony A.},
  year = {2005},
  month = jun,
  journal = {Neuron},
  volume = {46},
  number = {5},
  pages = {703--713},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2005.05.002},
  urldate = {2023-04-12},
  abstract = {In this article we develop the concept that the hippocampus and the midbrain dopaminergic neurons of the ventral tegmental area (VTA) form a functional loop. Activation of the loop begins when the hippocampus detects newly arrived information that is not already stored in its long-term memory. The resulting novelty signal is conveyed through the subiculum, accumbens, and ventral pallidum to the VTA where it contributes (along with salience and goal information) to the novelty-dependent firing of these cells. In the upward arm of the loop, dopamine (DA) is released within the hippocampus; this produces an enhancement of LTP and learning. These findings support a model whereby the hippocampal-VTA loop regulates the entry of information into long-term memory.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/QVDPU6ZU/Lisman and Grace - 2005 - The Hippocampal-VTA Loop Controlling the Entry of.pdf}
}

@article{lismanNeoHebbianFrameworkEpisodic2011,
  title = {A {{neoHebbian}} Framework for Episodic Memory; Role of Dopamine-Dependent Late {{LTP}}},
  author = {Lisman, John and Grace, Anthony A. and Duzel, Emrah},
  year = {2011},
  month = oct,
  journal = {Trends in Neurosciences},
  series = {Special {{Issue}}: {{Hippocampus}} and {{Memory}}},
  volume = {34},
  number = {10},
  pages = {536--547},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2011.07.006},
  urldate = {2023-05-09},
  abstract = {According to the Hebb rule, the change in the strength of a synapse depends only on the local interaction of presynaptic and postsynaptic events. Studies at many types of synapses indicate that the early phase of long-term potentiation (LTP) has Hebbian properties. However, it is now clear that the Hebb rule does not account for late LTP; this requires an additional signal that is non-local. For novel information and motivational events such as rewards this signal at hippocampal CA1 synapses is mediated by the neuromodulator, dopamine. In this Review we discuss recent experimental findings that support the view that this `neoHebbian' framework can account for memory behavior in a variety of learning situations.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/UAJ69H6D/Lisman et al. - 2011 - A neoHebbian framework for episodic memory\; role o.pdf;/Users/daniekru/Zotero/storage/FLZ43A3P/S016622361100110X.html}
}

@article{lismanViewpointsHowHippocampus2017,
  title = {Viewpoints: How the Hippocampus Contributes to Memory, Navigation and Cognition},
  shorttitle = {Viewpoints},
  author = {Lisman, John and Buzs{\'a}ki, Gy{\"o}rgy and Eichenbaum, Howard and Nadel, Lynn and Ranganath, Charan and Redish, A. David},
  year = {2017},
  month = oct,
  journal = {Nature neuroscience},
  volume = {20},
  number = {11},
  pages = {1434--1447},
  issn = {1097-6256},
  doi = {10.1038/nn.4661},
  urldate = {2023-01-16},
  abstract = {The hippocampus serves a critical function in memory, navigation, and cognition. Nature Neuroscience asked John Lisman to lead a group of researchers in a dialog on shared and distinct viewpoints on the hippocampus., There has been a long history of studying the hippocampus, but recent work has made it possible to study the cellular and network basis of defined operations---operations that include cognitive processes that have been otherwise difficult to study (see Box 1 for useful terminology). These operations deal with the context-dependent representation of complex memories, the role of mental exploration based on imagined rather than real movements, and the use of recalled information for navigation and decision-making. The progress that has been made in understanding the hippocampus has motivated the study of other brain regions that provide hippocampal input or receive hippocampal output; the hippocampus is thus serving as a nucleating point for the larger goal of understanding the neural codes that allow inter-regional communication and more generally, understanding how memory-guided behavior is achieved by large scale integration of brain regions. In generating a discussion among experts in the study of the cognitive processes of the hippocampus, the editors and I have posed questions that probe important principles of hippocampal function. We hope that the resulting discussion will make clear to readers the progress that has been made, while also identifying issues where consensus has not yet been achieved and that should be pursued in future research. -- John Lisman},
  pmcid = {PMC5943637},
  pmid = {29073641},
  file = {/Users/daniekru/Zotero/storage/7K8VT2N6/Lisman et al. - 2017 - Viewpoints how the hippocampus contributes to mem.pdf}
}

@article{liuBiologicallyPlausibleSequence2020,
  title = {Biologically {{Plausible Sequence Learning}} with {{Spiking Neural Networks}}},
  author = {Liu, Zuozhu and Chotibut, Thiparat and Hillar, Christopher and Lin, Shaowei},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {02},
  eprint = {1911.10943},
  primaryclass = {cond-mat, q-bio},
  pages = {1316--1323},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i02.5487},
  urldate = {2023-05-27},
  abstract = {Motivated by the celebrated discrete-time model of nervous activity outlined by McCulloch and Pitts in 1943, we propose a novel continuous-time model, the McCulloch-Pitts network (MPN), for sequence learning in spiking neural networks. Our model has a local learning rule, such that the synaptic weight updates depend only on the information directly accessible by the synapse. By exploiting asymmetry in the connections between binary neurons, we show that MPN can be trained to robustly memorize multiple spatiotemporal patterns of binary vectors, generalizing the ability of the symmetric Hopfield network to memorize static spatial patterns. In addition, we demonstrate that the model can efficiently learn sequences of binary pictures as well as generative models for experimental neural spike-train data. Our learning rule is consistent with spike-timing-dependent plasticity (STDP), thus providing a theoretical ground for the systematic design of biologically inspired networks with large and robust long-range sequence storage capacity.},
  archiveprefix = {arXiv},
  keywords = {68T01 (Primary) 68T05 60J20 (Secondary),Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,I.2.11,I.2.6,I.5.1,Quantitative Biology - Neurons and Cognition,to study},
  file = {/Users/daniekru/Zotero/storage/8J8KKUH6/Liu et al. - 2020 - Biologically Plausible Sequence Learning with Spik.pdf;/Users/daniekru/Zotero/storage/FHKNL45B/1911.html}
}

@misc{liuSeeingBelievingBrainInspired2023,
  title = {Seeing Is {{Believing}}: {{Brain-Inspired Modular Training}} for {{Mechanistic Interpretability}}},
  shorttitle = {Seeing Is {{Believing}}},
  author = {Liu, Ziming and Gan, Eric and Tegmark, Max},
  year = {2023},
  month = jun,
  number = {arXiv:2305.08746},
  eprint = {2305.08746},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.08746},
  urldate = {2025-01-16},
  abstract = {We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Representation Theory,Quantitative Biology - Neurons and Cognition},
  file = {/Users/daniekru/Zotero/storage/FF6NUCAR/Liu et al. - 2023 - Seeing is Believing Brain-Inspired Modular Training for Mechanistic Interpretability.pdf;/Users/daniekru/Zotero/storage/DNG2XU74/2305.html}
}

@article{lohaniDopamineModulationPrefrontal2019,
  title = {Dopamine {{Modulation}} of {{Prefrontal Cortex Activity Is Manifold}} and {{Operates}} at {{Multiple Temporal}} and {{Spatial Scales}}},
  author = {Lohani, Sweyta and Martig, Adria K. and Deisseroth, Karl and Witten, Ilana B. and Moghaddam, Bita},
  year = {2019},
  month = apr,
  journal = {Cell Reports},
  volume = {27},
  number = {1},
  pages = {99-114.e6},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2019.03.012},
  urldate = {2024-05-07},
  abstract = {Although the function of dopamine in subcortical structures is largely limited to reward and movement, dopamine neurotransmission in the prefrontal cortex (PFC) is critical to a multitude of temporally and functionally diverse processes, such as attention, working memory, behavioral flexibility, action planning, and sustained motivational and affective states. How does dopamine influence computation of these temporally complex functions? We find causative links between sustained and burst patterns of phasic dopamine neuron activation and modulation of medial PFC neuronal activity at multiple spatiotemporal scales. These include a multidirectional and weak impact on individual neuron rate~activity but a robust influence on coordinated ensemble activity, gamma oscillations, and gamma-theta coupling that persisted for minutes. In addition, PFC network responses to burst pattern of dopamine firing were~selectively strengthened in behaviorally active states. This multiplex mode of modulation by dopamine input may enable PFC to compute and generate spatiotemporally diverse and specialized outputs.},
  keywords = {attention,cognition,ensemble activity,gamma oscillation,ventral tegmental area,working memory},
  file = {/Users/daniekru/Zotero/storage/YSAY9638/Lohani et al. - 2019 - Dopamine Modulation of Prefrontal Cortex Activity .pdf;/Users/daniekru/Zotero/storage/2KNBMVK4/S2211124719303195.html}
}

@article{lopez-ibanezReproducibilityEvolutionaryComputation2021,
  title = {Reproducibility in {{Evolutionary Computation}}},
  author = {{L{\'o}pez-ib{\'a}{\~n}ez}, Manuel and Branke, Juergen and Paquete, Lu{\'i}s},
  year = {2021},
  month = oct,
  journal = {ACM Transactions on Evolutionary Learning and Optimization},
  volume = {1},
  number = {4},
  pages = {14:1--14:21},
  doi = {10.1145/3466624},
  urldate = {2023-10-06},
  abstract = {Experimental studies are prevalent in Evolutionary Computation (EC), and concerns about the reproducibility and replicability of such studies have increased in recent times, reflecting similar concerns in other scientific fields. In this article, we discuss, within the context of EC, the different types of reproducibility and suggest a classification that refines the badge system of the Association of Computing Machinery (ACM) adopted by ACM Transactions on Evolutionary Learning and Optimization (TELO). We identify cultural and technical obstacles to reproducibility in the EC field. Finally, we provide guidelines and suggest tools that may help to overcome some of these reproducibility obstacles.},
  keywords = {benchmarking,empirical study,Evolutionary computation,reproducibility},
  file = {/Users/daniekru/Zotero/storage/RRAQX8RB/López-ibáñez et al. - 2021 - Reproducibility in Evolutionary Computation.pdf}
}

@article{lotterDeepPredictiveCoding2017,
  title = {Deep {{Predictive Coding Networks}} for {{Video Prediction}} and {{Unsupervised Learning}}},
  author = {Lotter, William and Kreiman, Gabriel and Cox, David},
  year = {2017},
  month = feb,
  journal = {arXiv:1605.08104 [cs, q-bio]},
  eprint = {1605.08104},
  primaryclass = {cs, q-bio},
  urldate = {2021-12-03},
  abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network ("PredNet") architecture that is inspired by the concept of "predictive coding" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/daniekru/Zotero/storage/HG9XXV83/Lotter et al. - 2017 - Deep Predictive Coding Networks for Video Predicti.pdf;/Users/daniekru/Zotero/storage/9AZGVQ55/1605.html}
}

@article{lukChoiceCodingFrontal2013,
  title = {Choice {{Coding}} in {{Frontal Cortex}} during {{Stimulus-Guided}} or {{Action-Guided Decision-Making}}},
  author = {Luk, Chung-Hay and Wallis, Jonathan D.},
  year = {2013},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {33},
  number = {5},
  pages = {1864--1871},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4920-12.2013},
  urldate = {2024-11-27},
  abstract = {To optimally obtain desirable outcomes, organisms must track outcomes predicted by stimuli in the environment (stimulus--outcome or SO associations) and outcomes predicted by their own actions (action--outcome or AO associations). Anterior cingulate cortex (ACC) and orbitofrontal cortex (OFC) are implicated in tracking outcomes, but anatomical and functional studies suggest a dissociation, with ACC and OFC responsible for encoding AO and SO associations, respectively. To examine whether this dissociation held at the single neuron level, we trained two subjects to perform choice tasks that required using AO or SO associations. OFC and ACC neurons encoded the action that the subject used to indicate its choice, but this encoding was stronger in OFC during the SO task and stronger in ACC during the AO task. These results are consistent with a division of labor between the two areas in terms of using rewards associated with either stimuli or actions to guide decision-making.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2013 the authors 0270-6474/13/331864-08\$15.00/0},
  langid = {english},
  pmid = {23365226},
  file = {/Users/daniekru/Zotero/storage/UX5ID2F5/Luk and Wallis - 2013 - Choice Coding in Frontal Cortex during Stimulus-Guided or Action-Guided Decision-Making.pdf}
}

@article{madadiaslDopaminergicModulationSynaptic2019,
  title = {Dopaminergic {{Modulation}} of {{Synaptic Plasticity}}, {{Its Role}} in {{Neuropsychiatric Disorders}}, and {{Its Computational Modeling}}},
  author = {Madadi Asl, Mojtaba and Vahabie, Abdol-Hossein and Valizadeh, Alireza},
  year = {2019},
  journal = {Basic and Clinical Neuroscience},
  volume = {10},
  number = {1},
  pages = {1--12},
  issn = {2008-126X},
  doi = {10.32598/bcn.9.10.125},
  urldate = {2024-04-26},
  abstract = {Neuromodulators modify intrinsic characteristics of the nervous system in order to reconfigure the functional properties of neural circuits. This reconfiguration is crucial for the flexibility of the nervous system to respond on an input-modulated basis. Such a functional rearrangement is realized by modification of intrinsic properties of the neural circuits including synaptic interactions. Dopamine is an important neuromodulator involved in motivation and stimulus-reward learning process, and adjusts synaptic dynamics in multiple time scales through different pathways. The modification of synaptic plasticity by dopamine underlies the change in synaptic transmission and integration mechanisms, which affects intrinsic properties of the neural system including membrane excitability, probability of neurotransmitters release, receptors' response to neurotransmitters, protein trafficking, and gene transcription. Dopamine also plays a central role in behavioral control, whereas its malfunction can cause cognitive disorders. Impaired dopamine signaling is implicated in several neuropsychiatric disorders such as Parkinson's disease, drug addiction, schizophrenia, attention-deficit/hyperactivity disorder, obsessive-compulsive disorder and Tourette's syndrome. Therefore, dopamine plays a crucial role in the nervous system, where its proper modulation of neural circuits may enhance plasticity-related procedures, but disturbances in dopamine signaling might be involved in numerous neuropsychiatric disorders. In recent years, several computational models are proposed to formulate the involvement of dopamine in synaptic plasticity or neuropsychiatric disorders and address their connection based on the experimental findings.},
  pmcid = {PMC6484184},
  pmid = {31031889},
  file = {/Users/daniekru/Zotero/storage/S4YBA7FY/Madadi Asl et al. - 2019 - Dopaminergic Modulation of Synaptic Plasticity, It.pdf}
}

@article{maesLearningSpatiotemporalSignals2020,
  title = {Learning Spatiotemporal Signals Using a Recurrent Spiking Network That Discretizes Time},
  author = {Maes, Amadeus and Barahona, Mauricio and Clopath, Claudia},
  year = {2020},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {1},
  pages = {e1007606},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007606},
  urldate = {2020-10-30},
  abstract = {Learning to produce spatiotemporal sequences is a common task that the brain has to solve. The same neurons may be used to produce different sequential behaviours. The way the brain learns and encodes such tasks remains unknown as current computational models do not typically use realistic biologically-plausible learning. Here, we propose a model where a spiking recurrent network of excitatory and inhibitory spiking neurons drives a read-out layer: the dynamics of the driver recurrent network is trained to encode time which is then mapped through the read-out neurons to encode another dimension, such as space or a phase. Different spatiotemporal patterns can be learned and encoded through the synaptic weights to the read-out neurons that follow common Hebbian learning rules. We demonstrate that the model is able to learn spatiotemporal dynamics on time scales that are behaviourally relevant and we show that the learned sequences are robustly replayed during a regime of spontaneous activity.},
  langid = {english},
  keywords = {Action potentials,Learning,Membrane potential,Neural networks,Neuronal plasticity,Neurons,Supervisors,Synapses},
  file = {/Users/daniekru/Zotero/storage/DRYTUNCJ/Maes et al. - 2020 - Learning spatiotemporal signals using a recurrent .pdf;/Users/daniekru/Zotero/storage/5KQZCBY4/article.html;/Users/daniekru/Zotero/storage/8CKEHIY4/article.html}
}

@article{malakasisSynapticTurnoverPromotes2023,
  title = {Synaptic Turnover Promotes Efficient Learning in Bio-Realistic Spiking Neural Networks},
  author = {Malakasis, Nikos and Chavlis, Spyridon and Poirazi, Panayiota},
  year = {2023},
  month = may,
  journal = {bioRxiv},
  pages = {2023.05.22.541722},
  doi = {10.1101/2023.05.22.541722},
  urldate = {2024-12-10},
  abstract = {While artificial machine learning systems achieve superhuman performance in specific tasks such as language processing, image and video recognition, they do so use extremely large datasets and huge amounts of power. On the other hand, the brain remains superior in several cognitively challenging tasks while operating with the energy of a small lightbulb. We use a biologically constrained spiking neural network model to explore how the neural tissue achieves such high efficiency and assess its learning capacity on discrimination tasks. We found that synaptic turnover, a form of structural plasticity, which is the ability of the brain to form and eliminate synapses continuously, increases both the speed and the performance of our network on all tasks tested. Moreover, it allows accurate learning using a smaller number of examples. Importantly, these improvements are most significant under conditions of resource scarcity, such as when the number of trainable parameters is halved and when the task difficulty is increased. Our findings provide new insights into the mechanisms that underlie efficient learning in the brain and can inspire the development of more efficient and flexible machine learning algorithms.},
  pmcid = {PMC10245885},
  pmid = {37292929},
  file = {/Users/daniekru/Zotero/storage/NH5LIBTA/Malakasis et al. - 2023 - Synaptic turnover promotes efficient learning in bio-realistic spiking neural networks.pdf}
}

@incollection{maniAntLionOptimizer2018,
  title = {Ant {{Lion Optimizer}} ({{ALO}}) {{Algorithm}}},
  booktitle = {Advanced {{Optimization}} by {{Nature-Inspired Algorithms}}},
  author = {Mani, Melika and {Bozorg-Haddad}, Omid and Chu, Xuefeng},
  editor = {{Bozorg-Haddad}, Omid},
  year = {2018},
  series = {Studies in {{Computational Intelligence}}},
  pages = {105--116},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-10-5221-7_11},
  urldate = {2023-02-16},
  abstract = {This chapter introduces the ant lion optimizer(ALO), which mimics the hunting behavior of antlions in the larvae stage. Specifically, this chapter includes literature review, details of the ALO algorithm, and a pseudo-code for its implementation.},
  isbn = {978-981-10-5221-7},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/GCY3T9NT/Mani et al. - 2018 - Ant Lion Optimizer (ALO) Algorithm.pdf}
}

@article{mannsGradualChangesHippocampal2007,
  title = {Gradual {{Changes}} in {{Hippocampal Activity Support Remembering}} the {{Order}} of {{Events}}},
  author = {Manns, Joseph R. and Howard, Marc W. and Eichenbaum, Howard},
  year = {2007},
  month = nov,
  journal = {Neuron},
  volume = {56},
  number = {3},
  pages = {530--540},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2007.08.017},
  urldate = {2023-01-30},
  abstract = {The hippocampus is thought to contribute to episodic memory in part by binding stimuli to their spatiotemporal context. The present study examined how hippocampal neuronal populations encode spatial and temporal context as rats performed a task in which they were required to remember the order of trial-unique sequences of odors. The results suggest that a gradual change in the pattern of hippocampal activity served as a temporal context for odor-sampling events and was important for successful subsequent memory of the order of those odors.},
  langid = {english},
  keywords = {SYSNEURO},
  file = {/Users/daniekru/Zotero/storage/AFX2H5DH/Manns et al. - 2007 - Gradual Changes in Hippocampal Activity Support Re.pdf;/Users/daniekru/Zotero/storage/CXZCEDK9/S0896627307006435.html}
}

@article{manteContextdependentComputationRecurrent2013,
  title = {Context-Dependent Computation by Recurrent Dynamics in Prefrontal Cortex},
  author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V. and Newsome, William T.},
  year = {2013},
  month = nov,
  journal = {Nature},
  volume = {503},
  number = {7474},
  pages = {78--84},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature12742},
  urldate = {2023-07-13},
  abstract = {Prefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of the computations underlying this role remains largely unknown. In particular, individual prefrontal neurons often generate remarkably complex responses that defy deep understanding of their contribution to behaviour. Here we study prefrontal cortex activity in macaque monkeys trained to flexibly select and integrate noisy sensory inputs towards a choice. We find that the observed complexity and functional roles of single neurons are readily understood in the framework of a dynamical process unfolding at the level of the population. The population dynamics can be reproduced by a trained recurrent neural network, which suggests a previously unknown mechanism for selection and integration of task-relevant inputs. This mechanism indicates that selection and integration are two aspects of a single dynamical process unfolding within the same prefrontal circuits, and potentially provides a novel, general framework for understanding context-dependent computations.},
  copyright = {2013 Springer Nature Limited},
  langid = {english},
  keywords = {Cognitive neuroscience,to study},
  file = {/Users/daniekru/Zotero/storage/6SFQLCKP/Mante et al. - 2013 - Context-dependent computation by recurrent dynamic.pdf}
}

@article{marcosDeterminingMonkeyFree2016,
  title = {Determining {{Monkey Free Choice Long}} before the {{Choice Is Made}}: {{The Principal Role}} of {{Prefrontal Neurons Involved}} in {{Both Decision}} and {{Motor Processes}}},
  shorttitle = {Determining {{Monkey Free Choice Long}} before the {{Choice Is Made}}},
  author = {Marcos, Encarni and Genovesio, Aldo},
  year = {2016},
  month = sep,
  journal = {Frontiers in Neural Circuits},
  volume = {10},
  publisher = {Frontiers},
  issn = {1662-5110},
  doi = {10.3389/fncir.2016.00075},
  urldate = {2024-11-28},
  abstract = {{$<$}p{$>$}When choices are made freely, they might emerge from pre-existing neural activity. However, whether neurons in the prefrontal cortex (PF) show this anticipatory effect and, if so, in which part of the process they are involved is still debated. To answer this question, we studied PF activity in monkeys while they performed a strategy task. In this task when the stimulus changed from the previous trial, the monkeys had to shift their response to one of two spatial goals, excluding the one that had been previously selected. Under this free-choice condition, the prestimulus activity of the same neurons that are involved in decision and motor processes predicted future choices. These neurons developed the same goal preferences during the prestimulus presentation as they did later in the decision phase. In contrast, the same effect was not observed in motor-only neurons and it was present but weaker in decision-only neurons. Overall, our results suggest that the PF neuronal activity predicts upcoming actions mainly through the decision-making network that integrate in time decision and motor task aspects.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Bias,Decision Making,monkeys,Neurophysiology,prefrontal,strategy},
  file = {/Users/daniekru/Zotero/storage/R52M755Q/Marcos and Genovesio - 2016 - Determining Monkey Free Choice Long before the Choice Is Made The Principal Role of Prefrontal Neur.pdf}
}

@incollection{marderNeurotransmittersNeuromodulators1987,
  title = {Neurotransmitters and {{Neuromodulators}}},
  booktitle = {The {{Crustacean Stomatogastric System}}: {{A Model}} for the {{Study}} of {{Central Nervous Systems}}},
  author = {Marder, E.},
  editor = {Selverston, Allen I. and Moulins, Maurice},
  year = {1987},
  pages = {263--306},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-71516-7_10},
  urldate = {2022-10-27},
  abstract = {It has been long appreciated that a large number of different substances likely to be neurotransmitters and neuromodulators are present in all nervous systems, from the simplest to the most complex. One of the fundamental questions that has puzzled neurobiologists through the decades is why there are so many different substances used as signaling molecules.},
  isbn = {978-3-642-71516-7},
  langid = {english},
  keywords = {Pacemaker Potential,Pyloric Rhythm,Rhythmic Gastric Mill,Stomatogastric Ganglion,Stomatogastric Nervous System}
}

@article{martensLearningRecurrentNeural,
  title = {Learning {{Recurrent Neural Networks}} with {{Hessian-Free Optimization}}},
  author = {Martens, James and Sutskever, Ilya},
  abstract = {In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-theart method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/I2WHXYLG/Martens and Sutskever - Learning Recurrent Neural Networks with Hessian-Fr.pdf}
}

@article{martinRepresentationObjectConcepts2007a,
  title = {The {{Representation}} of {{Object Concepts}} in the {{Brain}}},
  author = {Martin, Alex},
  year = {2007},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {58},
  number = {1},
  pages = {25--45},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.57.102904.190143},
  urldate = {2024-12-19},
  abstract = {Evidence from functional neuroimaging of the human brain indicates that information about salient properties of an object---such as what it looks like, how it moves, and how it is used---is stored in sensory and motor systems active when that information was acquired. As a result, object concepts belonging to different categories like animals and tools are represented in partially distinct, sensory- and motor property--based neural networks. This suggests that object concepts are not explicitly represented, but rather emerge from weighted activity within property-based brain regions. However, some property-based regions seem to show a categorical organization, thus providing evidence consistent with category-based, domain-specific formulations as well.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/2BZ6XLAW/Martin - 2007 - The Representation of Object Concepts in the Brain.pdf}
}

@inproceedings{marzulloTensorbasedMutationOperator2017,
  title = {A Tensor-Based Mutation Operator for {{Neuroevolution}} of {{Augmenting Topologies}} ({{NEAT}})},
  booktitle = {2017 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Marzullo, Aldo and Stamile, Claudio and Terracina, Giorgio and Calimeri, Francesco and Van Huffel, Sabine},
  year = {2017},
  month = jun,
  pages = {681--687},
  doi = {10.1109/CEC.2017.7969376},
  abstract = {In Genetic Algorithms, the mutation operator is used to maintain genetic diversity in the population throughout the evolutionary process. Various kinds of mutation may occur over time, typically depending on a fixed probability value called mutation rate. In this work we make use of a novel data-science approach in order to adaptively generate mutation rates for each locus to the Neuroevolution of Augmenting Topologies (NEAT) algorithm. The trail of high quality candidate solutions obtained during the search process is represented as a third-order tensor; factorization of such a tensor reveals the latent relationship between solutions, determining the mutation probability which is likely to yield improvement at each locus. The single pole balancing problem is used as case study to analyze the effectiveness of the proposed approach. Results show that the tensor approach improves the performance of the standard NEAT algorithm for the case study.},
  keywords = {Biological cells,Biological neural networks,Genetic Algorithm,Genetic algorithms,Mutation,NEAT,Neurons,Sociology,Tensile stress,Tensor decomposition,Topology},
  file = {/Users/daniekru/Zotero/storage/AZ8CKN25/Marzullo et al. - 2017 - A tensor-based mutation operator for Neuroevolutio.pdf;/Users/daniekru/Zotero/storage/ZYPG5KT5/stamp.html}
}

@article{matsumotoClimateChangeImpacts2018,
  title = {Climate Change Impacts on Socioeconomic Activities through Labor Productivity Changes Considering Interactions between Socioeconomic and Climate Systems},
  author = {Matsumoto, Ken'ichi},
  year = {2018},
  month = dec,
  journal = {Journal of Cleaner Production},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2018.12.127},
  urldate = {2018-12-16},
  abstract = {While human socioeconomic activity leads to climate change, the latter also affects the former; socioeconomic and climate systems have considerable interactions. Some studies have looked at the effects of climate change on labor productivity and gross domestic product, yet they have not considered the interaction between socioeconomic and climate systems. This study therefore examined that aspect as well as the economic impact of climate-change-induced labor productivity change. Business-as-usual and two emissions reduction scenarios---2{$^\circ$}C and Representative Concentration Pathway 4.5---were adopted. Data analysis employed a computable general equilibrium model and a simple climate model. The results show that global economic impacts of climate-change-induced labor productivity change have not been large. A negative effect on economic activities was found when the relationship between climate change and labor productivity was considered in the economic model. Although such impacts were larger in the business-as-usual scenario, that was not the case in the 2\,{$^\circ$}C scenario. The results suggest that greater levels of climate change are in accordance with greater socioeconomic impact at the global level. In particular, impact on high-temperature regions was found to be considerable. Interestingly, not all regions experienced economic loss from climate change. Some in the low-to medium-temperature zones received a positive economic effect because of comparative advantage caused by differences in labor productivity changes among regions. The coupled modeling scheme ultimately was effective in evaluating the interaction. Expanded assessment of climate change, mitigation, and adaptation will aid further understanding of the interaction of climate change and socioeconomic activities.},
  keywords = {Climate change,Economic activities,Energy supply,Gross domestic product,Labor productivity,Model interactions},
  file = {/Users/daniekru/Zotero/storage/3JGS2TKR/Matsumoto - 2018 - Climate change impacts on socioeconomic activities.pdf;/Users/daniekru/Zotero/storage/J7EUVGIJ/S095965261833837X.html}
}

@misc{mavor-parkerSimpleApproachStateAction2023,
  title = {A {{Simple Approach}} for {{State-Action Abstraction}} Using a {{Learned MDP Homomorphism}}},
  author = {{Mavor-Parker}, Augustine N. and Sargent, Matthew J. and Banino, Andrea and Griffin, Lewis D. and Barry, Caswell},
  year = {2023},
  month = jul,
  number = {arXiv:2209.06356},
  eprint = {2209.06356},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-27},
  abstract = {Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by a factor equal to the cardinality of the action space. We call this method equivalent effect abstraction. In a gridworld setting, we demonstrate empirically that equivalent effect abstraction can improve sample efficiency in a model-free setting and planning efficiency for modelbased approaches. Furthermore, we show on cartpole that our approach outperforms an existing method for learning homomorphisms, while using 33x less training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/YG335I4I/Mavor-Parker et al. - 2023 - A Simple Approach for State-Action Abstraction usi.pdf;/Users/daniekru/Zotero/storage/L2U9UVG8/2209.html}
}

@article{mcclellandIntegrationNewInformation2020,
  title = {Integration of New Information in Memory: New Insights from a Complementary Learning Systems Perspective},
  shorttitle = {Integration of New Information in Memory},
  author = {McClelland, James L. and McNaughton, Bruce L. and Lampinen, Andrew K.},
  year = {2020},
  month = may,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1799},
  pages = {20190637},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2019.0637},
  urldate = {2020-05-27},
  abstract = {According to complementary learning systems theory, integrating new memories into the neocortex of the brain without interfering with what is already known depends on a gradual learning process, interleaving new items with previously learned items. However, empirical studies show that information consistent with prior knowledge can sometimes be integrated very quickly. We use artificial neural networks with properties like those we attribute to the neocortex to develop an understanding of the role of consistency with prior knowledge in putatively neocortex-like learning systems, providing new insights into when integration will be fast or slow and how integration might be made more efficient when the items to be learned are hierarchically structured. The work relies on deep linear networks that capture the qualitative aspects of the learning dynamics of the more complex nonlinear networks used in previous work. The time course of learning in these networks can be linked to the hierarchical structure in the training data, captured mathematically as a set of dimensions that correspond to the branches in the hierarchy. In this context, a new item to be learned can be characterized as having aspects that project onto previously known dimensions, and others that require adding a new branch/dimension. The projection onto the known dimensions can be learned rapidly without interleaving, but learning the new dimension requires gradual interleaved learning. When a new item only overlaps with items within one branch of a hierarchy, interleaving can focus on the previously known items within this branch, resulting in faster integration with less interleaving overall. The discussion considers how the brain might exploit these facts to make learning more efficient and highlights predictions about what aspects of new information might be hard or easy to learn.This article is part of the Theo Murphy meeting issue `Memory reactivation: replaying events past, present and future'.},
  file = {/Users/daniekru/Zotero/storage/UXFLQZFS/McClelland et al. - 2020 - Integration of new information in memory new insi.pdf;/Users/daniekru/Zotero/storage/TFSQ7D9F/rstb.2019.html}
}

@article{mccormickNeuromodulationBrainState2020,
  title = {Neuromodulation of {{Brain State}} and {{Behavior}}},
  author = {McCormick, David A. and Nestvogel, Dennis B. and He, Biyu J.},
  year = {2020},
  month = jul,
  journal = {Annual Review of Neuroscience},
  volume = {43},
  number = {1},
  doi = {10.1146/annurev-neuro-100219-105424},
  urldate = {2022-10-24},
  abstract = {Neural activity and behavior are both notoriously variable, with responses differing widely between repeated presentation of identical stimuli or trials. Recent results in humans and animals reveal that these variations are not random in their nature, but may in fact be due in large part to rapid shifts in neural, cognitive, and behavioral states. Here we review recent advances in the understanding of rapid variations in the waking state, how variations are generated, and how they modulate neural and behavioral responses in both mice and humans. We propose that the brain has an identifiable set of states through which it wanders continuously in a nonrandom fashion, owing to the activity of both ascending modulatory and fast-acting corticocortical and subcortical-cortical neural pathways. These state variations provide the backdrop upon which the brain operates, and understanding them is critical to making progress in revealing the neural mechanisms underlying cognition and behavior.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/PVC6EEIR/McCormick et al. - 2020 - Neuromodulation of Brain State and Behavior.pdf;/Users/daniekru/Zotero/storage/RQK5GHJ6/10206122.html}
}

@article{mcnameeFlexibleModulationSequence2021,
  title = {Flexible Modulation of Sequence Generation in the Entorhinal--Hippocampal System},
  author = {McNamee, Daniel C. and Stachenfeld, Kimberly L. and Botvinick, Matthew M. and Gershman, Samuel J.},
  year = {2021},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {24},
  number = {6},
  pages = {851--862},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00831-7},
  urldate = {2025-02-22},
  abstract = {Exploration, consolidation and planning depend on the generation of sequential state representations. However, these algorithms require disparate forms of sampling dynamics for optimal performance. We theorize how the brain should adapt internally generated sequences for particular cognitive functions and propose a neural mechanism by which this may be accomplished within the entorhinal--hippocampal circuit. Specifically, we demonstrate that the systematic modulation along the medial entorhinal cortex dorsoventral axis of grid population input into the hippocampus facilitates a flexible generative process that can interpolate between qualitatively distinct regimes of sequential hippocampal reactivations. By relating the emergent hippocampal activity patterns drawn from our model to empirical data, we explain and reconcile a diversity of recently observed, but apparently unrelated, phenomena such as generative cycling, diffusive hippocampal reactivations and jumping trajectory events.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational neuroscience,Hippocampus},
  file = {/Users/daniekru/Zotero/storage/JUEJ7IPE/McNamee et al. - 2021 - Flexible modulation of sequence generation in the entorhinal–hippocampal system.pdf}
}

@article{mcnaughtonPathIntegrationNeural2006,
  title = {Path Integration and the Neural Basis of the 'Cognitive Map'},
  author = {McNaughton, Bruce L. and Battaglia, Francesco P. and Jensen, Ole and Moser, Edvard I. and Moser, May-Britt},
  year = {2006},
  month = aug,
  journal = {Nature Reviews Neuroscience},
  volume = {7},
  number = {8},
  pages = {663--678},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn1932},
  urldate = {2025-02-24},
  abstract = {Accumulating evidence indicates that the foundation of mammalian spatial orientation and learning is based on an internal network that can keep track of relative position and orientation (from an arbitrary starting point) on the basis of integration of self-motion cues derived from locomotion, vestibular activation and optic flow (path integration).Place cells in the hippocampal formation exhibit elevated activity at discrete spots in a given environment, and this spatial representation is determined primarily on the basis of which cells were active at the starting point and how far and in what direction the animal has moved since then. Environmental features become associatively bound to this intrinsic spatial framework and can serve to correct for cumulative error in the path integration process.Theoretical studies suggested that a path integration system could involve cooperative interactions (attractor dynamics) among a population of place coding neurons, the synaptic coupling of which defines a two-dimensional attractor map. These cells would communicate with an additional group of neurons, the activity of which depends on the conjunction of movement speed, location and orientation (head direction) information, allowing position on the attractor map to be updated by self-motion information.The attractor map hypothesis contains an inherent boundary problem: what happens when the animal's movements carry it beyond the boundary of the map? One solution to this problem is to make the boundaries of the map periodic by coupling neurons at each edge to those on the opposite edge, resulting in a toroidal synaptic matrix. This solution predicts that, in a sufficiently large space, place cells would exhibit a regularly spaced grid of place fields, something that has never been observed in the hippocampus proper.Recent discoveries in layer II of the medial entorhinal cortex (MEC), the main source of hippocampal afferents, indicate that these cells do have regularly spaced place fields (grid cells). In addition, cells in the deeper layers of this structure exhibit grid fields that are conjunctive for head orientation and movement speed. Pure head direction neurons are also found there. Therefore, all of the components of previous theoretical models for path integration appear in the MEC, suggesting that this network is the core of the path integration system.The scale of MEC spatial firing grids increases systematically from the dorsal to the ventral poles of this structure, in much the same way as is observed for hippocampal place cells, and we show how non-periodic hippocampal place fields could arise from the combination of inputs from entorhinal grid cells, if the inputs cover a range of spatial scales rather than a single scale. This phenomenon, in the spatial domain, is analogous to the low frequency 'beats' heard when two pure tones of slightly different frequencies are combined.The problem of how a two-dimensional synaptic matrix with periodic boundary conditions, postulated to underlie grid cell behaviour, could be self-organized in early development is addressed. Based on principles derived from Alan Turing's theory of spontaneous symmetry breaking in chemical systems, we suggest that topographically organized, grid-like patterns of neural activity might be present in the immature cortex, and that these activity patterns guide the development of the proposed periodic synaptic matrix through a mechanism involving competitive synaptic plasticity.},
  copyright = {2006 Springer Nature Limited},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/ZVZDPHRC/McNaughton et al. - 2006 - Path integration and the neural basis of the 'cognitive map'.pdf}
}

@article{meiEffectsNeuromodulationinspiredMechanisms2023,
  title = {Effects of Neuromodulation-Inspired Mechanisms on the Performance of Deep Neural Networks in a Spatial Learning Task},
  author = {Mei, Jie and Meshkinnejad, Rouzbeh and Mohsenzadeh, Yalda},
  year = {2023},
  month = feb,
  journal = {iScience},
  volume = {26},
  number = {2},
  pages = {106026},
  issn = {25890042},
  doi = {10.1016/j.isci.2023.106026},
  urldate = {2023-10-27},
  abstract = {In recent years, the biological underpinnings of adaptive learning have been modeled, leading to faster model convergence and various behavioral benefits in tasks including spatial navigation and cue-reward association. Furthermore, studies have investigated how the neuromodulatory system, a major driver of synaptic plasticity and state-dependent changes in the brain neuronal activities, plays a role in training deep neural networks (DNNs). In this study, we extended previous studies on neuromodulation-inspired DNNs and explored the effects of neuromodulatory components on learning and single unit activities in a spatial learning task. Under the multiscale neuromodulatory framework, plastic components, dropout probability modulation, and learning rate decay were added to the single unit, layer, and whole network levels of DNN models, respectively. We observed behavioral benefits including faster learning and smaller error of ambulation. We then concluded that neuromodulatory components can affect learning trajectories, outcomes, and single unit activities, in a component- and hyperparameter-dependent manner.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/IB5X4RN6/Mei et al. - 2023 - Effects of neuromodulation-inspired mechanisms on .pdf}
}

@article{meiInformingDeepNeural2022,
  title = {Informing Deep Neural Networks by Multiscale Principles of Neuromodulatory Systems},
  author = {Mei, Jie and Muller, Eilif and Ramaswamy, Srikanth},
  year = {2022},
  month = mar,
  journal = {Trends in Neurosciences},
  volume = {45},
  number = {3},
  pages = {237--250},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2021.12.008},
  urldate = {2022-10-24},
  abstract = {Our brains have evolved the ability to configure and adapt their processing states to match the unique challenges of acting and learning in diverse environments and behavioral contexts. In biological nervous systems, such state specification and adaptation arise in part from neuromodulators, including acetylcholine, noradrenaline, serotonin, and dopamine, whose diffuse release fine-tunes neuronal and synaptic dynamics and plasticity to complement the behavioral context in real-time. Despite the demonstrated effectiveness of deep neural networks for specific tasks, they remain relatively inflexible at generalizing across tasks or adapting to ever-changing behavioral demands. In this article, we provide an overview of neuromodulatory systems and their relationship to emerging pertinent principles in deep neural networks. We further outline opportunities for the integration of neuromodulatory principles into deep neural networks, towards endowing artificial intelligence with a key ingredient underlying the flexibility and learning capability of biological systems.},
  langid = {english},
  keywords = {acetylcholine,adaptive learning,dopamine,multiscale organization,noradrenaline,serotonin},
  file = {/Users/daniekru/Zotero/storage/23R3AJRE/Mei et al. - 2022 - Informing deep neural networks by multiscale princ.pdf;/Users/daniekru/Zotero/storage/IEKZFEGG/S0166223621002563.html}
}

@inproceedings{meilingerNetworkReferenceFrames2008,
  title = {The {{Network}} of {{Reference Frames Theory}}: {{A Synthesis}} of {{Graphs}} and {{Cognitive Maps}}},
  shorttitle = {The {{Network}} of {{Reference Frames Theory}}},
  booktitle = {Spatial {{Cognition VI}}. {{Learning}}, {{Reasoning}}, and {{Talking}} about {{Space}}},
  author = {Meilinger, Tobias},
  editor = {Freksa, Christian and Newcombe, Nora S. and G{\"a}rdenfors, Peter and W{\"o}lfl, Stefan},
  year = {2008},
  pages = {344--360},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-87601-4_25},
  abstract = {The network of reference frames theory explains the orientation behavior of human and non-human animals in directly experienced environmental spaces, such as buildings or towns. This includes self-localization, route and survey navigation. It is a synthesis of graph representations and cognitive maps, and solves the problems associated with explaining orientation behavior based either on graphs, maps or both of them in parallel. Additionally, the theory points out the unique role of vista spaces and asymmetries in spatial memory. New predictions are derived from the theory, one of which has been tested recently.},
  isbn = {978-3-540-87601-4},
  langid = {english},
  keywords = {cognitive map,environmental space,graph,reference frame,route knowledge,self-localization,spatial memory,survey knowledge},
  file = {/Users/daniekru/Zotero/storage/G6LJ58ZN/Meilinger - 2008 - The Network of Reference Frames Theory A Synthesis of Graphs and Cognitive Maps.pdf}
}

@misc{MemoryEngramStorage,
  title = {{Memory engram storage and retrieval.pdf {\textbar} Synaptic Plasticity {\textbar} Long Term Potentiation}},
  journal = {Scribd},
  urldate = {2020-04-15},
  abstract = {Scribd {\`e} il pi{\`u} grande sito di social reading e publishing al mondo.},
  howpublished = {https://www.scribd.com/document/328666527/Memory-engram-storage-and-retrieval-pdf},
  langid = {italian},
  file = {/Users/daniekru/Zotero/storage/HM89ZAVL/Memory-engram-storage-and-retrieval-pdf.html}
}

@article{meulemansTheoreticalFrameworkTarget2020,
  title = {A {{Theoretical Framework}} for {{Target Propagation}}},
  author = {Meulemans, Alexander and Carzaniga, Francesco S. and Suykens, Johan A. K. and Sacramento, Jo{\~a}o and Grewe, Benjamin F.},
  year = {2020},
  month = jul,
  journal = {arXiv:2006.14331 [cs, stat]},
  eprint = {2006.14331},
  primaryclass = {cs, stat},
  urldate = {2020-10-30},
  abstract = {The success of deep learning, a brain-inspired form of AI, has sparked interest in understanding how the brain could similarly learn across multiple layers of neurons. However, the majority of biologically-plausible learning algorithms have not yet reached the performance of backpropagation (BP), nor are they built on strong theoretical foundations. Here, we analyze target propagation (TP), a popular but not yet fully understood alternative to BP, from the standpoint of mathematical optimization. Our theory shows that TP is closely related to Gauss-Newton optimization and thus substantially differs from BP. Furthermore, our analysis reveals a fundamental limitation of difference target propagation (DTP), a well-known variant of TP, in the realistic scenario of non-invertible neural networks. We provide a first solution to this problem through a novel reconstruction loss that improves feedback weight training, while simultaneously introducing architectural flexibility by allowing for direct feedback connections from the output to each hidden layer. Our theory is corroborated by experimental results that show significant improvements in performance and in the alignment of forward weight updates with loss gradients, compared to DTP.},
  archiveprefix = {arXiv},
  keywords = {68T07,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/GFVS2747/Meulemans et al. - 2020 - A Theoretical Framework for Target Propagation.pdf;/Users/daniekru/Zotero/storage/ET8WUUWL/2006.html}
}

@misc{miikkulainenEvolvingDeepNeural2017,
  title = {Evolving {{Deep Neural Networks}}},
  author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Dan and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak},
  year = {2017},
  month = mar,
  number = {arXiv:1703.00548},
  eprint = {1703.00548},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-01-23},
  abstract = {The success of deep learning depends on finding an architecture to fit the task. As deep learning has scaled up to more challenging tasks, the architectures have become difficult to design by hand. This paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,to study},
  file = {/Users/daniekru/Zotero/storage/6T4RTC3D/Miikkulainen et al. - 2017 - Evolving Deep Neural Networks.pdf;/Users/daniekru/Zotero/storage/8PFCG7VG/1703.html}
}

@article{millerCombinedMechanismsNeural2019,
  title = {Combined Mechanisms of Neural Firing Rate Homeostasis},
  author = {Miller, Paul and Cannon, Jonathan},
  year = {2019},
  journal = {Biological Cybernetics},
  volume = {113},
  number = {1},
  pages = {47--59},
  issn = {0340-1200},
  doi = {10.1007/s00422-018-0768-8},
  urldate = {2024-12-12},
  abstract = {Spikes in the membrane potential of neurons comprise the currency of information processing in the brain. The ability of neurons to convert any information present across their multiple inputs into a significant modification to the pattern of their emitted spikes depends on the rate at which they emit spikes. If the mean rate is near the neuron's maximum, or if the rate is near zero, then changes in the inputs have minimal impact on the neuron's firing rate. Therefore, a neuron needs to control its mean rate. Protocols that either dramatically increase or decrease a neuron's firing rate lead to multiple compensatory changes that return the neuron's mean rate toward its prior value. In this primer, first as a summary of our previous work (Cannon and Miller in J Neurophysiol 116(5):2004--2022, ; Cannon and Miller in J Math Neurosci 7(1):1, ), we describe the advantages and disadvantages of having more than one such control mechanism responding to the neuron's firing rate. We suggest how problems of two, coexisting, potentially competing mechanisms can be overcome. Key requirements are: (1) the control be of a distribution of values, which the controlled variable achieves over a fast timescale compared to the timescale of the control system; (2) at least one of the control mechanisms be nonlinear; and (3) the two control systems are satisfied by a stable distribution or range of values that can be achieved by the variable. We show examples of functional control systems, including the previously studied integral feedback controller and new simulations of a ``bang--bang'' controller, that allow for compensation when inputs to the system change. Finally, we present new results describing how the underlying signal processing pathways would produce mechanisms of dual control, as opposed to a single mechanism with two outputs, and compare the responses of these systems to changes of input statistics.},
  pmcid = {PMC6510813},
  pmid = {29955960},
  file = {/Users/daniekru/Zotero/storage/K5Z9JTU5/Miller and Cannon - 2019 - Combined mechanisms of neural firing rate homeostasis.pdf}
}

@article{millerIntegrativeTheoryPrefrontal2001,
  title = {An {{Integrative Theory}} of {{Prefrontal Cortex Function}}},
  author = {Miller, Earl K. and Cohen, Jonathan D.},
  year = {2001},
  journal = {Annual Review of Neuroscience},
  volume = {24},
  number = {1},
  pages = {167--202},
  doi = {10.1146/annurev.neuro.24.1.167},
  urldate = {2021-12-02},
  abstract = {The prefrontal cortex has long been suspected to play an important role in cognitive control, in the ability to orchestrate thought and action in accordance with internal goals. Its neural basis, however, has remained a mystery. Here, we propose that cognitive control stems from the active maintenance of patterns of activity in the prefrontal cortex that represent goals and the means to achieve them. They provide bias signals to other brain structures whose net effect is to guide the flow of activity along neural pathways that establish the proper mappings between inputs, internal states, and outputs needed to perform a given task. We review neurophysiological, neurobiological, neuroimaging, and computational studies that support this theory and discuss its implications as well as further issues to be addressed},
  pmid = {11283309},
  keywords = {attention,cognition,executive control,frontal lobes,working memory},
  file = {/Users/daniekru/Zotero/storage/EXYF9PKM/Miller and Cohen - 2001 - An Integrative Theory of Prefrontal Cortex Functio.pdf}
}

@article{millerIntegrativeTheoryPrefrontal2001a,
  title = {An {{Integrative Theory}} of {{Prefrontal Cortex Function}}},
  author = {Miller, Earl K. and Cohen, Jonathan D.},
  year = {2001},
  month = mar,
  journal = {Annual Review of Neuroscience},
  volume = {24},
  number = {1},
  pages = {167--202},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.24.1.167},
  urldate = {2024-05-06},
  abstract = {▪ Abstract{\enspace} The prefrontal cortex has long been suspected to play an important role in cognitive control, in the ability to orchestrate thought and action in accordance with internal goals. Its neural basis, however, has remained a mystery. Here, we propose that cognitive control stems from the active maintenance of patterns of activity in the prefrontal cortex that represent goals and the means to achieve them. They provide bias signals to other brain structures whose net effect is to guide the flow of activity along neural pathways that establish the proper mappings between inputs, internal states, and outputs needed to perform a given task. We review neurophysiological, neurobiological, neuroimaging, and computational studies that support this theory and discuss its implications as well as further issues to be addressed},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/K5SC3U6J/Miller and Cohen - 2001 - An Integrative Theory of Prefrontal Cortex Functio.pdf}
}

@article{milsteinBidirectionalSynapticPlasticity2021,
  title = {Bidirectional Synaptic Plasticity Rapidly Modifies Hippocampal Representations},
  author = {Milstein, Aaron D and Li, Yiding and Bittner, Katie C and Grienberger, Christine and Soltesz, Ivan and Magee, Jeffrey C and Romani, Sandro},
  editor = {Toth, Katalin and Colgin, Laura L and Epsztein, Jerome and Naud, Richard},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e73046},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.73046},
  urldate = {2023-02-06},
  abstract = {Learning requires neural adaptations thought to be mediated by activity-dependent synaptic plasticity. A relatively non-standard form of synaptic plasticity driven by dendritic calcium spikes, or plateau potentials, has been reported to underlie place field formation in rodent hippocampal CA1 neurons. Here, we found that this behavioral timescale synaptic plasticity (BTSP) can also reshape existing place fields via bidirectional synaptic weight changes that depend on the temporal proximity of plateau potentials to pre-existing place fields. When evoked near an existing place field, plateau potentials induced less synaptic potentiation and more depression, suggesting BTSP might depend inversely on postsynaptic activation. However, manipulations of place cell membrane potential and computational modeling indicated that this anti-correlation actually results from a dependence on current synaptic weight such that weak inputs potentiate and strong inputs depress. A network model implementing this bidirectional synaptic learning rule suggested that BTSP enables population activity, rather than pairwise neuronal correlations, to drive neural adaptations to experience.},
  keywords = {computational model,dendrites,hippocampus,learning,place cell,plasticity},
  file = {/Users/daniekru/Zotero/storage/QY2FBY62/Milstein et al. - 2021 - Bidirectional synaptic plasticity rapidly modifies.pdf}
}

@book{miyakeModelsWorkingMemory1999,
  title = {Models of {{Working Memory}}: {{Mechanisms}} of {{Active Maintenance}} and {{Executive Control}}},
  shorttitle = {Models of {{Working Memory}}},
  author = {Miyake, Akira and Shah, Priti},
  year = {1999},
  month = apr,
  publisher = {Cambridge University Press},
  abstract = {Working memory is currently a "hot" topic in cognitive psychology and neuroscience. Because of their radically different scopes and emphases, however, comparing different models and theories and understanding how they relate to one another has been a difficult task. This work offers a much-needed forum for systematically comparing and contrasting existing models of working memory. It does so by asking each contributor to address the same comprehensive set of important theoretical questions on working memory. The answers to these questions elucidate the emerging general consensus on the nature of working memory among different theorists and clarify incompatible theoretical claims that must be resolved in future research. As such, this volume serves not only as a milestone that documents the state of the art in the field, but also as a theoretical guidebook that will promote new lines of research and more precise and comprehensive models of working memory.},
  googlebooks = {Dmu23\_pfbb8C},
  isbn = {978-0-521-58721-1},
  langid = {english},
  keywords = {Psychology / Cognitive Psychology & Cognition}
}

@article{moldwinGradientClusteronModel2021,
  title = {The Gradient Clusteron: {{A}} Model Neuron That Learns to Solve Classification Tasks via Dendritic Nonlinearities, Structural Plasticity, and Gradient Descent},
  shorttitle = {The Gradient Clusteron},
  author = {Moldwin, Toviah and Kalmenson, Menachem and Segev, Idan},
  year = {2021},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {5},
  pages = {e1009015},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009015},
  urldate = {2023-01-11},
  abstract = {Synaptic clustering on neuronal dendrites has been hypothesized to play an important role in implementing pattern recognition. Neighboring synapses on a dendritic branch can interact in a synergistic, cooperative manner via nonlinear voltage-dependent mechanisms, such as NMDA receptors. Inspired by the NMDA receptor, the single-branch clusteron learning algorithm takes advantage of location-dependent multiplicative nonlinearities to solve classification tasks by randomly shuffling the locations of ``under-performing'' synapses on a model dendrite during learning (``structural plasticity''), eventually resulting in synapses with correlated activity being placed next to each other on the dendrite. We propose an alternative model, the gradient clusteron, or G-clusteron, which uses an analytically-derived gradient descent rule where synapses are "attracted to" or "repelled from" each other in an input- and location-dependent manner. We demonstrate the classification ability of this algorithm by testing it on the MNIST handwritten digit dataset and show that, when using a softmax activation function, the accuracy of the G-clusteron on the all-versus-all MNIST task ({\textasciitilde}85\%) approaches that of logistic regression ({\textasciitilde}93\%). In addition to the location update rule, we also derive a learning rule for the synaptic weights of the G-clusteron (``functional plasticity'') and show that a G-clusteron that utilizes the weight update rule can achieve {\textasciitilde}89\% accuracy on the MNIST task. We also show that a G-clusteron with both the weight and location update rules can learn to solve the XOR problem from arbitrary initial conditions.},
  langid = {english},
  keywords = {Artificial neural networks,Dendritic structure,Learning,Neuronal dendrites,Neuronal plasticity,Neurons,Synapses,Synaptic plasticity},
  file = {/Users/daniekru/Zotero/storage/EX83IN2I/Moldwin et al. - 2021 - The gradient clusteron A model neuron that learns.pdf}
}

@article{moldwinPerceptronLearningClassification2020,
  title = {Perceptron {{Learning}} and {{Classification}} in a {{Modeled Cortical Pyramidal Cell}}},
  author = {Moldwin, Toviah and Segev, Idan},
  year = {2020},
  journal = {Frontiers in Computational Neuroscience},
  volume = {14},
  issn = {1662-5188},
  urldate = {2023-01-11},
  abstract = {The perceptron learning algorithm and its multiple-layer extension, the backpropagation algorithm, are the foundations of the present-day machine learning revolution. However, these algorithms utilize a highly simplified mathematical abstraction of a neuron; it is not clear to what extent real biophysical neurons with morphologically-extended non-linear dendritic trees and conductance-based synapses can realize perceptron-like learning. Here we implemented the perceptron learning algorithm in a realistic biophysical model of a layer 5 cortical pyramidal cell with a full complement of non-linear dendritic channels. We tested this biophysical perceptron (BP) on a classification task, where it needed to correctly binarily classify 100, 1,000, or 2,000 patterns, and a generalization task, where it was required to discriminate between two ``noisy'' patterns. We show that the BP performs these tasks with an accuracy comparable to that of the original perceptron, though the classification capacity of the apical tuft is somewhat limited. We concluded that cortical pyramidal neurons can act as powerful classification devices.},
  file = {/Users/daniekru/Zotero/storage/J93WBEWR/Moldwin and Segev - 2020 - Perceptron Learning and Classification in a Modele.pdf}
}

@article{montagueFrameworkMesencephalicDopamine1996,
  title = {A Framework for Mesencephalic Dopamine Systems Based on Predictive {{Hebbian}} Learning},
  author = {Montague, P. R. and Dayan, P. and Sejnowski, T. J.},
  year = {1996},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {16},
  number = {5},
  pages = {1936--1947},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.16-05-01936.1996},
  urldate = {2024-04-26},
  abstract = {We develop a theoretical framework that shows how mesencephalic dopamine systems could distribute to their targets a signal that represents information about future expectations. In particular, we show how activity in the cerebral cortex can make predictions about future receipt of reward and how fluctuations in the activity levels of neurons in diffuse dopamine systems above and below baseline levels would represent errors in these predictions that are delivered to cortical and subcortical targets. We present a model for how such errors could be constructed in a real brain that is consistent with physiological results for a subset of dopaminergic neurons located in the ventral tegmental area and surrounding dopaminergic neurons. The theory also makes testable predictions about human choice behavior on a simple decision-making task. Furthermore, we show that, through a simple influence on synaptic plasticity, fluctuations in dopamine release can act to change the predictions in an appropriate manner.},
  chapter = {Articles},
  copyright = {{\copyright} 1996 by Society for Neuroscience},
  langid = {english},
  pmid = {8774460},
  file = {/Users/daniekru/Zotero/storage/P9HGS9Z3/Montague et al. - 1996 - A framework for mesencephalic dopamine systems bas.pdf}
}

@article{moralesHighThroughputTaskStudy2020,
  title = {High-{{Throughput Task}} to {{Study Memory Recall During Spatial Navigation}} in {{Rodents}}},
  author = {Morales, Lucia and Tom{\`a}s, David P. and Dalmau, Josep and {de la Rocha}, Jaime and Jercog, Pablo E.},
  year = {2020},
  month = may,
  journal = {Frontiers in Behavioral Neuroscience},
  volume = {14},
  publisher = {Frontiers},
  issn = {1662-5153},
  doi = {10.3389/fnbeh.2020.00064},
  urldate = {2025-02-16},
  abstract = {{$<$}p{$>$}Spatial navigation is one of the most frequently used behavioral paradigms to study memory formation in rodents. Commonly used tasks to study memory are labor-intensive, preventing the simultaneous testing of multiple animals with the tendency to yield a low number of trials, curtailing the statistical power. Moreover, they are not tailored to be combined with neurophysiology recordings because they are not based on overt stereotyped behavioral responses that can be precisely timed. Here we present a novel task to study long-term memory formation and recall during spatial navigation. The task consists of learning sessions during which mice need to find the rewarding port that changes from day to day. Hours after learning, there is a recall session during which mice search for the location of the memorized rewarding port. During the recall sessions, the animals repeatedly poke the remembered port over many trials (up to {$\sim$}20) without receiving a reward (i.e., no positive feedback) as a readout of memory. In this task, mice show memory of port locations learned on up to three previous days. This eight-port maze task requires minimal human intervention, allowing for simultaneous and unsupervised testing of several mice in parallel, yielding a high number of recall trials per session over many days, and compatible with recordings of neural activity.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Correlation between neuronal activity and behavior,Data output for machine-learning algorithms analysis tools,Freely-moving calcium imaging recordings,High-throughput experiemtation,Single-session memory test,spatial navigation and memory},
  file = {/Users/daniekru/Zotero/storage/FNHNFCWM/Morales et al. - 2020 - High-Throughput Task to Study Memory Recall During Spatial Navigation in Rodents.pdf}
}

@article{morenoSynchronizationKuramotoOscillators2004,
  title = {Synchronization of {{Kuramoto}} Oscillators in Scale-Free Networks},
  author = {Moreno, Y. and Pacheco, A. F.},
  year = {2004},
  month = oct,
  journal = {EPL (Europhysics Letters)},
  volume = {68},
  number = {4},
  pages = {603},
  publisher = {IOP Publishing},
  issn = {0295-5075},
  doi = {10.1209/epl/i2004-10238-x},
  urldate = {2020-05-27},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/PH75X3I9/Moreno and Pacheco - 2004 - Synchronization of Kuramoto oscillators in scale-f.pdf;/Users/daniekru/Zotero/storage/4U52RRLD/i2004-10238-x.html}
}

@misc{murakamiDistinctSourcesDeterministic2016,
  title = {Distinct Sources of Deterministic and Stochastic Components of Action Timing Decisions in Rodent Frontal Cortex},
  author = {Murakami, Masayoshi and Shteingart, Hanan and Loewenstein, Yonatan and Mainen, Zachary F.},
  year = {2016},
  month = nov,
  doi = {10.1101/088963},
  urldate = {2024-05-07},
  abstract = {The selection and timing of actions are subject to determinate influences such as sensory cues and internal state as well as to effectively stochastic variability. Although stochastic choice mechanisms are assumed by many theoretical models, their origin and mechanisms remain poorly understood. Here we investigated this issue by studying how neural circuits in the frontal cortex determine action timing in rats performing a waiting task. Electrophysiological recordings from two regions necessary for this behavior, medial prefrontal cortex (mPFC) and secondary motor cortex (M2), revealed an unexpected functional dissociation. Both areas encoded deterministic biases in action timing, but only M2 neurons reflected stochastic trial-bytrial fluctuations. This differential coding was reflected in distinct timescales of neural dynamics in the two frontal cortical areas. These results suggest a two-stage model in which stochastic components of action timing decisions are injected by circuits downstream of those carrying deterministic bias signals.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/INEJV6FR/Murakami et al. - 2016 - Distinct sources of deterministic and stochastic c.pdf}
}

@inproceedings{nagarajanRecognisingEnglishLanguage2022,
  title = {Recognising the {{English Language}} Using {{Context Free Grammar}} with {{PyFormlang}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Electronics}}, {{Computing}} and {{Communication Technologies}} ({{CONECCT}})},
  author = {Nagarajan, Harshitha and Vancha, Punitha and M, Supriya},
  year = {2022},
  month = jul,
  pages = {1--6},
  publisher = {IEEE},
  address = {Bangalore, India},
  doi = {10.1109/CONECCT55679.2022.9865855},
  urldate = {2024-05-08},
  abstract = {Natural language recognition is a sub-field of Natural Language Processing (NLP), a popular research playground that lies in the intersection of multiple areas of linguistics, computer science, artificial intelligence and machine learning. The purpose of NLP is to define a computer that can "understand" the contents of documents, including the language's contextual nuances. The first step, however, would be to pre-process and parse a particular statement to check if it is a legitimate sentence of the language or not. This is where Automata theory comes into the picture. Using the Python PyFormlang and nltk libraries, we develop an English language recognizer based on Context free grammar (CFG) representations of the English Language, with parts-of-speech (POS) tags making up the constituencies of the CFG. Syntactically accurate sentences are accepted if parsed without errors, else they are deemed invalid. We also layout the set of productions for the English language, which has proven to work well with most sentences including simple and complex, with an accuracy of 84.90\%.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-9781-7},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/EATKSGUM/Nagarajan et al. - 2022 - Recognising the English Language using Context Fre.pdf}
}

@inproceedings{najarroSelfAssemblingArtificialNeural2023,
  title = {Towards {{Self-Assembling Artificial Neural Networks}} through {{Neural Developmental Programs}}},
  booktitle = {{{ALIFE}} 2023: {{Ghost}} in the {{Machine}}: {{Proceedings}} of the 2023 {{Artificial Life Conference}}},
  author = {Najarro, Elias and Sudhakaran, Shyam and Risi, Sebastian},
  year = {2023},
  month = jul,
  publisher = {MIT Press},
  doi = {10.1162/isal_a_00697},
  urldate = {2024-01-12},
  abstract = {Abstract. Biological nervous systems are created in a fundamentally different way than current artificial neural networks. Despite its impressive results in a variety of different domains, deep learning often requires considerable engineering effort to design high-performing neural architectures. By contrast, biological nervous systems are grown through a dynamic self-organizing process. In this paper, we take initial steps toward neural networks that grow through a developmental process that mirrors key properties of embryonic development in biological organisms. The growth process is guided by another neural network, which we call a Neural Developmental Program (NDP) and which operates through local communication alone. We investigate the role of neural growth on different machine learning benchmarks and different optimization methods (evolutionary training, online RL, offline RL, and supervised learning). Additionally, we highlight future research directions and opportunities enabled by having self-organization driving the growth of neural networks.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/7M7DTFPM/Najarro et al. - 2023 - Towards Self-Assembling Artificial Neural Networks.pdf}
}

@article{nakashimaHippocampalFormationinspiredGlobal2024,
  title = {Hippocampal Formation-Inspired Global Self-Localization: Quick Recovery from the Kidnapped Robot Problem from an Egocentric Perspective},
  shorttitle = {Hippocampal Formation-Inspired Global Self-Localization},
  author = {Nakashima, Takeshi and Otake, Shunsuke and Taniguchi, Akira and Maeyama, Katsuyoshi and El Hafi, Lotfi and Taniguchi, Tadahiro and Yamakawa, Hiroshi},
  year = {2024},
  month = jul,
  journal = {Frontiers in Computational Neuroscience},
  volume = {18},
  publisher = {Frontiers},
  issn = {1662-5188},
  doi = {10.3389/fncom.2024.1398851},
  urldate = {2024-12-02},
  abstract = {{$<$}p{$>$}It remains difficult for mobile robots to continue accurate self-localization when they are suddenly teleported to a location that is different from their beliefs during navigation. Incorporating insights from neuroscience into developing a spatial cognition model for mobile robots may make it possible to acquire the ability to respond appropriately to changing situations, similar to living organisms. Recent neuroscience research has shown that during teleportation in rat navigation, neural populations of place cells in the cornu ammonis-3 region of the hippocampus, which are sparse representations of each other, switch discretely. In this study, we construct a spatial cognition model using brain reference architecture-driven development, a method for developing brain-inspired software that is functionally and structurally consistent with the brain. The spatial cognition model was realized by integrating the recurrent state---space model, a world model, with Monte Carlo localization to infer allocentric self-positions within the framework of neuro-symbol emergence in the robotics toolkit. The spatial cognition model, which models the cornu ammonis-1 and -3 regions with each latent variable, demonstrated improved self-localization performance of mobile robots during teleportation in a simulation environment. Moreover, it was confirmed that sparse neural activity could be obtained for the latent variables corresponding to cornu ammonis-3. These results suggest that spatial cognition models incorporating neuroscience insights can contribute to improving the self-localization technology for mobile robots. The project website is {$<$}ext-link ext-link-type="uri" xlink:href="https://nakashimatakeshi.github.io/HF-IGL/" xmlns:xlink="http://www.w3.org/1999/xlink"{$>$}https://nakashimatakeshi.github.io/HF-IGL/{$<$}/ext-link{$>$}.{$<$}/p{$>$}},
  langid = {english},
  keywords = {allocentric,brain-inspired AI,Egocentric,Kidnapped robot problem,Monte Carlo localization,Probabilistic Generative Model,world model},
  file = {/Users/daniekru/Zotero/storage/PH5N2ZE2/Nakashima et al. - 2024 - Hippocampal formation-inspired global self-localization quick recovery from the kidnapped robot pro.pdf}
}

@article{neherGridCellsPlace2017,
  title = {From Grid Cells to Place Cells with Realistic Field Sizes},
  author = {Neher, Torsten and Azizi, Amir Hossein and Cheng, Sen},
  year = {2017},
  month = jul,
  journal = {PLOS ONE},
  volume = {12},
  number = {7},
  pages = {e0181618},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0181618},
  urldate = {2023-11-03},
  abstract = {While grid cells in the medial entorhinal cortex (MEC) of rodents have multiple, regularly arranged firing fields, place cells in the cornu ammonis (CA) regions of the hippocampus mostly have single spatial firing fields. Since there are extensive projections from MEC to the CA regions, many models have suggested that a feedforward network can transform grid cell firing into robust place cell firing. However, these models generate place fields that are consistently too small compared to those recorded in experiments. Here, we argue that it is implausible that grid cell activity alone can be transformed into place cells with robust place fields of realistic size in a feedforward network. We propose two solutions to this problem. Firstly, weakly spatially modulated cells, which are abundant throughout EC, provide input to downstream place cells along with grid cells. This simple model reproduces many place cell characteristics as well as results from lesion studies. Secondly, the recurrent connections between place cells in the CA3 network generate robust and realistic place fields. Both mechanisms could work in parallel in the hippocampal formation and this redundancy might account for the robustness of place cell responses to a range of disruptions of the hippocampal circuitry.},
  langid = {english},
  keywords = {Autocorrelation,Hippocampal formation,Hippocampus,Neurons,Sensory cues,Sensory perception,Spatial autocorrelation,Synaptic plasticity,to study},
  file = {/Users/daniekru/Zotero/storage/IXH789VU/Neher et al. - 2017 - From grid cells to place cells with realistic fiel.pdf}
}

@article{neherGridCellsPlace2017a,
  title = {From Grid Cells to Place Cells with Realistic Field Sizes},
  author = {Neher, Torsten and Azizi, Amir Hossein and Cheng, Sen},
  year = {2017},
  month = jul,
  journal = {PLOS ONE},
  volume = {12},
  number = {7},
  pages = {e0181618},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0181618},
  urldate = {2023-11-09},
  abstract = {While grid cells in the medial entorhinal cortex (MEC) of rodents have multiple, regularly arranged firing fields, place cells in the cornu ammonis (CA) regions of the hippocampus mostly have single spatial firing fields. Since there are extensive projections from MEC to the CA regions, many models have suggested that a feedforward network can transform grid cell firing into robust place cell firing. However, these models generate place fields that are consistently too small compared to those recorded in experiments. Here, we argue that it is implausible that grid cell activity alone can be transformed into place cells with robust place fields of realistic size in a feedforward network. We propose two solutions to this problem. Firstly, weakly spatially modulated cells, which are abundant throughout EC, provide input to downstream place cells along with grid cells. This simple model reproduces many place cell characteristics as well as results from lesion studies. Secondly, the recurrent connections between place cells in the CA3 network generate robust and realistic place fields. Both mechanisms could work in parallel in the hippocampal formation and this redundancy might account for the robustness of place cell responses to a range of disruptions of the hippocampal circuitry.},
  langid = {english},
  keywords = {Autocorrelation,Hippocampal formation,Hippocampus,Neurons,Sensory cues,Sensory perception,Spatial autocorrelation,Synaptic plasticity,to study},
  file = {/Users/daniekru/Zotero/storage/DJK4NUPQ/Neher et al. - 2017 - From grid cells to place cells with realistic fiel.pdf}
}

@article{nivEvolutionReinforcementLearning2002,
  title = {Evolution of {{Reinforcement Learning}} in {{Uncertain Environments}}: {{A Simple Explanation}} for {{Complex Foraging Behaviors}}},
  author = {Niv, Yael and Joel, Daphna and Meilijson, Isaac and Ruppin, Eytan},
  year = {2002},
  journal = {International Society for Adaptive Behavior},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/5ZAAEAZT/Niv et al. - Evolution of Reinforcement Learning in Uncertain E.pdf}
}

@article{nunesSpikingNeuralNetworks2022,
  title = {Spiking {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Spiking {{Neural Networks}}},
  author = {Nunes, Jo{\~a}o D. and Carvalho, Marcelo and Carneiro, Diogo and Cardoso, Jaime S.},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {60738--60764},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3179968},
  urldate = {2024-12-10},
  abstract = {The field of Deep Learning (DL) has seen a remarkable series of developments with increasingly accurate and robust algorithms. However, the increase in performance has been accompanied by an increase in the parameters, complexity, and training and inference time of the models, which means that we are rapidly reaching a point where DL may no longer be feasible. On the other hand, some specific applications need to be carefully considered when developing DL models due to hardware limitations or power requirements. In this context, there is a growing interest in efficient DL algorithms, with Spiking Neural Networks (SNNs) being one of the most promising paradigms. Due to the inherent asynchrony and sparseness of spike trains, these types of networks have the potential to reduce power consumption while maintaining relatively good performance. This is attractive for efficient DL and, if successful, could replace traditional Artificial Neural Networks (ANNs) in many applications. However, despite significant progress, the performance of SNNs on benchmark datasets is often lower than that of traditional ANNs. Moreover, due to the non-differentiable nature of their activation functions, it is difficult to train SNNs with direct backpropagation, so appropriate training strategies must be found. Nevertheless, significant efforts have been made to develop competitive models. This survey covers the main ideas behind SNNs and reviews recent trends in learning rules and network architectures, with a particular focus on biologically inspired strategies. It also provides some practical considerations of state-of-the-art SNNs and discusses relevant research opportunities.},
  keywords = {Artificial neural networks,Biological neural networks,Biological system modeling,Biology,Computational modeling,computer vision,efficient deep learning,event-driven,machine learning,Micromechanical devices,neuromorphic computing,neuromorphic hardware,Neurons,spiking neural networks,Training},
  file = {/Users/daniekru/Zotero/storage/YIN799P8/Nunes et al. - 2022 - Spiking Neural Networks A Survey.pdf;/Users/daniekru/Zotero/storage/8L7L48WK/9787485.html}
}

@misc{ockerFlexibleNeuralConnectivity2020,
  title = {Flexible Neural Connectivity under Constraints on Total Connection Strength},
  author = {Ocker, Gabriel Koch and Buice, Michael A.},
  year = {2020},
  month = jan,
  primaryclass = {New Results},
  pages = {603027},
  publisher = {bioRxiv},
  doi = {10.1101/603027},
  urldate = {2024-12-12},
  abstract = {Neural computation is determined by neuron dynamics and circuit connectivity. Uncertain and dynamic environments may require neural hardware to adapt to different computational tasks, each requiring different connectivity configurations. At the same time, connectivity is subject to a variety of constraints, placing limits on the possible computations a given neural circuit can perform. Here we examine the hypothesis that the organization of neural circuitry favors computational flexibility: that it makes many computational solutions available, given physiological constraints. From this hypothesis, we develop models of the degree distributions of connectivity based on constraints on a neuron's total synaptic weight. To test these models, we examine reconstructions of the mushroom bodies from the first instar larva and the adult Drosophila melanogaster. We perform a Bayesian model comparison for two constraint models and a random wiring null model. Overall, we find that flexibility under a homeostatically fixed total synaptic weight describes Kenyon cell connectivity better than other models, suggesting a principle shaping the apparently random structure of Kenyon cell wiring. Furthermore, we find evidence that larval Kenyon cells are more flexible earlier in development, suggesting a mechanism whereby neural circuits begin as flexible systems that develop into specialized computational circuits. Author summary High-throughput electron microscopic anatomical experiments have begun to yield detailed maps of neural circuit connectivity. Uncovering the principles that govern these circuit structures is a major challenge for systems neuroscience. Healthy neural circuits must be able to perform computational tasks while satisfying physiological constraints. Those constraints can restrict a neuron's possible connectivity, and thus potentially restrict its computation. Here we examine simple models of constraints on total synaptic weights, and calculate the number of circuit configurations they allow: their computational flexibility. We propose probabilistic models of connectivity that weight the number of synaptic partners according to computational flexibility under a constraint and test them using recent wiring diagrams from a learning center, the mushroom body, in the fly brain. We compare constraints that fix or bound a neuron's total connection strength to a simple random wiring null model. Of these models, the fixed total connection strength matched the overall connectivity best in mushroom bodies from both larval and adult flies. We also provide evidence suggesting that neural circuits are more flexible in early stages of development and lose this flexibility as they grow towards specialized function.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/E2GTUF6R/Ocker and Buice - 2020 - Flexible neural connectivity under constraints on total connection strength.pdf}
}

@article{odohertyAbstractRewardPunishment2001,
  title = {Abstract Reward and Punishment Representations in the Human Orbitofrontal Cortex},
  author = {O'Doherty, J. and Kringelbach, M. L. and Rolls, E. T. and Hornak, J. and Andrews, C.},
  year = {2001},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {4},
  number = {1},
  pages = {95--102},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/82959},
  urldate = {2024-05-06},
  abstract = {The orbitofrontal cortex (OFC) is implicated in emotion and emotion-related learning. Using event-related functional magnetic resonance imaging (fMRI), we measured brain activation in human subjects doing an emotion-related visual reversal-learning task in which choice of the correct stimulus led to a probabilistically determined 'monetary' reward and choice of the incorrect stimulus led to a monetary loss. Distinct areas of the OFC were activated by monetary rewards and punishments. Moreover, in these areas, we found a correlation between the magnitude of the brain activation and the magnitude of the rewards and punishments received. These findings indicate that one emotional involvement of the human orbitofrontal cortex is its representation of the magnitudes of abstract rewards and punishments, such as receiving or losing money.},
  copyright = {2001 Nature America Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/HYX78HJT/O'Doherty et al. - 2001 - Abstract reward and punishment representations in .pdf}
}

@article{ojaOjaLearningRule2008,
  title = {Oja Learning Rule},
  author = {Oja, Erkki},
  year = {2008},
  month = mar,
  journal = {Scholarpedia},
  volume = {3},
  number = {3},
  pages = {3612},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.3612},
  urldate = {2024-12-09},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/UYLQVD39/Oja_learning_rule.html}
}

@article{oreillyMakingWorkingMemory2006,
  title = {Making {{Working Memory Work}}: {{A Computational Model}} of {{Learning}} in the {{Prefrontal Cortex}} and {{Basal Ganglia}}},
  shorttitle = {Making {{Working Memory Work}}},
  author = {O'Reilly, Randall C. and Frank, Michael J.},
  year = {2006},
  month = feb,
  journal = {Neural Computation},
  volume = {18},
  number = {2},
  pages = {283--328},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976606775093909},
  urldate = {2022-10-03},
  abstract = {The prefrontal cortex has long been thought to subserve both working memory (the holding of information online for processing) and executive functions (deciding how to manipulate working memory and perform processing). Although many computational models of working memory have been developed, the mechanistic basis of executive function remains elusive, often amounting to a homunculus. This article presents an attempt to deconstruct this homunculus through powerful learning mechanisms that allow a computational model of the prefrontal cortex to control both itself and other brain areas in a strategic, task-appropriate manner. These learning mechanisms are based on subcortical structures in the midbrain, basal ganglia, and amygdala, which together form an actor-critic architecture. The critic system learns which prefrontal representations are task relevant and trains the actor, which in turn provides a dynamic gating mechanism for controlling working memory updating. Computationally, the learning mechanism is designed to simultaneously solve the temporal and structural credit assignment problems. The model's performance compares favorably with standard backpropagation-based temporal learning mechanisms on the challenging 1-2-AX working memory task and other benchmark working memory tasks.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/5HLWNR8U/O'Reilly and Frank - 2006 - Making Working Memory Work A Computational Model .pdf}
}

@article{ormondPlaceFieldExpansion2015,
  title = {Place Field Expansion after Focal {{MEC}} Inactivations Is Consistent with Loss of {{Fourier}} Components and Path Integrator Gain Reduction},
  author = {Ormond, Jake and McNaughton, Bruce L.},
  year = {2015},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {112},
  number = {13},
  pages = {4116--4121},
  issn = {0027-8424},
  doi = {10.1073/pnas.1421963112},
  urldate = {2025-02-24},
  abstract = {The discovery of grid cells in the entorhinal cortex quickly led to the hypothesis that grid cell inputs might be responsible for the generation of discrete place fields downstream in the hippocampus. Simulations have shown that if this conjecture is correct, then lesions of dorsal medial entorhinal cortex (MEC), which expresses the most tightly spaced grids, should cause place fields to expand, whereas lesions of ventral MEC, which expresses the most widely spaced grids, should cause fields to contract. Using muscimol to make small inactivations along the dorsoventral axis of the MEC, we tested this hypothesis. Our results deviate from the prediction but most likely result from a combination of the predicted effects and a weakening of the self-motion signal within the MEC., Both hippocampal place fields and medial entorhinal cortex (MEC) grid fields increase in scale along the dorsoventral axis. Because the connections from MEC to hippocampus are topographically organized and divergent, it has been hypothesized that place fields are generated by a Fourier-like summation of inputs over a range of spatial scales. This hypothesis predicts that inactivation of dorsal MEC should cause place field expansion, whereas inactivation of ventral MEC should cause field contraction. Inactivation of dorsal MEC caused substantial expansion of place fields; however, as inactivations were made more ventrally, the effect diminished but never switched to contraction. Expansion was accompanied by proportional decreases in theta power, intrinsic oscillation frequencies, phase precession slopes, and firing rates. Our results are most consistent with the predicted loss of specific Fourier components coupled with a path integration gain reduction, which raises the overall place field scale and masks the contraction expected from ventral inactivations.},
  pmcid = {PMC4386360},
  pmid = {25733884},
  file = {/Users/daniekru/Zotero/storage/GHGFK2PQ/Ormond and McNaughton - 2015 - Place field expansion after focal MEC inactivations is consistent with loss of Fourier components an.pdf}
}

@article{ortnerNeuromorphicHardwareLearns2019,
  title = {Neuromorphic {{Hardware Learns}} to {{Learn}}},
  author = {Ortner, Thomas and Scherr, Franz and Pehle, Christian and Meier, Kyle and Maass, Wolfgang},
  year = {2019},
  month = may,
  journal = {Frontiers in Neuroscience},
  volume = {13},
  doi = {10.3389/fnins.2019.00483},
  abstract = {Hyperparameters and learning algorithms for neuromorphic hardware are usually chosen by hand to suit a particular task. In contrast, networks of neurons in the brain were optimized through extensive evolutionary and developmental processes to work well on a range of computing and learning tasks. Occasionally this process has been emulated through genetic algorithms, but these require themselves hand-design of their details and tend to provide a limited range of improvements. We employ instead other powerful gradient-free optimization tools, such as cross-entropy methods and evolutionary strategies, in order to port the function of biological optimization processes to neuromorphic hardware. As an example, we show these optimization algorithms enable neuromorphic agents to learn very efficiently from rewards. In particular, meta-plasticity, i.e., the optimization of the learning rule which they use, substantially enhances reward-based learning capability of the hardware. In addition, we demonstrate for the first time Learning-to-Learn benefits from such hardware, in particular, the capability to extract abstract knowledge from prior learning experiences that speeds up the learning of new but related tasks. Learning-to-Learn is especially suited for accelerated neuromorphic hardware, since it makes it feasible to carry out the required very large number of network computations.},
  file = {/Users/daniekru/Zotero/storage/BRUCLEYK/Ortner et al. - 2019 - Neuromorphic Hardware Learns to Learn.pdf}
}

@article{palacios-filardoAcetylcholinePrioritisesDirect2021,
  title = {Acetylcholine Prioritises Direct Synaptic Inputs from Entorhinal Cortex to {{CA1}} by Differential Modulation of Feedforward Inhibitory Circuits},
  author = {{Palacios-Filardo}, Jon and Udakis, Matt and Brown, Giles A. and Tehan, Benjamin G. and Congreve, Miles S. and Nathan, Pradeep J. and Brown, Alastair J. H. and Mellor, Jack R.},
  year = {2021},
  month = sep,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5475},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25280-5},
  urldate = {2023-05-09},
  abstract = {Acetylcholine release in the hippocampus plays a central role in the formation of new memory representations. An influential but largely untested theory proposes that memory formation requires acetylcholine to enhance responses in CA1 to new sensory information from entorhinal cortex whilst depressing inputs from previously encoded representations in CA3. Here, we show that excitatory inputs from entorhinal cortex and CA3 are depressed equally by synaptic release of acetylcholine in CA1. However, feedforward inhibition from entorhinal cortex exhibits greater depression than CA3 resulting in a selective enhancement of excitatory-inhibitory balance and CA1 activation by entorhinal inputs. Entorhinal and CA3 pathways engage different feedforward interneuron subpopulations and cholinergic modulation of presynaptic function is mediated differentially by muscarinic M3 and M4 receptors, respectively. Thus, our data support a role and mechanisms for acetylcholine to prioritise novel information inputs to CA1 during memory formation.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Cellular neuroscience,Neural circuits,Neuroscience,Synaptic transmission},
  file = {/Users/daniekru/Zotero/storage/8UXPXUL4/Palacios-Filardo et al. - 2021 - Acetylcholine prioritises direct synaptic inputs f.pdf}
}

@article{palacios-filardoNeuromodulationHippocampalLongterm2019,
  title = {Neuromodulation of Hippocampal Long-Term Synaptic Plasticity},
  author = {{Palacios-Filardo}, Jon and Mellor, Jack R},
  year = {2019},
  month = feb,
  journal = {Current Opinion in Neurobiology},
  series = {Neurobiology of {{Learning}} and {{Plasticity}}},
  volume = {54},
  pages = {37--43},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2018.08.009},
  urldate = {2023-02-07},
  abstract = {Multiple neuromodulators including acetylcholine, noradrenaline, dopamine and serotonin are released in response to uncertainty to focus attention on events where the predicted outcome does not match observed reality. In these situations, internal representations need to be updated, a process that requires long-term synaptic plasticity. Through a variety of common and divergent mechanisms, it is recently shown that all these neuromodulators facilitate the induction and/or expression of long-term synaptic plasticity within the hippocampus. Under physiological conditions, this may be critical for suprathreshold induction of plasticity endowing neuromodulators with a gating function and providing a mechanism by which neuromodulators enable the targeted updating of memory with relevant information to improve the accuracy of future predictions.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FKLPPV5R/Palacios-Filardo and Mellor - 2019 - Neuromodulation of hippocampal long-term synaptic .pdf;/Users/daniekru/Zotero/storage/BZ6Y7TBE/S0959438818301041.html}
}

@techreport{pangNonHebbianCodeEpisodic2024,
  type = {Preprint},
  title = {A Non-{{Hebbian}} Code for Episodic Memory},
  author = {Pang, Rich and Recanatesi, Stefano},
  year = {2024},
  month = feb,
  institution = {Neuroscience},
  doi = {10.1101/2024.02.28.582531},
  urldate = {2024-03-08},
  abstract = {Hebbian plasticity has long dominated neurobiological models of memory formation. Yet plasticity rules operating on one-shot episodic memory timescales rarely depend on both pre- and postsynaptic spiking, challenging Hebbian theory in this crucial regime. To address this, we present an episodic memory model governed by a simple non-Hebbian rule depending only on presynaptic activity. We show that this rule, capitalizing on high-dimensional neural activity with restricted transitions, naturally stores episodes as paths through complex state spaces like those underlying a world model. The resulting memory traces, which we term path vectors, are highly expressive and decodable with an odor-tracking algorithm. We show that path vectors are robust alternatives to Hebbian traces when created via spiking and support diverse one-shot sequential and associative recall tasks, and policy learning. Thus, non-Hebbian plasticity is sufficient for flexible memory and learning, and well-suited to encode episodes and policies as paths through a world model.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZAHFXHRR/Pang and Recanatesi - 2024 - A non-Hebbian code for episodic memory.pdf}
}

@article{parkSymmetryLearningRate2017,
  title = {Symmetry of Learning Rate in Synaptic Plasticity Modulates Formation of Flexible and Stable Memories},
  author = {Park, Youngjin and Choi, Woochul and Paik, Se-Bum},
  year = {2017},
  month = jul,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {5671},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-05929-2},
  urldate = {2024-12-09},
  abstract = {Spike-timing-dependent plasticity (STDP) is considered critical to learning and memory functions in the human brain. Across various types of synapse, STDP is observed as different profiles of Hebbian and anti-Hebbian learning rules. However, the specific roles of diverse STDP profiles in memory formation still remain elusive. Here, we show that the symmetry of the learning rate profile in STDP is crucial to determining the character of stored memory. Using computer simulations, we found that an asymmetric learning rate generates flexible memory that is volatile and easily overwritten by newly appended information. Moreover, a symmetric learning rate generates stable memory that can coexist with newly appended information. In addition, by combining these two conditions, we could realize a hybrid memory type that operates in a way intermediate between stable and flexible memory. Our results demonstrate that various attributes of memory functions may originate from differences in the synaptic stability.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Learning algorithms,Neural circuits,Spike-timing-dependent plasticity},
  file = {/Users/daniekru/Zotero/storage/SPH68UTU/Park et al. - 2017 - Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories.pdf}
}

@article{parkSymmetryLearningRate2017a,
  title = {Symmetry of Learning Rate in Synaptic Plasticity Modulates Formation of Flexible and Stable Memories},
  author = {Park, Youngjin and Choi, Woochul and Paik, Se-Bum},
  year = {2017},
  month = jul,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {5671},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-05929-2},
  urldate = {2024-12-09},
  abstract = {Spike-timing-dependent plasticity (STDP) is considered critical to learning and memory functions in the human brain. Across various types of synapse, STDP is observed as different profiles of Hebbian and anti-Hebbian learning rules. However, the specific roles of diverse STDP profiles in memory formation still remain elusive. Here, we show that the symmetry of the learning rate profile in STDP is crucial to determining the character of stored memory. Using computer simulations, we found that an asymmetric learning rate generates flexible memory that is volatile and easily overwritten by newly appended information. Moreover, a symmetric learning rate generates stable memory that can coexist with newly appended information. In addition, by combining these two conditions, we could realize a hybrid memory type that operates in a way intermediate between stable and flexible memory. Our results demonstrate that various attributes of memory functions may originate from differences in the synaptic stability.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Learning algorithms,Neural circuits,Spike-timing-dependent plasticity},
  file = {/Users/daniekru/Zotero/storage/L26S9AVW/Park et al. - 2017 - Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories.pdf}
}

@article{parrEditorialProbabilisticPerspectives2021,
  title = {Editorial: {{Probabilistic Perspectives}} on {{Brain}} ({{Dys}})Function},
  shorttitle = {Editorial},
  author = {Parr, Thomas and Markovi{\'c}, Dimitrije and Ramstead, Maxwell James D. and Smith, Ryan and Hesp, Casper and Friston, Karl},
  year = {2021},
  month = jun,
  journal = {Frontiers in Artificial Intelligence},
  volume = {4},
  pages = {710179},
  issn = {2624-8212},
  doi = {10.3389/frai.2021.710179},
  urldate = {2021-12-01},
  pmcid = {PMC8215382},
  pmid = {34164617},
  file = {/Users/daniekru/Zotero/storage/NM3MWJC3/Parr et al. - 2021 - Editorial Probabilistic Perspectives on Brain (Dy.pdf}
}

@article{pascanuNeurodynamicalModelWorking2011,
  title = {A Neurodynamical Model for Working Memory},
  author = {Pascanu, Razvan and Jaeger, Herbert},
  year = {2011},
  month = mar,
  journal = {Neural Networks},
  volume = {24},
  number = {2},
  pages = {199--207},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2010.10.003},
  urldate = {2024-05-13},
  abstract = {Neurodynamical models of working memory (WM) should provide mechanisms for storing, maintaining, retrieving, and deleting information. Many models address only a subset of these aspects. Here we present a rather simple WM model in which all of these performance modes are trained into a recurrent neural network (RNN) of the echo state network (ESN) type. The model is demonstrated on a bracket level parsing task with a stream of rich and noisy graphical script input. In terms of nonlinear dynamics, memory states correspond, intuitively, to attractors in an input-driven system. As a supplementary contribution, the article proposes a rigorous formal framework to describe such attractors, generalizing from the standard definition of attractors in autonomous (input-free) dynamical systems.},
  keywords = {Attractor,Echo state networks,Recurrent neural networks,Working memory},
  file = {/Users/daniekru/Zotero/storage/3WI7NL5V/S0893608010001899.html}
}

@techreport{payeurBurstdependentSynapticPlasticity2020,
  type = {Preprint},
  title = {Burst-Dependent Synaptic Plasticity Can Coordinate Learning in Hierarchical Circuits},
  author = {Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A. and Naud, Richard},
  year = {2020},
  month = mar,
  institution = {Neuroscience},
  doi = {10.1101/2020.03.30.015511},
  urldate = {2020-05-25},
  abstract = {Synaptic plasticity is believed to be a key physiological mechanism for learning. It is well-established that it depends on pre and postsynaptic activity. However, models that rely solely on pre and postsynaptic activity for synaptic changes have, to date, not been able to account for learning complex tasks that demand hierarchical networks. Here, we show that if synaptic plasticity is regulated by high-frequency bursts of spikes, then neurons higher in the hierarchy can coordinate the plasticity of lower-level connections. Using simulations and mathematical analyses, we demonstrate that, when paired with short-term synaptic dynamics, regenerative activity in the apical dendrites, and synaptic plasticity in feedback pathways, a burst-dependent learning rule can solve challenging tasks that require deep network architectures. Our results demonstrate that well-known properties of dendrites, synapses, and synaptic plasticity are sufficient to enable sophisticated learning in hierarchical circuits.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FSL83IMM/Payeur et al. - 2020 - Burst-dependent synaptic plasticity can coordinate.pdf}
}

@misc{PDFConceptualDistinctiveness,
  title = {(1) ({{PDF}}) {{Conceptual Distinctiveness Supports Detailed Visual Long-Term Memory}} for {{Real-World Objects}}},
  journal = {ResearchGate},
  urldate = {2020-06-06},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/45460079\_Conceptual\_Distinctiveness\_Supports\_Detailed\_Visual\_Long-Term\_Memory\_for\_Real-World\_Objects},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/QNK7IZZH/45460079_Conceptual_Distinctiveness_Supports_Detailed_Visual_Long-Term_Memory_for_Real-World_Ob.html}
}

@article{peerFormatCognitiveMap2024,
  title = {The Format of the Cognitive Map Depends on the Structure of the Environment.},
  author = {Peer, Michael and Nadar, Catherine and Epstein, Russell A.},
  year = {2024},
  month = jan,
  journal = {Journal of Experimental Psychology: General},
  volume = {153},
  number = {1},
  pages = {224--240},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0001498},
  urldate = {2025-02-15},
  copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/YZ9TXBAI/Peer et al. - 2024 - The format of the cognitive map depends on the structure of the environment..pdf}
}

@article{peerStructuringKnowledgeCognitive2021,
  title = {Structuring {{Knowledge}} with {{Cognitive Maps}} and {{Cognitive Graphs}}},
  author = {Peer, Michael and Brunec, Iva K. and Newcombe, Nora S. and Epstein, Russell A.},
  year = {2021},
  month = jan,
  journal = {Trends in cognitive sciences},
  volume = {25},
  number = {1},
  pages = {37--54},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2020.10.004},
  urldate = {2025-02-15},
  abstract = {Humans and animals use mental representations of the spatial structure of the world to navigate. The classical view is that these representations take the form of Euclidean cognitive maps, but alternative theories suggest they are cognitive graphs consisting of locations connected by paths. Here we review evidence suggesting that both map-like and graph-like representations exist in the mind/brain, relying on partially overlapping neural systems. Maps and graphs can operate simultaneously or separately, and they may be applied to both spatial and nonspatial knowledge. By providing structural frameworks for complex information, cognitive maps and cognitive graphs may provide fundamental organizing schemata that allow us to navigate in physical, social, and conceptual spaces.},
  pmcid = {PMC7746605},
  pmid = {33248898},
  file = {/Users/daniekru/Zotero/storage/Z9GM3U7Z/Peer et al. - 2021 - Structuring Knowledge with Cognitive Maps and Cognitive Graphs.pdf}
}

@misc{PerceptualDissociationsViews,
  title = {Perceptual Dissociations among Views of Objects, Scenes, and Reachable Spaces},
  journal = {ResearchGate},
  urldate = {2020-06-06},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/328845294\_Perceptual\_dissociations\_among\_views\_of\_objects\_scenes\_and\_reachable\_spaces},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FNBI8Q6S/328845294_Perceptual_dissociations_among_views_of_objects_scenes_and_reachable_spaces.html}
}

@article{petersModulatingNeuromodulatorsDopamine2021,
  title = {Modulating the {{Neuromodulators}}: {{Dopamine}}, {{Serotonin}}, and the {{Endocannabinoid System}}},
  shorttitle = {Modulating the {{Neuromodulators}}},
  author = {Peters, Kate Z. and Cheer, Joseph F. and Tonini, Raffaella},
  year = {2021},
  month = jun,
  journal = {Trends in Neurosciences},
  volume = {44},
  number = {6},
  pages = {464--477},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2021.02.001},
  urldate = {2022-10-27},
  abstract = {Dopamine (DA), serotonin (5-hydroxytryptamine, 5-HT), and endocannabinoids (ECs) are key neuromodulators involved in many aspects of motivated behavior, including reward processing, reinforcement learning, and behavioral flexibility. Among the longstanding views about possible relationships between these neuromodulators is the idea of DA and 5-HT acting as opponents. This view has been challenged by emerging evidence that 5-HT supports reward seeking via activation of DA neurons in the ventral tegmental area. Adding an extra layer of complexity to these interactions, the endocannabinoid system is uniquely placed to influence dopaminergic and serotonergic neurotransmission. In this review we discuss how these three neuromodulatory systems interact at the cellular and circuit levels. Technological advances that facilitate precise identification and control of genetically targeted neuronal populations will help to achieve a better understanding of the complex relationship between these essential systems, and the potential relevance for motivated behavior.},
  langid = {english},
  keywords = {addiction,circuits,motivation,neuromodulation,reward,to study},
  file = {/Users/daniekru/Zotero/storage/3HU384CA/Peters et al. - 2021 - Modulating the Neuromodulators Dopamine, Serotoni.pdf;/Users/daniekru/Zotero/storage/CGWLCDJ2/S0166223621000229.html}
}

@article{petrovStateSpacesSemigroups,
  title = {From {{State Spaces}} to {{Semigroups}}: {{Leveraging Algebraic Formalism}} for {{Automated Planning}}},
  author = {Petrov, Alice and Muise, Christian},
  abstract = {This paper introduces an algebraic formalism linking transformation semigroups and the state transition systems induced by classical planning problems. We investigate some basic planning problems with interesting properties and establish fundamental characteristics of the corresponding semigroups, such as their ideals and Green's relations. Furthermore, we leverage semigroup theory to propose new approaches to existing concepts in automated planning, including the identification of landmark actions and the study of dead ends. We demonstrate that algebraic results can be applied to facilitate an understanding of a planning problem's state space and explore its solutions, thus verifying the relevance and effectiveness of such formal modeling.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/GUB9GDSG/Petrov and Muise - From State Spaces to Semigroups Leveraging Algebr.pdf}
}

@article{pettersenStudyingPlaceCell,
  title = {Studying {{Place Cell Formation}} and {{Remapping}} in an {{Artificial Neural Network Model}}},
  author = {Pettersen, Markus Borud},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FQKNACEP/Pettersen - Studying Place Cell Formation and Remapping in an .pdf}
}

@inproceedings{pietrowObjectsDetectionRecognition2017,
  title = {Objects Detection and Recognition System Using Artificial Neural Networks and Drones},
  booktitle = {2017 {{Signal Processing Symposium}} ({{SPSympo}})},
  author = {Pietrow, Dymitr and Matuszewski, Jan},
  year = {2017},
  month = sep,
  pages = {1--5},
  doi = {10.1109/SPS.2017.8053689},
  abstract = {The paper presents digital image objects detection and recognition system using artificial neural networks and drones. It contains description based on example of person identification system where face is the key object of processing. It describes the structure of the system and components of the learning sub-system as well as the processing sub-system (detection, recognition). It consists of a description and examples of the learning and processing algorithms and the technologies applied. The results of calculations of efficiency and speed of each algorithm are presented by tables and appropriate characteristics. The article also describes possibilities of further developments of system.},
  keywords = {Algorithm design and analysis,artificial neural networks,autonomous aerial vehicles,Biological neural networks,detection,digital image objects detection,drones,Drones,Graphics processing units,Histograms,image recognition,Instruction sets,learning (artificial intelligence),learning sub-system,neural nets,neural networks,object detection,objects recognition system,person identification system,processing sub-system,recognition,robot vision},
  file = {/Users/daniekru/Zotero/storage/4XR8Z9S7/8053689.html}
}

@article{pikaTakingTurnsBridging2018,
  title = {Taking Turns: Bridging the Gap between Human and Animal Communication},
  shorttitle = {Taking Turns},
  author = {Pika, Simone and Wilkinson, Ray and Kendrick, Kobin H. and Vernes, Sonja C.},
  year = {2018},
  month = jun,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {285},
  number = {1880},
  pages = {20180598},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2018.0598},
  urldate = {2023-06-27},
  abstract = {Language, humans' most distinctive trait, still remains a `mystery' for evolutionary theory. It is underpinned by a universal infrastructure---cooperative turn-taking---which has been suggested as an ancient mechanism bridging the existing gap between the articulate human species and their inarticulate primate cousins. However, we know remarkably little about turn-taking systems of non-human animals, and methodological confounds have often prevented meaningful cross-species comparisons. Thus, the extent to which cooperative turn-taking is uniquely human or represents a homologous and/or analogous trait is currently unknown. The present paper draws attention to this promising research avenue by providing an overview of the state of the art of turn-taking in four animal taxa---birds, mammals, insects and anurans. It concludes with a new comparative framework to spur more research into this research domain and to test which elements of the human turn-taking system are shared across species and taxa.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/LY7UJZ9W/Pika et al. - 2018 - Taking turns bridging the gap between human and a.pdf}
}

@article{pilarskiOptimalPolicyBernoulli2021,
  title = {Optimal {{Policy}} for {{Bernoulli Bandits}}: {{Computation}} and {{Algorithm Gauge}}},
  shorttitle = {Optimal {{Policy}} for {{Bernoulli Bandits}}},
  author = {Pilarski, Sebastian and Pilarski, Slawomir and Varro, Daniel},
  year = {2021},
  month = feb,
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {2},
  number = {1},
  pages = {2--17},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3074122},
  urldate = {2024-03-25},
  abstract = {Bernoulli multi-armed bandits are a reinforcement learning model used to study a variety of choice optimization problems. Often such optimizations concern a finite-time horizon. In principle, statistically optimal policies can be computed via dynamic programming, but doing so is considered infeasible due to prohibitive computational requirements and implementation complexity. Hence, suboptimal algorithms are applied in practice, despite their unknown level of suboptimality. In this article, we demonstrate that optimal policies can be efficiently computed for large time horizons or number of arms thanks to a novel memory organization and indexing scheme. We use optimal policies to gauge the suboptimality of several well-known finite- and infinitetime horizon algorithms including Whittle and Gittins indices, epsilon-greedy, Thompson sampling, and upper-confidence bound (UCB) algorithms. Our simulation study shows that all but one evaluated algorithm perform significantly worse than the optimal policy. The Whittle index offers a nearly optimal strategy for multiarmed Bernoulli bandits despite its suboptimal decisions---up to 10\%---compared to an optimal policy table. Lastly, we discuss optimizations of known algorithms. We derive a novel solution from UCB1-tuned. It outperforms other infinite-time horizon algorithms when dealing with many arms.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FRRR75ZZ/Pilarski et al. - 2021 - Optimal Policy for Bernoulli Bandits Computation .pdf}
}

@techreport{podlaskiHighCapacityDynamic2020,
  type = {Preprint},
  title = {High Capacity and Dynamic Accessibility in Associative Memory Networks with Context-Dependent Neuronal and Synaptic Gating},
  author = {Podlaski, William F. and Agnes, Everton J. and Vogels, Tim P.},
  year = {2020},
  month = jan,
  institution = {Neuroscience},
  doi = {10.1101/2020.01.08.898528},
  urldate = {2023-05-29},
  abstract = {Biological memory is known to be flexible --- memory formation and recall depend on factors such as the behavioral context of the organism. However, this property is often ignored in associative memory models. Here, we bring this dynamic nature of memory to the fore by introducing a novel model of associative memory, which we refer to as the context-modular memory network. In our model, stored memory patterns are associated to one of several background network states, or contexts. Memories are accessible when their corresponding context is active, and are otherwise inaccessible. Context modulates the effective network connectivity by imposing a specific configuration of neuronal and synaptic gating -- gated neurons (respectively synapses) have their activity (respectively weights) momentarily silenced, thereby reducing interference from memories belonging to other contexts. Memory patterns are randomly and independently chosen, while neuronal and synaptic gates may be selected randomly or optimized through a process of contextual synaptic refinement. Through signal-to-noise and mean field analyses, we show that context-modular memory networks can exhibit substantially increased memory capacity with random neuronal gating, but not with random synaptic gating. For contextual synaptic refinement, we devise a method in which synapses are gated off for a given context if they destabilize the memory patterns in that context, drastically improving memory capacity. Notably, synaptic refinement allows for patterns to be accessible in multiple contexts, stabilizing memory patterns even for weight matrices that do not contain any information about the memory patterns such as Gaussian random matrices. Lastly, we show that context modulates the relative stability of accessible versus inaccessible memories, thereby confirming that contextual control acts as a mechanism to temporarily hide or reveal particular memories. Overall, our model integrates recent ideas about context-dependent memory organization with classic associative memory models, highlights an intriguing trade-off between memory capacity and accessibility, and carries important implications for the understanding of biological memory storage and recall in the brain.},
  langid = {english}
}

@misc{pogodinSynapticWeightDistributions2024,
  title = {Synaptic {{Weight Distributions Depend}} on the {{Geometry}} of {{Plasticity}}},
  author = {Pogodin, Roman and Cornford, Jonathan and Ghosh, Arna and Gidel, Gauthier and Lajoie, Guillaume and Richards, Blake},
  year = {2024},
  month = mar,
  number = {arXiv:2305.19394},
  eprint = {2305.19394},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-03-11},
  abstract = {A growing literature in computational neuroscience leverages gradient descent and learning algorithms that approximate it to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes - i.e. the geometry of synaptic plasticity. Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with non-Euclidean distances. Finally, we show that it should be possible to experimentally test for different synaptic geometries by comparing synaptic weight distributions before and after learning. Overall, our work shows that the current paradigm in theoretical work on synaptic plasticity that assumes Euclidean synaptic geometry may be misguided and that it should be possible to experimentally determine the true geometry of synaptic plasticity in the brain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,to study},
  file = {/Users/daniekru/Zotero/storage/DQMDVJ44/Pogodin et al. - 2024 - Synaptic Weight Distributions Depend on the Geomet.pdf;/Users/daniekru/Zotero/storage/E2EBQDRJ/2305.html}
}

@misc{polatRatsRelyAirflow2023,
  title = {Rats Rely on Airflow Cues for Self-Motion Perception},
  author = {Polat, Lior and Harpaz, Tamar and Zaidel, Adam},
  year = {2023},
  month = dec,
  primaryclass = {New Results},
  pages = {2023.12.19.572298},
  publisher = {bioRxiv},
  doi = {10.1101/2023.12.19.572298},
  urldate = {2025-02-18},
  abstract = {Self-motion perception is a vital skill for all species. It is an inherently multisensory process, that combines inertial (body-based) and relative (with respect to the environment) motion cues. While extensively studied in human and non-human primates, there is currently no paradigm to test self-motion perception in rodents using both inertial and relative self-motion cues. We developed a novel rodent motion simulator using two synchronized robotic arms to generate inertial, relative or combined (inertial and relative) cues of self-motion. Eight rats were trained to perform a task of heading-discrimination, similar to the popular primate paradigm. Strikingly, the rats relied heavily on airflow for relative self-motion perception, with little contribution from optic flow (performance in the dark was almost as good). Relative self-motion (airflow) was perceived with greater reliability vs. inertial. Disrupting airflow (using a fan or windshield) damaged relative, but not inertial, self-motion perception. However, whiskers were not needed for this function. Lastly, the rats integrated relative and inertial self-motion cues in a reliability-based (Bayesian-like) manner. These results implicate airflow as a dominant cue for self-motion perception in rats, and provide a new domain to investigate the neural bases of self-motion perception and multisensory processing in awake behaving rodents.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/LS39VE6Z/Polat et al. - 2023 - Rats rely on airflow cues for self-motion perception.pdf}
}

@article{poucetSpatialCognitiveMaps1993,
  title = {Spatial Cognitive Maps in Animals: {{New}} Hypotheses on Their Structure and Neural Mechanisms},
  shorttitle = {Spatial Cognitive Maps in Animals},
  author = {Poucet, Bruno},
  year = {1993},
  journal = {Psychological Review},
  volume = {100},
  number = {2},
  pages = {163--182},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.100.2.163},
  abstract = {Provides a hierarchical model of animal spatial cognitive maps. Such maps include both topological information, which affords loose, yet operational, representations of the connectivity of space and its overall arrangement, and metric information, which provides information about angles and distances. The model holds that maps can be initially described as a set of location-dependent reference frameworks providing directional information about other locations. The addition of an overall directional reference allows for the buildup of more complete (allocentric) representations. A survey of recent neurobiological data provides some hints about the brain structures involved in these processes and suggests that the hippocampal formation and the posterior parietal cortex would act differently by handling topological and metric information, respectively. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Animals,Cognitive Maps,Models,Neurobiology,Spatial Organization},
  file = {/Users/daniekru/Zotero/storage/9ZWIEJTS/Poucet - 1993 - Spatial cognitive maps in animals New hypotheses on their structure and neural mechanisms.pdf;/Users/daniekru/Zotero/storage/Z28A2L3U/1993-28408-001.html}
}

@article{prillDynamicPropertiesNetwork2005,
  title = {Dynamic {{Properties}} of {{Network Motifs Contribute}} to {{Biological Network Organization}}},
  author = {Prill, Robert J. and Iglesias, Pablo A. and Levchenko, Andre},
  year = {2005},
  month = oct,
  journal = {PLOS Biology},
  volume = {3},
  number = {11},
  pages = {e343},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0030343},
  urldate = {2022-11-17},
  abstract = {Biological networks, such as those describing gene regulation, signal transduction, and neural synapses, are representations of large-scale dynamic systems. Discovery of organizing principles of biological networks can be enhanced by embracing the notion that there is a deep interplay between network structure and system dynamics. Recently, many structural characteristics of these non-random networks have been identified, but dynamical implications of the features have not been explored comprehensively. We demonstrate by exhaustive computational analysis that a dynamical property---stability or robustness to small perturbations---is highly correlated with the relative abundance of small subnetworks (network motifs) in several previously determined biological networks. We propose that robust dynamical stability is an influential property that can determine the non-random structure of biological networks.},
  langid = {english},
  keywords = {Dynamical systems,Gene expression,Gene regulation,Genetic networks,Network analysis,Network motifs,Neural networks,Transcriptional control},
  file = {/Users/daniekru/Zotero/storage/B4WHDTX7/Prill et al. - 2005 - Dynamic Properties of Network Motifs Contribute to.pdf;/Users/daniekru/Zotero/storage/PGTI96JZ/article.html}
}

@article{puigDopamineModulationLearning2014,
  title = {Dopamine Modulation of Learning and Memory in the Prefrontal Cortex: Insights from Studies in Primates, Rodents, and Birds},
  shorttitle = {Dopamine Modulation of Learning and Memory in the Prefrontal Cortex},
  author = {Puig, M. Victoria and Rose, Jonas and Schmidt, Robert and Freund, Nadja},
  year = {2014},
  month = aug,
  journal = {Frontiers in Neural Circuits},
  volume = {8},
  publisher = {Frontiers},
  issn = {1662-5110},
  doi = {10.3389/fncir.2014.00093},
  urldate = {2024-04-26},
  abstract = {In this review, we provide a brief overview over the current knowledge about the role of dopamine transmission in the prefrontal cortex during learning and memory. We discuss work in humans, monkeys, rats, and birds in order to provide a basis for comparison across species that might help identify crucial features and constraints of the dopaminergic system in executive function. Computational models of dopamine function are introduced to provide a framework for such a comparison. We also provide a brief evolutionary perspective showing that the dopaminergic system is highly preserved across mammals. Even birds, following a largely independent evolution of higher cognitive abilities, have evolved a comparable dopaminergic system. Finally, we discuss the unique advantages and challenges of using different animal models for advancing our understanding of dopamine function in the healthy and diseased brain.},
  langid = {english},
  keywords = {dopamine receptors,evolution,Executive Function,Learning and Memory (Neurosciences),Neuromodulation,prefrontal cortex (PFC),working memory},
  file = {/Users/daniekru/Zotero/storage/T58MVUWQ/Puig et al. - 2014 - Dopamine modulation of learning and memory in the .pdf}
}

@misc{qiForcedExplorationBandit2023,
  title = {Forced {{Exploration}} in {{Bandit Problems}}},
  author = {Qi, Han and Guo, Fei and Zhu, Li},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07285},
  eprint = {2312.07285},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.07285},
  urldate = {2024-12-11},
  abstract = {The multi-armed bandit(MAB) is a classical sequential decision problem. Most work requires assumptions about the reward distribution (e.g., bounded), while practitioners may have difficulty obtaining information about these distributions to design models for their problems, especially in non-stationary MAB problems. This paper aims to design a multi-armed bandit algorithm that can be implemented without using information about the reward distribution while still achieving substantial regret upper bounds. To this end, we propose a novel algorithm alternating between greedy rule and forced exploration. Our method can be applied to Gaussian, Bernoulli and other subgaussian distributions, and its implementation does not require additional information. We employ a unified analysis method for different forced exploration strategies and provide problem-dependent regret upper bounds for stationary and piecewise-stationary settings. Furthermore, we compare our algorithm with popular bandit algorithms on different reward distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/NL68RNHP/Qi et al. - 2023 - Forced Exploration in Bandit Problems.pdf;/Users/daniekru/Zotero/storage/LKMRC3GV/2312.html}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/DTEF7CIS/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@article{ratteImpactNeuronalProperties2013,
  title = {Impact of {{Neuronal Properties}} on {{Network Coding}}: {{Roles}} of {{Spike Initiation Dynamics}} and {{Robust Synchrony Transfer}}},
  shorttitle = {Impact of {{Neuronal Properties}} on {{Network Coding}}},
  author = {Ratt{\'e}, St{\'e}phanie and Hong, Sungho and De Schutter, Erik and Prescott, Steven A.},
  year = {2013},
  month = jun,
  journal = {Neuron},
  volume = {78},
  number = {5},
  pages = {758--772},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.05.030},
  urldate = {2024-12-12},
  abstract = {Neural networks are more than the sum of their parts, but the properties of those parts are nonetheless important. For instance, neuronal properties affect the degree to which neurons receiving common input will spike synchronously, and whether that synchrony will propagate through the network. Stimulus-evoked synchrony can help or hinder network coding depending on the type of code. In this Perspective, we describe how spike initiation dynamics influence neuronal input-output properties, how those properties affect synchronization, and how synchronization affects network coding. We propose that synchronous and asynchronous spiking can be used to multiplex temporal (synchrony) and rate coding and discuss how pyramidal neurons would be well suited for that task.},
  file = {/Users/daniekru/Zotero/storage/LZLP6GCM/Ratté et al. - 2013 - Impact of Neuronal Properties on Network Coding Roles of Spike Initiation Dynamics and Robust Synch.pdf;/Users/daniekru/Zotero/storage/2IV4M2Z9/S0896627313004509.html}
}

@article{rebolaOperationPlasticityHippocampal2017,
  title = {Operation and Plasticity of Hippocampal {{CA3}} Circuits: Implications for Memory Encoding},
  shorttitle = {Operation and Plasticity of Hippocampal {{CA3}} Circuits},
  author = {Rebola, Nelson and Carta, Mario and Mulle, Christophe},
  year = {2017},
  month = apr,
  journal = {Nature Reviews Neuroscience},
  volume = {18},
  number = {4},
  pages = {208--220},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn.2017.10},
  urldate = {2023-04-12},
  abstract = {On theoretical grounds, the architecture of CA3 circuits seems to be well adapted for the rapid storage and retrieval of associative memories. This is thought to require plastic changes in the strength of specific synaptic contacts.Dentate gyrus cells provide sparse but powerful synaptic mossy fibre connections to CA3 pyramidal cells, which display a large dynamic range of presynaptic plasticity. This repertoire was recently extended to include postsynaptic plasticity of NMDA receptor-mediated excitatory postsynaptic currents (EPSCs), making these synapses competent for conventional long-term potentiation of AMPA receptor-mediated EPSCs.Local recurrent connectivity gives rise to the CA3 autoassociative network amenable to spike-timing dependent plasticity, which can be facilitated by heterosynaptic interactions.Local GABAergic loops control spike transfer at CA3 connections. GABAergic connectivity is subject to prominent structural and molecular plasticity in relation to memory encoding.Mice impaired in the plasticity of CA3--CA3 or dentate gyrus--CA3 connections show deficits in one-trial memory tasks. Nevertheless, a direct link between memory and functional plasticity of specific excitatory or inhibitory connections is still awaited.},
  copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Hippocampus,Long-term potentiation,Short-term potentiation,Spike-timing-dependent plasticity,to study},
  file = {/Users/daniekru/Zotero/storage/H748VS9F/Rebola et al. - 2017 - Operation and plasticity of hippocampal CA3 circui.pdf}
}

@article{recanatesiDimensionalityRecurrentSpiking2019,
  title = {Dimensionality in Recurrent Spiking Networks: {{Global}} Trends in Activity and Local Origins in Connectivity},
  shorttitle = {Dimensionality in Recurrent Spiking Networks},
  author = {Recanatesi, Stefano and Ocker, Gabriel Koch and Buice, Michael A. and {Shea-Brown}, Eric},
  year = {2019},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {7},
  pages = {e1006446},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006446},
  urldate = {2022-11-16},
  abstract = {The dimensionality of a network's collective activity is of increasing interest in neuroscience. This is because dimensionality provides a compact measure of how coordinated network-wide activity is, in terms of the number of modes (or degrees of freedom) that it can independently explore. A low number of modes suggests a compressed low dimensional neural code and reveals interpretable dynamics [1], while findings of high dimension may suggest flexible computations [2, 3]. Here, we address the fundamental question of how dimensionality is related to connectivity, in both autonomous and stimulus-driven networks. Working with a simple spiking network model, we derive three main findings. First, the dimensionality of global activity patterns can be strongly, and systematically, regulated by local connectivity structures. Second, the dimensionality is a better indicator than average correlations in determining how constrained neural activity is. Third, stimulus evoked neural activity interacts systematically with neural connectivity patterns, leading to network responses of either greater or lesser dimensionality than the stimulus.},
  langid = {english},
  keywords = {Action potentials,Covariance,Network analysis,Network motifs,Network reciprocity,Neural networks,Neurons,Scale-free networks},
  file = {/Users/daniekru/Zotero/storage/564GTME5/Recanatesi et al. - 2019 - Dimensionality in recurrent spiking networks Glob.pdf;/Users/daniekru/Zotero/storage/GS9X2HBV/article.html}
}

@misc{RediscoveringAreaCA2,
  title = {Rediscovering Area {{CA2}}: Unique Properties and Functions {\textbar} {{Nature Reviews Neuroscience}}},
  urldate = {2023-04-20},
  howpublished = {https://www.nature.com/articles/nrn.2015.22},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/LQURX38Y/nrn.2015.html}
}

@misc{renBraininspiredArtificialIntelligence2024,
  title = {Brain-Inspired {{Artificial Intelligence}}: {{A Comprehensive Review}}},
  shorttitle = {Brain-Inspired {{Artificial Intelligence}}},
  author = {Ren, Jing and Xia, Feng},
  year = {2024},
  month = aug,
  number = {arXiv:2408.14811},
  eprint = {2408.14811},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.14811},
  urldate = {2025-01-16},
  abstract = {Current artificial intelligence (AI) models often focus on enhancing performance through meticulous parameter tuning and optimization techniques. However, the fundamental design principles behind these models receive comparatively less attention, which can limit our understanding of their potential and constraints. This comprehensive review explores the diverse design inspirations that have shaped modern AI models, i.e., brain-inspired artificial intelligence (BIAI). We present a classification framework that categorizes BIAI approaches into physical structure-inspired and human behavior-inspired models. We also examine the real-world applications where different BIAI models excel, highlighting their practical benefits and deployment challenges. By delving into these areas, we provide new insights and propose future research directions to drive innovation and address current gaps in the field. This review offers researchers and practitioners a comprehensive overview of the BIAI landscape, helping them harness its potential and expedite advancements in AI development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/daniekru/Zotero/storage/F8JAFE7T/Ren and Xia - 2024 - Brain-inspired Artificial Intelligence A Comprehensive Review.pdf;/Users/daniekru/Zotero/storage/MKIX46KS/2408.html}
}

@article{retailleauMichelinRedGuide2014,
  title = {The {{Michelin}} Red Guide of the Brain: Role of Dopamine in Goal-Oriented Navigation},
  shorttitle = {The {{Michelin}} Red Guide of the Brain},
  author = {Retailleau, Aude and Boraud, Thomas},
  year = {2014},
  month = mar,
  journal = {Frontiers in Systems Neuroscience},
  volume = {8},
  publisher = {Frontiers},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2014.00032},
  urldate = {2025-02-17},
  abstract = {{$<$}p{$>$}Spatial learning has been recognized over the years to be under the control of the hippocampus and related temporal lobe structures. Hippocampal damage often causes severe impairments in the ability to learn and remember a location in space defined by distal visual cues. Such cognitive disabilities are found in Parkinsonian patients. We recently investigated the role of dopamine in navigation in the 6-Hydroxy-dopamine (6-OHDA) rat, a model of Parkinson's disease (PD) commonly used to investigate the pathophysiology of dopamine depletion (Retailleau et al., {$<$}xref ref-type="bibr" rid="B51"{$>$}2013{$<$}/xref{$>$}). We demonstrated that dopamine (DA) is essential to spatial learning as its depletion results in spatial impairments. Our results showed that the behavioral effect of DA depletion is correlated with modification of the neural encoding of spatial features and decision making processes in hippocampus. However, the origin of these alterations in the neural processing of the spatial information needs to be clarified. It could result from a local effect: dopamine depletion disturbs directly the processing of relevant spatial information at hippocampal level. Alternatively, it could result from a more distributed network effect: dopamine depletion elsewhere in the brain (entorhinal cortex, striatum, etc.) modifies the way hippocampus processes spatial information. Recent experimental evidence in rodents, demonstrated indeed, that other brain areas are involved in the acquisition of spatial information. Amongst these, the cortex---basal ganglia (BG) loop is known to be involved in reinforcement learning and has been identified as an important contributor to spatial learning. In particular, it has been shown that altered activity of the BG striatal complex can impair the ability to perform spatial learning tasks. The present review provides a glimpse of the findings obtained over the past decade that support a dialog between these two structures during spatial learning under DA control.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Basal Ganglia,Dopamine,Hippocampus,spatial navigation,Striatum},
  file = {/Users/daniekru/Zotero/storage/686YS7ML/Retailleau and Boraud - 2014 - The Michelin red guide of the brain role of dopamine in goal-oriented navigation.pdf}
}

@misc{ReviewNeuroscienceInspiredMachine,
  title = {A {{Review}} of {{Neuroscience-Inspired Machine Learning}}},
  urldate = {2025-01-16},
  howpublished = {https://arxiv.org/html/2403.18929v1},
  file = {/Users/daniekru/Zotero/storage/44Q2UNJS/2403.html}
}

@article{reynoldsDopaminedependentPlasticityCorticostriatal2002,
  title = {Dopamine-Dependent Plasticity of Corticostriatal Synapses},
  author = {Reynolds, John N.J and Wickens, Jeffery R},
  year = {2002},
  month = jun,
  journal = {Neural Networks},
  volume = {15},
  number = {4-6},
  pages = {507--521},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00045-X},
  urldate = {2024-05-07},
  abstract = {Knowledge of the effect of dopamine on corticostriatal synaptic plasticity has advanced rapidly over the last 5 years. We consider this new knowledge in relation to three factors proposed earlier to describe the rules for synaptic plasticity in the corticostriatal pathway. These factors are a phasic increase in dopamine release, presynaptic activity and postsynaptic depolarisation. A function is proposed which relates the amount of dopamine release in the striatum to the modulation of corticostriatal synaptic efficacy. It is argued that this function, and the experimental data from which it arises, are compatible with existing models which associate the reward-related firing of dopamine neurons with changes in corticostriatal synaptic efficacy. q 2002 Elsevier Science Ltd. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/68B6VBQA/Reynolds and Wickens - 2002 - Dopamine-dependent plasticity of corticostriatal s.pdf}
}

@article{ricebergRewardStabilityDetermines2012,
  title = {Reward {{Stability Determines}} the {{Contribution}} of {{Orbitofrontal Cortex}} to {{Adaptive Behavior}}},
  author = {Riceberg, Justin S. and Shapiro, Matthew L.},
  year = {2012},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {32},
  number = {46},
  pages = {16402--16409},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0776-12.2012},
  urldate = {2024-05-06},
  abstract = {Animals respond to changing contingencies to maximize reward. The orbitofrontal cortex (OFC) is important for flexible responding when established contingencies change, but the underlying cognitive mechanisms are debated. We tested rats with sham or OFC lesions in radial maze tasks that varied the frequency of contingency changes and measured both perseverative and non-perseverative errors. When contingencies were changed rarely, rats with sham lesions learned quickly and performed better than rats with OFC lesions. Rats with sham lesions made fewer non-perseverative errors, rarely entering non-rewarded arms, and more win--stay responses by returning to recently rewarded arms compared with rats with OFC lesions. When contingencies were changed rapidly, however, rats with sham lesions learned slower, made more non-perseverative errors and fewer lose--shift responses, and returned more often to non-rewarded arms than rats with OFC lesions. The results support the view that the OFC integrates reward history and suggest that the availability of outcome expectancy signals can either improve or impair adaptive responding depending on reward stability.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2012 the authors 0270-6474/12/3216402-08\$15.00/0},
  langid = {english},
  pmid = {23152622},
  file = {/Users/daniekru/Zotero/storage/QNUCGLPW/Riceberg and Shapiro - 2012 - Reward Stability Determines the Contribution of Or.pdf}
}

@article{richardsStudyPlasticityHas,
  title = {The Study of Plasticity Has Always Been about Gradients},
  author = {Richards, Blake Aaron and Kording, Konrad Paul},
  journal = {The Journal of Physiology},
  volume = {n/a},
  number = {n/a},
  issn = {1469-7793},
  doi = {10.1113/JP282747},
  urldate = {2023-04-26},
  abstract = {The experimental study of learning and plasticity has always been driven by an implicit question: how can physiological changes be adaptive and improve performance? For example, in Hebbian plasticity only synapses from presynaptic neurons that were active are changed, avoiding useless changes. Similarly, in dopamine-gated learning synapse changes depend on reward or lack thereof and do not change when everything is predictable. Within machine learning we can make the question of which changes are adaptive concrete: performance improves when changes correlate with the gradient of an objective function quantifying performance. This result is general for any system that improves through small changes. As such, physiology has always implicitly been seeking mechanisms that allow the brain to approximate gradients. Coming from this perspective we review the existing literature on plasticity-related mechanisms, and we show how these mechanisms relate to gradient estimation. We argue that gradients are a unifying idea to explain the many facets of neuronal plasticity. Abstract figure legend. This article is protected by copyright. All rights reserved},
  copyright = {This article is protected by copyright. All rights reserved},
  langid = {english},
  keywords = {feedback connections,gradients,intrinsic plasticity,learning,machine learning,neuromodulators,synaptic plasticity},
  file = {/Users/daniekru/Zotero/storage/RRJQR4M9/Richards and Kording - The study of plasticity has always been about grad.pdf;/Users/daniekru/Zotero/storage/62UJ2524/JP282747.html}
}

@misc{richensRobustAgentsLearn2024,
  title = {Robust Agents Learn Causal World Models},
  author = {Richens, Jonathan and Everitt, Tom},
  year = {2024},
  month = feb,
  number = {arXiv:2402.10877},
  eprint = {2402.10877},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-12},
  abstract = {It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,to study},
  file = {/Users/daniekru/Zotero/storage/NHH53L33/Richens and Everitt - 2024 - Robust agents learn causal world models.pdf;/Users/daniekru/Zotero/storage/J5FVZ3TX/2402.html}
}

@misc{RobertGhristHome,
  title = {Robert Ghrist Home Page},
  urldate = {2022-11-04},
  howpublished = {https://www2.math.upenn.edu/{\textasciitilde}ghrist/notes.html},
  file = {/Users/daniekru/Zotero/storage/KT8HBVN6/notes.html}
}

@article{roeschDopamineNeuronsEncode2007,
  title = {Dopamine Neurons Encode the Better Option in Rats Deciding between Differently Delayed or Sized Rewards},
  author = {Roesch, Matthew R. and Calu, Donna J. and Schoenbaum, Geoffrey},
  year = {2007},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {12},
  pages = {1615--1624},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn2013},
  urldate = {2024-04-26},
  abstract = {The dopamine system is thought to be involved in making decisions about reward. Here we recorded from the ventral tegmental area in rats learning to choose between differently delayed and sized rewards. As expected, the activity of many putative dopamine neurons reflected reward prediction errors, changing when the value of the reward increased or decreased unexpectedly. During learning, neural responses to reward in these neurons waned and responses to cues that predicted reward emerged. Notably, this cue-evoked activity varied with size and delay. Moreover, when rats were given a choice between two differently valued outcomes, the activity of the neurons initially reflected the more valuable option, even when it was not subsequently selected.},
  copyright = {2007 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/82Y9XINE/Roesch et al. - 2007 - Dopamine neurons encode the better option in rats .pdf}
}

@article{rohrmeierPrinciplesStructureBuilding2015,
  title = {Principles of Structure Building in Music, Language and Animal Song},
  author = {Rohrmeier, Martin and Zuidema, Willem and Wiggins, Geraint A. and Scharff, Constance},
  year = {2015},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {370},
  number = {1664},
  pages = {20140097},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2014.0097},
  urldate = {2023-07-20},
  abstract = {Human language, music and a variety of animal vocalizations constitute ways of sonic communication that exhibit remarkable structural complexity. While the complexities of language and possible parallels in animal communication have been discussed intensively, reflections on the complexity of music and animal song, and their comparisons, are underrepresented. In some ways, music and animal songs are more comparable to each other than to language as propositional semantics cannot be used as indicator of communicative success or wellformedness, and notions of grammaticality are less easily defined. This review brings together accounts of the principles of structure building in music and animal song. It relates them to corresponding models in formal language theory, the extended Chomsky hierarchy (CH), and their probabilistic counterparts. We further discuss common misunderstandings and shortcomings concerning the CH and suggest ways to move beyond. We discuss language, music and animal song in the context of their function and motivation and further integrate problems and issues that are less commonly addressed in the context of language, including continuous event spaces, features of sound and timbre, representation of temporality and interactions of multiple parallel feature streams. We discuss these aspects in the light of recent theoretical, cognitive, neuroscientific and modelling research in the domains of music, language and animal song.},
  keywords = {animal vocalization,Chomsky hierarchy,comparative perspective,computational modelling,language,music,to study},
  file = {/Users/daniekru/Zotero/storage/TMCFW59R/Rohrmeier et al. - 2015 - Principles of structure building in music, languag.pdf}
}

@article{rollsGenerationTimeHippocampal2019,
  title = {The {{Generation}} of {{Time}} in the {{Hippocampal Memory System}}},
  author = {Rolls, Edmund T. and Mills, Patrick},
  year = {2019},
  month = aug,
  journal = {Cell Reports},
  volume = {28},
  number = {7},
  pages = {1649-1658.e6},
  issn = {22111247},
  doi = {10.1016/j.celrep.2019.07.042},
  urldate = {2023-07-13},
  abstract = {We propose that ramping time cells in the lateral entorhinal cortex can be produced by synaptic adaptation and demonstrate this in an integrate-and-fire attractor network model. We propose that competitive networks in the hippocampal system can convert these entorhinal ramping cells into hippocampal time cells and demonstrate this in a competitive network. We propose that this conversion is necessary to provide orthogonal hippocampal time representations to encode the temporal sequence of events in hippocampal episodic memory, and we support that with analytic arguments. We demonstrate that this processing can produce hippocampal neuronal ensembles that not only show replay of the sequence later on, but can also do this in reverse order in reverse replay. This research addresses a major issue in neuroscience: the mechanisms by which time is encoded in the brain and how the time representations are then useful in the hippocampal memory of events and their order.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/VHJGRC9U/Rolls and Mills - 2019 - The Generation of Time in the Hippocampal Memory S.pdf}
}

@article{rollsSpatialViewCells1997,
  title = {Spatial {{View Cells}} in the {{Primate Hippocampus}}},
  author = {Rolls, Edmund T. and Robertson, Robert G. and {Georges-Fran{\c c}ois}, Pierre},
  year = {1997},
  journal = {European Journal of Neuroscience},
  volume = {9},
  number = {8},
  pages = {1789--1794},
  issn = {1460-9568},
  doi = {10.1111/j.1460-9568.1997.tb01538.x},
  urldate = {2023-03-06},
  abstract = {Hippocampal function was analysed by making recordings in rhesus monkeys actively walking in the laboratory. In a sample of 352 cells recorded in the hippocampus and parahippocampal cortex, a population of `spatial view' cells was found to respond when the monkey looked at a part of the environment. The responses of these hippocampal neurons (i) occur to a view of space `out there', not to the place where the monkey is, (ii) depend on where the monkey is looking, as shown by measuring eye position, (iii) do not encode head direction, and (iv) provide a spatial representation that is allocentric, i.e. in world coordinates. This representation of space `out there' would be an appropriate part of a primate memory system involved in memories of where in an environment an object was seen, and more generally in the memory of particular events or episodes, for which a spatial component normally provides part of the context.},
  langid = {english},
  keywords = {hippocampus,memory,place,space,view},
  file = {/Users/daniekru/Zotero/storage/V5ZUP6KZ/Rolls et al. - 1997 - Spatial View Cells in the Primate Hippocampus.pdf;/Users/daniekru/Zotero/storage/CAIVAPTN/j.1460-9568.1997.tb01538.html}
}

@article{rosenbloomFunctionalNeuroanatomyDecisionMaking2012,
  title = {The {{Functional Neuroanatomy}} of {{Decision-Making}}},
  author = {Rosenbloom, Michael H. and Schmahmann, Jeremy D. and Price, Bruce H.},
  year = {2012},
  month = jul,
  journal = {The Journal of Neuropsychiatry and Clinical Neurosciences},
  volume = {24},
  number = {3},
  pages = {266--277},
  publisher = {American Psychiatric Publishing},
  issn = {0895-0172},
  doi = {10.1176/appi.neuropsych.11060139},
  urldate = {2024-11-27},
  abstract = {Decision-making is a complex executive function that draws on past experience, present goals, and anticipation of outcome, and which is influenced by prevailing and predicted emotional tone and cultural context. Functional imaging investigations and focal lesion studies identify the orbitofrontal, anterior cingulate, and dorsolateral prefrontal cortices as critical to decision-making. The authors review the connections of these prefrontal regions with the neocortex, limbic system, basal ganglia, and cerebellum, highlight current ideas regarding the cognitive processes of decision-making that these networks subserve, and present a novel integrated neuroanatomical model for decision-making. Finally, clinical relevance of this circuitry is illustrated through a discussion of frontotemporal dementia, traumatic brain injury, and sociopathy.},
  file = {/Users/daniekru/Zotero/storage/6A6EJ78H/Rosenbloom et al. - 2012 - The Functional Neuroanatomy of Decision-Making.pdf}
}

@incollection{rothkegelJudgingSpatialRelations1998,
  title = {Judging {{Spatial Relations}} from {{Memory}}},
  booktitle = {Spatial {{Cognition}}: {{An Interdisciplinary Approach}} to {{Representing}} and {{Processing Spatial Knowledge}}},
  author = {Rothkegel, Rainer and Wender, Karl F. and Schumacher, Sabine},
  editor = {Freksa, Christian and Habel, Christopher and Wender, Karl F.},
  year = {1998},
  pages = {79--105},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-69342-4_5},
  urldate = {2025-02-23},
  abstract = {Representations and processes involved in judgments of spatial relations after route learning are investigated. The main objective is to decide which relations are explicitly represented and which are implicitly stored. Participants learned maps of fictitious cities by moving along streets on a computer screen. After learning, they estimated distances and bearings from memory. Response times were measured. Experiments 1 and 2 address the question of how distances along a route are represented in spatial memory. Reaction times increased with increasing number of objects along the paths, but not with increasing length of the paths. This supports the hypothesis that only distances between neighboring objects are explicitly encoded. Experiment 3 tested whether survey knowledge can emerge after route learning. Participants judged Euclidean distances and bearings. Reaction times for distance estimates support the hypotheses that survey knowledge has been developed in route learning. However, reaction times for bearing estimates did not conform with any of the predictions.},
  isbn = {978-3-540-69342-0},
  langid = {english},
  keywords = {Distance Judgment,Egocentric Distance,Neighboring Object,Spatial Relation,Survey Knowledge},
  file = {/Users/daniekru/Zotero/storage/B9USXETW/Rothkegel et al. - 1998 - Judging Spatial Relations from Memory.pdf}
}

@article{rouaultPrefrontalMechanismsCombining2019,
  title = {Prefrontal Mechanisms Combining Rewards and Beliefs in Human Decision-Making},
  author = {Rouault, Marion and Drugowitsch, Jan and Koechlin, Etienne},
  year = {2019},
  month = jan,
  journal = {Nature Communications},
  volume = {10},
  pages = {301},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08121-w},
  urldate = {2024-04-29},
  abstract = {In uncertain and changing environments, optimal decision-making requires integrating reward expectations with probabilistic beliefs about reward contingencies. Little is known, however, about how the prefrontal cortex (PFC), which subserves decision-making, combines these quantities. Here, using computational modelling and neuroimaging, we show that the ventromedial PFC encodes both reward expectations and proper beliefs about reward contingencies, while the dorsomedial PFC combines these quantities and guides choices that are at variance with those predicted by optimal decision theory: instead of integrating reward expectations with beliefs, the dorsomedial PFC built context-dependent reward expectations commensurable to beliefs and used these quantities as two concurrent appetitive components, driving choices. This neural mechanism accounts for well-known risk aversion effects in human decision-making. The results reveal that the irrationality of human choices commonly theorized as deriving from optimal computations over false beliefs, actually stems from suboptimal neural heuristics over rational beliefs about reward contingencies., Optimal decision-making requires integrating expectations about rewards with beliefs about reward contingencies. Here, the authors show that these aspects of reward are encoded in the ventromedial prefrontal cortex then combined in the dorsomedial prefrontal cortex, a process that guides choice biases characteristic of human decision-making.},
  pmcid = {PMC6336816},
  pmid = {30655534},
  file = {/Users/daniekru/Zotero/storage/N4FCVGQW/Rouault et al. - 2019 - Prefrontal mechanisms combining rewards and belief.pdf}
}

@article{rouaultPrefrontalMechanismsCombining2019a,
  title = {Prefrontal Mechanisms Combining Rewards and Beliefs in Human Decision-Making},
  author = {Rouault, Marion and Drugowitsch, Jan and Koechlin, Etienne},
  year = {2019},
  month = jan,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {301},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08121-w},
  urldate = {2024-11-27},
  abstract = {In uncertain and changing environments, optimal decision-making requires integrating reward expectations with probabilistic beliefs about reward contingencies. Little is known, however, about how the prefrontal cortex (PFC), which subserves decision-making, combines these quantities. Here, using computational modelling and neuroimaging, we show that the ventromedial PFC encodes both reward expectations and proper beliefs about reward contingencies, while the dorsomedial PFC combines these quantities and guides choices that are at variance with those predicted by optimal decision theory: instead of integrating reward expectations with beliefs, the dorsomedial PFC built context-dependent reward expectations commensurable to beliefs and used these quantities as two concurrent appetitive components, driving choices. This neural mechanism accounts for well-known risk aversion effects in human decision-making. The results reveal that the irrationality of human choices commonly theorized as deriving from optimal computations over false beliefs, actually stems from suboptimal neural heuristics over rational beliefs about reward contingencies.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Decision,Human behaviour},
  file = {/Users/daniekru/Zotero/storage/B2FRLAGM/Rouault et al. - 2019 - Prefrontal mechanisms combining rewards and beliefs in human decision-making.pdf}
}

@article{salajSpikefrequencyAdaptationProvides2020,
  title = {Spike-Frequency Adaptation Provides a Long Short-Term Memory to Networks of Spiking Neurons},
  author = {Salaj, Darjan and Subramoney, Anand and Krai{\v s}nikovi{\'c}, Ceca and Bellec, Guillaume and Legenstein, Robert and Maass, Wolfgang},
  year = {2020},
  month = may,
  journal = {bioRxiv},
  pages = {2020.05.11.081513},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2020.05.11.081513},
  urldate = {2020-05-26},
  abstract = {{$<$}p{$>$}Brains are able to integrate memory from the recent past into their current computations, seemingly without effort. This ability is critical for cognitive tasks such as speech understanding or working with sequences of symbols according to dynamically changing rules. But it has remained unknown how networks of spiking neurons in the brain can achieve that. We show that the presence of neurons with spike frequency adaptation makes a significant difference: Their inclusion in a network moves its performance for such computing tasks from a very low level close to the level of human performance. While artificial neural networks with special long short-term memory (LSTM) units had already reached such high performance levels, they lack biological plausibility. We find that neurons with spike-frequency adaptation, which occur especially frequently in higher cortical areas of the human brain, provide to brains a functional equivalent to LSTM units.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/QZMEVXQY/Salaj et al. - 2020 - Spike-frequency adaptation provides a long short-t.pdf;/Users/daniekru/Zotero/storage/EDWQ393U/2020.05.11.html}
}

@article{samavatSynapticInformationStorage2024,
  title = {Synaptic {{Information Storage Capacity Measured With Information Theory}}},
  author = {Samavat, Mohammad and Bartol, Thomas M. and Harris, Kristen M. and Sejnowski, Terrence J.},
  year = {2024},
  month = apr,
  journal = {Neural Computation},
  volume = {36},
  number = {5},
  pages = {781--802},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01659},
  urldate = {2024-12-12},
  abstract = {Variation in the strength of synapses can be quantified by measuring the anatomical properties of synapses. Quantifying precision of synaptic plasticity is fundamental to understanding information storage and retrieval in neural circuits. Synapses from the same axon onto the same dendrite have a common history of coactivation, making them ideal candidates for determining the precision of synaptic plasticity based on the similarity of their physical dimensions. Here, the precision and amount of information stored in synapse dimensions were quantified with Shannon information theory, expanding prior analysis that used signal detection theory (Bartol et~al., 2015). The two methods were compared using dendritic spine head volumes in the middle of the stratum radiatum of hippocampal area CA1 as well-defined measures of synaptic strength. Information theory delineated the number of distinguishable synaptic strengths based on nonoverlapping bins of dendritic spine head volumes. Shannon entropy was applied to measure synaptic information storage capacity (SISC) and resulted in a lower bound of 4.1 bits and upper bound of 4.59 bits of information based on 24 distinguishable sizes. We further compared the distribution of distinguishable sizes and a uniform distribution using Kullback-Leibler divergence and discovered that there was a nearly uniform distribution of spine head volumes across the sizes, suggesting optimal use of the distinguishable values. Thus, SISC provides a new analytical measure that can be generalized to probe synaptic strengths and capacity for plasticity in different brain regions of different species and among animals raised in different conditions or during learning. How brain diseases and disorders affect the precision of synaptic plasticity can also be probed.},
  file = {/Users/daniekru/Zotero/storage/ZPHWN89A/Samavat et al. - 2024 - Synaptic Information Storage Capacity Measured With Information Theory.pdf;/Users/daniekru/Zotero/storage/RT6RSMG6/Synaptic-Information-Storage-Capacity-Measured.html}
}

@article{sandved-smithComputationalPhenomenologyMental2021,
  title = {Towards a Computational Phenomenology of Mental Action: Modelling Meta-Awareness and Attentional Control with Deep Parametric Active Inference},
  shorttitle = {Towards a Computational Phenomenology of Mental Action},
  author = {{Sandved-Smith}, Lars and Hesp, Casper and Mattout, J{\'e}r{\'e}mie and Friston, Karl and Lutz, Antoine and Ramstead, Maxwell J D},
  year = {2021},
  month = dec,
  journal = {Neuroscience of Consciousness},
  volume = {2021},
  number = {1},
  pages = {niab018},
  issn = {2057-2107},
  doi = {10.1093/nc/niab018},
  urldate = {2023-04-27},
  abstract = {Meta-awareness refers to the capacity to explicitly notice the current content of consciousness and has been identified as a key component for the successful control of cognitive states, such as the deliberate direction of attention. This paper proposes a formal model of meta-awareness and attentional control using hierarchical active inference. To do so, we cast mental action as policy selection over higher-level cognitive states and add a further hierarchical level to model meta-awareness states that modulate the expected confidence (precision) in the mapping between observations and hidden cognitive states. We simulate the example of mind-wandering and its regulation during a task involving sustained selective attention on a perceptual object. This provides a computational case study for an inferential architecture that is apt to enable the emergence of these central components of human phenomenology, namely, the ability to access and control cognitive states. We propose that this approach can be generalized to other cognitive states, and hence, this paper provides the first steps towards the development of a computational phenomenology of mental action and more broadly of our ability to monitor and control our own cognitive states. Future steps of this work will focus on fitting the model with qualitative, behavioural, and neural data.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/69NDAZBH/Sandved-Smith et al. - 2021 - Towards a computational phenomenology of mental ac.pdf;/Users/daniekru/Zotero/storage/EUNVJH99/6358635.html}
}

@article{sargoliniConjunctiveRepresentationPosition2006,
  title = {Conjunctive {{Representation}} of {{Position}}, {{Direction}}, and {{Velocity}} in {{Entorhinal Cortex}}},
  author = {Sargolini, Francesca and Fyhn, Marianne and Hafting, Torkel and McNaughton, Bruce L. and Witter, Menno P. and Moser, May-Britt and Moser, Edvard I.},
  year = {2006},
  month = may,
  journal = {Science},
  volume = {312},
  number = {5774},
  pages = {758--762},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1125572},
  urldate = {2025-02-22},
  abstract = {Grid cells in the medial entorhinal cortex (MEC) are part of an environment-independent spatial coordinate system. To determine how information about location, direction, and distance is integrated in the grid-cell network, we recorded from each principal cell layer of MEC in rats that explored two-dimensional environments. Whereas layer II was predominated by grid cells, grid cells colocalized with head-direction cells and conjunctive grid {\texttimes} head-direction cells in the deeper layers. All cell types were modulated by running speed. The conjunction of positional, directional, and translational information in a single MEC cell type may enable grid coordinates to be updated during self-motion--based navigation.},
  file = {/Users/daniekru/Zotero/storage/YLKDTGMB/Sargolini et al. - 2006 - Conjunctive Representation of Position, Direction, and Velocity in Entorhinal Cortex.pdf}
}

@article{saxeExactSolutionsNonlinear2014,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = {2014},
  month = feb,
  journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
  eprint = {1312.6120},
  primaryclass = {cond-mat, q-bio, stat},
  urldate = {2021-12-03},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/Z83EXKGN/Saxe et al. - 2014 - Exact solutions to the nonlinear dynamics of learn.pdf;/Users/daniekru/Zotero/storage/FCBJUSDX/1312.html}
}

@article{schinaziHippocampalSizePredicts2013,
  title = {Hippocampal Size Predicts Rapid Learning of a Cognitive Map in Humans},
  author = {Schinazi, Victor R. and Nardi, Daniele and Newcombe, Nora S. and Shipley, Thomas F. and Epstein, Russell A.},
  year = {2013},
  journal = {Hippocampus},
  volume = {23},
  number = {6},
  pages = {515--528},
  issn = {1098-1063},
  doi = {10.1002/hipo.22111},
  urldate = {2025-02-22},
  abstract = {The idea that humans use flexible map-like representations of their environment to guide spatial navigation has a long and controversial history. One reason for this enduring controversy might be that individuals vary considerably in their ability to form and utilize cognitive maps. Here we investigate the behavioral and neuroanatomical signatures of these individual differences. Participants learned an unfamiliar campus environment over a period of three weeks. In their first visit, they learned the position of different buildings along two routes in separate areas of the campus. During the following weeks, they learned these routes for a second and third time, along with two paths that connected both areas of the campus. Behavioral assessments after each learning session indicated that subjects formed a coherent representation of the spatial structure of the entire campus after learning a single connecting path. Volumetric analyses of structural MRI data and voxel-based morphometry (VBM) indicated that the size of the right posterior hippocampus predicted the ability to use this spatial knowledge to make inferences about the relative positions of different buildings on the campus. An inverse relationship between gray matter volume and performance was observed in the caudate. These results suggest that (i) humans can rapidly acquire cognitive maps of large-scale environments and (ii) individual differences in hippocampal anatomy may provide the neuroanatomical substrate for individual differences in the ability to learn and flexibly use these cognitive maps. {\copyright} 2013 Wiley Periodicals, Inc.},
  copyright = {Copyright {\copyright} 2013 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {caudate,navigation,spatial learning,volumetry,voxel-based morphometry},
  file = {/Users/daniekru/Zotero/storage/SYMZ26JX/Schinazi et al. - 2013 - Hippocampal size predicts rapid learning of a cognitive map in humans.pdf;/Users/daniekru/Zotero/storage/JSM7KVNN/hipo.html}
}

@article{schmidgallBraininspiredLearningArtificial2024,
  title = {Brain-Inspired Learning in Artificial Neural Networks: {{A}} Review},
  shorttitle = {Brain-Inspired Learning in Artificial Neural Networks},
  author = {Schmidgall, Samuel and Ziaei, Rojin and Achterberg, Jascha and Kirsch, Louis and Hajiseyedrazi, S. Pardis and Eshraghian, Jason},
  year = {2024},
  month = may,
  journal = {APL Machine Learning},
  volume = {2},
  number = {2},
  pages = {021501},
  issn = {2770-9019},
  doi = {10.1063/5.0186054},
  urldate = {2025-01-16},
  abstract = {Artificial neural networks (ANNs) have emerged as an essential tool in machine learning, achieving remarkable success across diverse domains, including image and speech generation, game playing, and robotics. However, there exist fundamental differences between ANNs' operating mechanisms and those of the biological brain, particularly concerning learning processes. This paper presents a comprehensive review of current brain-inspired learning representations in artificial neural networks. We investigate the integration of more biologically plausible mechanisms, such as synaptic plasticity, to improve these networks' capabilities. Moreover, we delve into the potential advantages and challenges accompanying this approach. In this review, we pinpoint promising avenues for future research in this rapidly advancing field, which could bring us closer to understanding the essence of intelligence.},
  file = {/Users/daniekru/Zotero/storage/J8EPLIZ3/Schmidgall et al. - 2024 - Brain-inspired learning in artificial neural networks A review.pdf;/Users/daniekru/Zotero/storage/ZYHV5ZEN/Brain-inspired-learning-in-artificial-neural.html}
}

@article{schoyenHexagonsAllWay2025,
  title = {Hexagons All the Way down: Grid Cells as a Conformal Isometric Map of Space},
  shorttitle = {Hexagons All the Way Down},
  author = {Sch{\o}yen, Vemund Sigmundson and Beshkov, Kosio and Pettersen, Markus Borud and Hermansen, Erik and Holzhausen, Konstantin and {Malthe-S{\o}renssen}, Anders and Fyhn, Marianne and Lepper{\o}d, Mikkel Elle},
  year = {2025},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {21},
  number = {2},
  pages = {e1012804},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1012804},
  urldate = {2025-02-24},
  abstract = {Grid cells in the entorhinal cortex are known for their hexagonal spatial activity patterns and are thought to provide a neural metric for space, and support path integration. In this study, we further investigate grid cells as a metric of space by optimising them for a conformal isometric (CI) map of space using a model based on a superposition of plane waves. By optimising the phases within a single grid cell module, we find that the module can form a CI of two-dimensional flat space with phases arranging into a regular hexagonal pattern, supporting an accurate spatial metric. Additionally, we find that experimentally recorded grid cells exhibit CI properties, with one example module showing a phase arrangement similar to the hexagonal pattern observed in our model. These findings provide computational and preliminary experimental support for grid cells as a CI-based spatial representation. We also examine other properties that emerge in CI-optimised modules, including consistent energy expenditure across space and the minimal cell count required to support unique representation of space and maximally topologically persistent toroidal population activity. Altogether, our results suggest that grid cells are well-suited to form a CI map, with several beneficial properties arising from this organisation.},
  langid = {english},
  keywords = {Bioenergetics,Manifolds,Open data,Optimization,Permutation,Signal to noise ratio,Test statistics,Topology},
  file = {/Users/daniekru/Zotero/storage/7UAVTAH5/Schøyen et al. - 2025 - Hexagons all the way down grid cells as a conformal isometric map of space.pdf}
}

@article{schoyenNavigatingMultipleEnvironments,
  title = {Navigating {{Multiple Environments}} with {{Emergent Grid Cell Remapping}}},
  author = {Sch{\o}yen, Vemund and Pettersen, Markus Borud and Holzhausen, Konstantin and {Malthe-S{\o}renssen}, Anders and Lepper{\o}d, Mikkel Elle},
  abstract = {When animals encounter a new environment, their cells with spatially modulated activity such as place cells and grid cells remap. Grid cells are particular in their remapping in that populations coherently remap as observed in spacing, orientation and phase changes. This process points to generality in grid cell activity that forms a standard computation across environments, which many speculate is path integration. Recently, normative artificial neural network models have shown that path integration and grid-cell-like activity can be obtained in recurrent neural networks (RNN) trained to navigate in a simulated two-dimensional environment. Remarkably, the emergent spatial profile of these grid-like cells is similar to biological cell responses in that they set up a toroidal structure. However, these models have not yet been tested in multiple environments relative to biological remapping. In this work, we extend the RNN normative model to multiple environments, modelled with place cell global remapping. Through this perturbation, we show (i) that the model can navigate multiple environments, (ii) that putative grid cells remain biologically plausible, (iii) a mechanism of how grid cells may remap, and (iv) a discussion on generalisation with inertial navigation in the grid and place cell network.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/VPXTREJN/Schøyen et al. - Navigating Multiple Environments with Emergent Gri.pdf}
}

@article{schrimpfNeuralArchitectureLanguage2021,
  title = {The Neural Architecture of Language: {{Integrative}} Modeling Converges on Predictive Processing},
  shorttitle = {The Neural Architecture of Language},
  author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = {2021},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {118},
  number = {45},
  eprint = {27093649},
  eprinttype = {jstor},
  pages = {1--12},
  publisher = {National Academy of Sciences},
  issn = {0027-8424},
  urldate = {2024-05-06},
  abstract = {The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species' signature cognitive skill. We find that the most powerful ``transformer'' models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models' neural fits (``brain score'') and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
  file = {/Users/daniekru/Zotero/storage/8P9I7MU8/Schrimpf et al. - 2021 - The neural architecture of language Integrative m.pdf}
}

@article{schrumSolvingMultipleIsolated2016,
  title = {Solving {{Multiple Isolated}}, {{Interleaved}}, and {{Blended Tasks}} through {{Modular Neuroevolution}}},
  author = {Schrum, Jacob and Miikkulainen, Risto},
  year = {2016},
  month = sep,
  journal = {Evolutionary Computation},
  volume = {24},
  number = {3},
  pages = {459--490},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/EVCO_a_00181},
  urldate = {2023-01-20},
  abstract = {Many challenging sequential decision-making problems require agents to master multiple tasks. For instance, game agents may need to gather resources, attack opponents, and defend against attacks. Learning algorithms can thus benefit from having separate policies for these tasks, and from knowing when each one is appropriate. How well this approach works depends on how tightly coupled the tasks are. Three cases are identified: Isolated tasks have distinct semantics and do not interact, interleaved tasks have distinct semantics but do interact, and blended tasks have regions where semantics from multiple tasks overlap. Learning across multiple tasks is studied in this article with Modular Multiobjective NEAT, a neuroevolution framework applied to three variants of the challenging Ms. Pac-Man video game. In the standard blended version of the game, a surprising, highly effective machine-discovered task division surpasses humanspecified divisions, achieving the best scores to date in this game. In isolated and interleaved versions of the game, human-specified task divisions are also successful, though the best scores are surprisingly still achieved by machine discovery. Modular neuroevolution is thus shown to be capable of finding useful, unexpected task divisions better than those apparent to a human designer.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/5CZSQMUI/Schrum and Miikkulainen - 2016 - Solving Multiple Isolated, Interleaved, and Blende.pdf}
}

@article{schultzDopamineRewardPrediction2016,
  title = {Dopamine Reward Prediction Error Coding},
  author = {Schultz, Wolfram},
  year = {2016},
  month = mar,
  journal = {Dialogues in Clinical Neuroscience},
  volume = {18},
  number = {1},
  pages = {23--32},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.31887/DCNS.2016.18.1/wschultz},
  urldate = {2025-02-27},
  keywords = {dopamine,neuro-physiology,neuron,prediction,reward,striatum,substantia nigra,ventral tegmental area},
  file = {/Users/daniekru/Zotero/storage/N2NXILPL/Schultz - 2016 - Dopamine reward prediction error coding.pdf}
}

@article{schultzNeuralSubstratePrediction1997,
  title = {A {{Neural Substrate}} of {{Prediction}} and {{Reward}}},
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
  year = {1997},
  month = mar,
  journal = {Science},
  volume = {275},
  number = {5306},
  pages = {1593--1599},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.275.5306.1593},
  urldate = {2024-04-26},
  abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
  file = {/Users/daniekru/Zotero/storage/M99A4WN2/Schultz et al. - 1997 - A Neural Substrate of Prediction and Reward.pdf}
}

@article{schulzFindingStructureMultiarmed2020,
  title = {Finding Structure in Multi-Armed Bandits},
  author = {Schulz, Eric and Franklin, Nicholas T. and Gershman, Samuel J.},
  year = {2020},
  month = jun,
  journal = {Cognitive Psychology},
  volume = {119},
  pages = {101261},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2019.101261},
  urldate = {2024-05-07},
  abstract = {How do humans search for rewards? This question is commonly studied using multi-armed bandit tasks, which require participants to trade off exploration and exploitation. Standard multi-armed bandits assume that each option has an independent reward distribution. However, learning about options independently is unrealistic, since in the real world options often share an underlying structure. We study a class of structured bandit tasks, which we use to probe how generalization guides exploration. In a structured multi-armed bandit, options have a correlation structure dictated by a latent function. We focus on bandits in which rewards are linear functions of an option's spatial position. Across 5 experiments, we find evidence that participants utilize functional structure to guide their exploration, and also exhibit a learning-to-learn effect across rounds, becoming progressively faster at identifying the latent function. Our experiments rule out several heuristic explanations and show that the same findings obtain with non-linear functions. Comparing several models of learning and decision making, we find that the best model of human behavior in our tasks combines three computational mechanisms: (1) function learning, (2) clustering of reward distributions across rounds, and (3) uncertainty-guided exploration. Our results suggest that human reinforcement learning can utilize latent structure in sophisticated ways to improve efficiency.},
  keywords = {Decision making,Exploration-exploitation,Function learning,Gaussian process,Generalization,Latent structure,Learning,Learning-to-learn,Reinforcement learning,Structure learning},
  file = {/Users/daniekru/Zotero/storage/RC35QTAA/Schulz et al. - 2020 - Finding structure in multi-armed bandits.pdf;/Users/daniekru/Zotero/storage/WGWCDN48/S0010028519302518.html}
}

@article{schwartenbeckExplorationNoveltySurprise2013,
  title = {Exploration, Novelty, Surprise, and Free Energy Minimization},
  author = {Schwartenbeck, Philipp and FitzGerald, Thomas and Dolan, Ray and Friston, Karl},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  urldate = {2022-09-20},
  abstract = {This paper reviews recent developments under the free energy principle that introduce a normative perspective on classical economic (utilitarian) decision-making based on (active) Bayesian inference. It has been suggested that the free energy principle precludes novelty and complexity, because it assumes that biological systems---like ourselves---try to minimize the long-term average of surprise to maintain their homeostasis. However, recent formulations show that minimizing surprise leads naturally to concepts such as exploration and novelty bonuses. In this approach, agents infer a policy that minimizes surprise by minimizing the difference (or relative entropy) between likely and desired outcomes, which involves both pursuing the goal-state that has the highest expected utility (often termed ``exploitation'') and visiting a number of different goal-states (``exploration''). Crucially, the opportunity to visit new states increases the value of the current state. Casting decision-making problems within a variational framework, therefore, predicts that our behavior is governed by both the entropy and expected utility of future states. This dissolves any dialectic between minimizing surprise and exploration or novelty seeking.},
  file = {/Users/daniekru/Zotero/storage/JJ5GQBWQ/Schwartenbeck et al. - 2013 - Exploration, novelty, surprise, and free energy mi.pdf}
}

@misc{SequentialPredictiveLearning,
  title = {Sequential Predictive Learning Is a Unifying Theory for Hippocampal Representation and Replay {\textbar} {{bioRxiv}}},
  urldate = {2024-10-11},
  howpublished = {https://www.biorxiv.org/content/10.1101/2024.04.28.591528v2},
  file = {/Users/daniekru/Zotero/storage/4IPEDY6A/Sequential predictive learning is a unifying theory for hippocampal representation and replay  bioR.pdf}
}

@article{shainFMRIRevealsLanguagespecific2020,
  title = {{{fMRI}} Reveals Language-Specific Predictive Coding during Naturalistic Sentence Comprehension},
  author = {Shain, Cory and Blank, Idan Asher and {van Schijndel}, Marten and Schuler, William and Fedorenko, Evelina},
  year = {2020},
  month = feb,
  journal = {Neuropsychologia},
  volume = {138},
  pages = {107307},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2019.107307},
  urldate = {2024-05-08},
  abstract = {Much research in cognitive neuroscience supports prediction as a canonical computation of cognition across domains. Is such predictive coding implemented by feedback from higher-order domain-general circuits, or is it locally implemented in domain-specific circuits? What information sources are used to generate these predictions? This study addresses these two questions in the context of language processing. We present fMRI evidence from a naturalistic comprehension paradigm (1) that predictive coding in the brain's response to language is domain-specific, and (2) that these predictions are sensitive both to local word co-occurrence patterns and to hierarchical structure. Using a recently developed continuous-time deconvolutional regression technique that supports data-driven hemodynamic response function discovery from continuous BOLD signal fluctuations in response to naturalistic stimuli, we found effects of prediction measures in the language network but not in the domain-general multiple-demand network, which supports executive control processes and has been previously implicated in language comprehension. Moreover, within the language network, surface-level and structural prediction effects were separable. The predictability effects in the language network were substantial, with the model capturing over 37\% of explainable variance on held-out data. These findings indicate that human sentence processing mechanisms generate predictions about upcoming words using cognitive processes that are sensitive to hierarchical structure and specialized for language processing, rather than via feedback from high-level executive control mechanisms.},
  keywords = {fMRI,Language,Multiple demand network,Naturalistic,Predictive coding,Sentence processing,Surprisal,Syntactic structure},
  file = {/Users/daniekru/Zotero/storage/WXCWJR9A/S0028393219303495.html}
}

@misc{shervani-tabarMetaLearningBiologicallyPlausible2022,
  title = {Meta-{{Learning Biologically Plausible Plasticity Rules}} with {{Random Feedback Pathways}}},
  author = {{Shervani-Tabar}, Navid and Rosenbaum, Robert},
  year = {2022},
  month = nov,
  number = {arXiv:2210.16414},
  eprint = {2210.16414},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2022-11-04},
  abstract = {Backpropagation is widely used to train artificial neural networks, but its relationship to synaptic plasticity in the brain is unknown. Some biological models of backpropagation rely on feedback projections that are symmetric with feedforward connections, but experiments do not corroborate the existence of such symmetric backward connectivity. Random feedback alignment offers an alternative model in which errors are propagated backward through fixed, random backward connections. This approach successfully trains shallow models, but learns slowly and does not perform well with deeper models or online learning. In this study, we develop a novel meta-plasticity approach to discover interpretable, biologically plausible plasticity rules that improve online learning performance with fixed random feedback connections. The resulting plasticity rules show improved online training of deep models in the low data regime. Our results highlight the potential of meta-plasticity to discover effective, interpretable learning rules satisfying biological constraints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,to study},
  file = {/Users/daniekru/Zotero/storage/TPVXPY7C/Shervani-Tabar and Rosenbaum - 2022 - Meta-Learning Biologically Plausible Plasticity Ru.pdf;/Users/daniekru/Zotero/storage/52GVXC9R/2210.html}
}

@article{sheynikhovichLongtermMemorySynaptic2023,
  title = {Long-Term Memory, Synaptic Plasticity and Dopamine in Rodent Medial Prefrontal Cortex: {{Role}} in Executive Functions},
  shorttitle = {Long-Term Memory, Synaptic Plasticity and Dopamine in Rodent Medial Prefrontal Cortex},
  author = {Sheynikhovich, Denis and Otani, Satoru and Bai, Jing and Arleo, Angelo},
  year = {2023},
  month = jan,
  journal = {Frontiers in Behavioral Neuroscience},
  volume = {16},
  publisher = {Frontiers},
  issn = {1662-5153},
  doi = {10.3389/fnbeh.2022.1068271},
  urldate = {2024-04-26},
  abstract = {Mnemonic functions, supporting rodent behavior in complex tasks, include both long-term and (short-term) working memory components. While working memory is thought to rely on persistent activity states in an active neural network, long-term memory and synaptic plasticity contribute to the formation of the underlying synaptic structure, determining the range of possible states. Whereas the implication of working memory in executive functions, mediated by the prefrontal cortex (PFC) in primates and rodents, has been extensively studied, the contribution of long-term memory component to these tasks received little attention. This review summarizes available experimental data and theoretical work concerning cellular mechanisms of synaptic plasticity in the medial region of rodent PFC and the link between plasticity, memory and behavior in PFC-dependent tasks. A special attention is devoted to unique properties of dopaminergic modulation of prefrontal synaptic plasticity and its contribution to executive functions.},
  langid = {english},
  keywords = {behavioral flexibility,Computational models,Dopamine,executive functions,Long-term memory,Neuromodulation,Prefrontal Cortex,synaptic plasticity},
  file = {/Users/daniekru/Zotero/storage/M7NJNB4Y/Sheynikhovich et al. - 2023 - Long-term memory, synaptic plasticity and dopamine.pdf}
}

@misc{shinRecurrentPatternCompletion2023,
  title = {Recurrent Pattern Completion Drives the Neocortical Representation of Sensory Inference},
  author = {Shin, Hyeyoung and Ogando, Mora B. and Abdeladim, Lamiae and Durand, Severine and Belski, Hannah and Cabasco, Hannah and Loefler, Henry and Bawany, Ahad and Hardcastle, Ben and Wilkes, Josh and Nguyen, Katrina and Suarez, Lucas and Johnson, Tye and Han, Warren and Ouellette, Ben and Grasso, Connor and Swapp, Jackie and Ha, Vivian and Young, Ahrial and Caldejon, Shiella and Williford, Ali and Groblewski, Peter and Olsen, Shawn and Kiselycznyk, Carly and Lecoq, Jerome and Adesnik, Hillel},
  year = {2023},
  month = jun,
  primaryclass = {New Results},
  pages = {2023.06.05.543698},
  publisher = {bioRxiv},
  doi = {10.1101/2023.06.05.543698},
  urldate = {2023-06-07},
  abstract = {When sensory information is incomplete or ambiguous, the brain relies on prior expectations to infer perceptual objects. Despite the centrality of this process to perception, the neural mechanism of sensory inference is not known. Illusory contours (ICs) are key tools to study sensory inference because they contain edges or objects that are implied only by their spatial context. Using cellular resolution, mesoscale two-photon calcium imaging and multi-Neuropixels recordings in the mouse visual cortex, we identified a sparse subset of neurons in the primary visual cortex (V1) and higher visual areas that respond emergently to ICs. We found that these highly selective "IC-encoders" mediate the neural representation of IC inference. Strikingly, selective activation of these neurons using two-photon holographic optogenetics was sufficient to recreate IC representation in the rest of the V1 network, in the absence of any visual stimulus. This outlines a model in which primary sensory cortex facilitates sensory inference by selectively strengthening input patterns that match prior expectations through local, recurrent circuitry. Our data thus suggest a clear computational purpose for recurrence in the generation of holistic percepts under sensory ambiguity. More generally, selective reinforcement of top-down predictions by pattern-completing recurrent circuits in lower sensory cortices may constitute a key step in sensory inference.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/FBXLSPCA/Shin et al. - 2023 - Recurrent pattern completion drives the neocortica.pdf}
}

@misc{shouvalBehavioralTimeScale2020,
  title = {Behavioral {{Time Scale Plasticity}} of {{Place Fields}}: {{Mathematical Analysis}}},
  shorttitle = {Behavioral {{Time Scale Plasticity}} of {{Place Fields}}},
  author = {Shouval, Harel Z. and Cone, Ian},
  year = {2020},
  month = sep,
  primaryclass = {New Results},
  pages = {2020.09.11.293787},
  publisher = {bioRxiv},
  doi = {10.1101/2020.09.11.293787},
  urldate = {2023-07-14},
  abstract = {Traditional synaptic plasticity experiments and models depend on tight temporal correlations between pre- and postsynaptic activity. These tight temporal correlations, on the order of tens of milliseconds, are incompatible with significantly longer behavioral time scales, and as such might not be able to account for plasticity induced by behavior. Indeed, recent findings in hippocampus suggest that rapid, bidirectional synaptic plasticity which modifies place fields in CA1 operates at behavioral time scales. These experimental results suggest that presynaptic activity generates synaptic eligibility traces both for potentiation and depression, which last on the order of seconds. These traces can be converted to changes in synaptic efficacies by the activation of an instructive signal that depends on naturally occurring or experimentally induced plateau potentials. We have developed a simple mathematical model that is consistent with these observations. This model can be fully analyzed to find the fixed points of induced place fields, the convergence to these fixed points, and how these fixed points depend on system parameters such as the size and shape of presynaptic place fields, the animal's velocity, and the parameters of the plasticity rule.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/8C5JR4MP/Shouval and Cone - 2020 - Behavioral Time Scale Plasticity of Place Fields .pdf}
}

@article{shovalSnapShotNetworkMotifs2010,
  title = {{{SnapShot}}: {{Network Motifs}}},
  shorttitle = {{{SnapShot}}},
  author = {Shoval, Oren and Alon, Uri},
  year = {2010},
  month = oct,
  journal = {Cell},
  volume = {143},
  number = {2},
  pages = {326-326.e1},
  issn = {00928674},
  doi = {10.1016/j.cell.2010.09.050},
  urldate = {2022-11-04},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/N6FYNRJH/Shoval and Alon - 2010 - SnapShot Network Motifs.pdf}
}

@article{siegfordMultidisciplinaryApproachesAssessment2013,
  title = {Multidisciplinary Approaches and Assessment Techniques to Better Understand and Enhance Zoo Nonhuman Animal Welfare},
  author = {Siegford, Janice M.},
  year = {2013},
  journal = {Journal of applied animal welfare science: JAAWS},
  volume = {16},
  number = {4},
  pages = {300--318},
  issn = {1532-7604},
  doi = {10.1080/10888705.2013.827914},
  abstract = {Nonhuman animal welfare is a complex concept that encompasses an animal's biological functioning, emotional states, and opportunities to experience a natural life, including the performance of natural behaviors. Further, animal welfare can be viewed as quality of life from the perspective of the animal and thus must consider the animal's subjective experiences. Therefore, assessing and enhancing animal welfare should include multidisciplinary, scientific ventures that strive to create a complete picture of how animals' bodies and minds respond to both aversive and pleasant situations. Practical assessment of animal welfare should include outcome-based measures from the animal that provide information about the individual's welfare as well as resource-based measures that can help identify causes of or risk factors for poor welfare. Increasingly, scientists are examining the emotional states of animals as well as the impact of pain, pleasure, and consciousness on animal welfare. This article discusses approaches such as preference testing, instrumental learning, examination of space and resource use, and qualitative assessments of animal welfare that might be useful and practical for assessing and enhancing welfare in zoo settings.},
  langid = {english},
  pmid = {24079486},
  keywords = {Animal Welfare,Animals,Animals Zoo,Emotions,Pain Measurement,Pleasure},
  file = {/Users/daniekru/Zotero/storage/XN4V8HQS/Siegford - 2013 - Multidisciplinary approaches and assessment techni.pdf}
}

@misc{SimulationismFunctionEpisodic,
  title = {Simulationism and the {{Function}}(s) of {{Episodic Memory}}},
  journal = {ResearchGate},
  urldate = {2020-06-05},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/338845554\_Simulationism\_and\_the\_Functions\_of\_Episodic\_Memory},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/UZTFE86N/338845554_Simulationism_and_the_Functions_of_Episodic_Memory.html}
}

@article{skatchkovskyBayesianContinualLearning2022,
  title = {Bayesian Continual Learning via Spiking Neural Networks},
  author = {Skatchkovsky, Nicolas and Jang, Hyeryung and Simeone, Osvaldo},
  year = {2022},
  journal = {Frontiers in Computational Neuroscience},
  volume = {16},
  issn = {1662-5188},
  urldate = {2023-01-17},
  abstract = {Among the main features of biological intelligence are energy efficiency, capacity for continual adaptation, and risk management via uncertainty quantification. Neuromorphic engineering has been thus far mostly driven by the goal of implementing energy-efficient machines that take inspiration from the time-based computing paradigm of biological brains. In this paper, we take steps toward the design of neuromorphic systems that are capable of adaptation to changing learning tasks, while producing well-calibrated uncertainty quantification estimates. To this end, we derive online learning rules for spiking neural networks (SNNs) within a Bayesian continual learning framework. In it, each synaptic weight is represented by parameters that quantify the current epistemic uncertainty resulting from prior knowledge and observed data. The proposed online rules update the distribution parameters in a streaming fashion as data are observed. We instantiate the proposed approach for both real-valued and binary synaptic weights. Experimental results using Intel's Lava platform show the merits of Bayesian over frequentist learning in terms of capacity for adaptation and uncertainty quantification.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/PGMFFKR6/Skatchkovsky et al. - 2022 - Bayesian continual learning via spiking neural net.pdf}
}

@article{solariCholinergicModulationSpatial2018,
  title = {Cholinergic Modulation of Spatial Learning, Memory and Navigation},
  author = {Solari, Nicola and Hangya, Bal{\'a}zs},
  year = {2018},
  journal = {European Journal of Neuroscience},
  volume = {48},
  number = {5},
  pages = {2199--2230},
  issn = {1460-9568},
  doi = {10.1111/ejn.14089},
  urldate = {2024-03-16},
  abstract = {Spatial learning, including encoding and retrieval of spatial memories as well as holding spatial information in working memory generally serving navigation under a broad range of circumstances, relies on a network of structures. While central to this network are medial temporal lobe structures with a widely appreciated crucial function of the hippocampus, neocortical areas such as the posterior parietal cortex and the retrosplenial cortex also play essential roles. Since the hippocampus receives its main subcortical input from the medial septum of the basal forebrain (BF) cholinergic system, it is not surprising that the potential role of the septo-hippocampal pathway in spatial navigation has been investigated in many studies. Much less is known of the involvement in spatial cognition of the parallel projection system linking the posterior BF with neocortical areas. Here we review the current state of the art of the division of labour within this complex `navigation system', with special focus on how subcortical cholinergic inputs may regulate various aspects of spatial learning, memory and navigation.},
  copyright = {{\copyright} 2018 The Authors. European Journal of Neuroscience published by Federation of European Neuroscience Societies and JohnWiley \& Sons Ltd.},
  langid = {english},
  keywords = {acetylcholine,basal forebrain,hippocampus,posterior parietal cortex,retrosplenial cortex,to study},
  file = {/Users/daniekru/Zotero/storage/LG2KXQNT/Solari and Hangya - 2018 - Cholinergic modulation of spatial learning, memory.pdf;/Users/daniekru/Zotero/storage/SKC2LGU5/ejn.html}
}

@article{solstadGridCellsPlace2006,
  title = {From Grid Cells to Place Cells: {{A}} Mathematical Model},
  shorttitle = {From Grid Cells to Place Cells},
  author = {Solstad, Trygve and Moser, Edvard I. and Einevoll, Gaute T.},
  year = {2006},
  journal = {Hippocampus},
  volume = {16},
  number = {12},
  pages = {1026--1031},
  issn = {1098-1063},
  doi = {10.1002/hipo.20244},
  urldate = {2023-11-02},
  abstract = {Anatomical connectivity and recent neurophysiological results imply that grid cells in the medial entorhinal cortex are the principal cortical inputs to place cells in the hippocampus. The authors propose a model in which place fields of hippocampal pyramidal cells are formed by linear summation of appropriately weighted inputs from entorhinal grid cells. Single confined place fields could be formed by summing input from a modest number (10--50) of grid cells with relatively similar grid phases, diverse grid orientations, and a biologically plausible range of grid spacings. When the spatial phase variation in the grid-cell input was higher, multiple, and irregularly spaced firing fields were formed. These observations point to a number of possible constraints in the organization of functional connections between grid cells and place cells. {\copyright} 2006 Wiley-Liss, Inc.},
  langid = {english},
  keywords = {entorhinal cortex,grid cells,hippocampus,memory,place cells,spatial representation},
  file = {/Users/daniekru/Zotero/storage/5SHJY6AM/Solstad et al. - 2006 - From grid cells to place cells A mathematical mod.pdf;/Users/daniekru/Zotero/storage/JJVC9XPT/hipo.html}
}

@article{solstadRepresentationGeometricBorders2008,
  title = {Representation of {{Geometric Borders}} in the {{Entorhinal Cortex}}},
  author = {Solstad, Trygve and Boccara, Charlotte N. and Kropff, Emilio and Moser, May-Britt and Moser, Edvard I.},
  year = {2008},
  month = dec,
  journal = {Science},
  volume = {322},
  number = {5909},
  pages = {1865--1868},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1166466},
  urldate = {2025-02-22},
  abstract = {We report the existence of an entorhinal cell type that fires when an animal is close to the borders of the proximal environment. The orientation-specific edge-apposing activity of these ``border cells'' is maintained when the environment is stretched and during testing in enclosures of different size and shape in different rooms. Border cells are relatively sparse, making up less than 10\% of the local cell population, but can be found in all layers of the medial entorhinal cortex as well as the adjacent parasubiculum, often intermingled with head-direction cells and grid cells. Border cells may be instrumental in planning trajectories and anchoring grid fields and place fields to a geometric reference frame.},
  file = {/Users/daniekru/Zotero/storage/S3YLHAMZ/Solstad et al. - 2008 - Representation of Geometric Borders in the Entorhinal Cortex.pdf}
}

@article{solteszCA1PyramidalCell2018,
  title = {{{CA1}} Pyramidal Cell Diversity Enabling Parallel Information Processing in the Hippocampus},
  author = {Soltesz, Ivan and Losonczy, Attila},
  year = {2018},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {4},
  pages = {484--493},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0118-0},
  urldate = {2023-02-13},
  abstract = {Hippocampal network operations supporting spatial navigation and declarative memory are traditionally interpreted in a framework where each hippocampal area, such as the dentate gyrus, CA3, and CA1, consists of homogeneous populations of functionally equivalent principal neurons. However, heterogeneity within hippocampal principal cell populations, in particular within pyramidal cells at the main CA1 output node, is increasingly recognized and includes developmental, molecular, anatomical, and functional differences. Here we review recent progress in the delineation of hippocampal principal cell subpopulations by focusing on radially defined subpopulations of CA1 pyramidal cells, and we consider how functional segregation of information streams, in parallel channels with nonuniform properties, could represent a general organizational principle of the hippocampus supporting diverse behaviors.},
  copyright = {2018 The Publisher},
  langid = {english},
  keywords = {Cellular neuroscience,Hippocampus,to study},
  file = {/Users/daniekru/Zotero/storage/3FFZC8FT/Soltesz and Losonczy - 2018 - CA1 pyramidal cell diversity enabling parallel inf.pdf}
}

@article{sorscherUnifiedTheoryComputational2023,
  title = {A Unified Theory for the Computational and Mechanistic Origins of Grid Cells},
  author = {Sorscher, Ben and Mel, Gabriel C. and Ocko, Samuel A. and Giocomo, Lisa M. and Ganguli, Surya},
  year = {2023},
  month = jan,
  journal = {Neuron},
  volume = {111},
  number = {1},
  pages = {121-137.e13},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2022.10.003},
  urldate = {2023-09-24},
  abstract = {The discovery of entorhinal grid cells has generated considerable interest in how and why hexagonal firing fields might emerge in a generic manner from neural circuits, and what their computational significance might be. Here, we forge a link between the problem of path integration and the existence of hexagonal grids, by demonstrating that such grids arise in neural networks trained to path integrate under simple biologically plausible constraints. Moreover, we develop a unifying theory for why hexagonal grids are ubiquitous in path-integrator circuits. Such trained networks also yield powerful mechanistic hypotheses, exhibiting realistic levels of biological variability not captured by hand-designed models. We furthermore develop methods to analyze the connectome and activity maps of our networks to elucidate fundamental mechanisms underlying path integration. These methods provide a road map to go from connectomic and physiological measurements to conceptual understanding in a manner that could generalize to other settings.},
  keywords = {grid cells,mechanistic models,medial entorhinal cortex,navigation,neural data,neural fitting,neural networks,normative models,path integration,pattern formation,to study},
  file = {/Users/daniekru/Zotero/storage/MH6W9HVR/Sorscher et al. - 2023 - A unified theory for the computational and mechani.pdf;/Users/daniekru/Zotero/storage/5WFIQ4CC/S0896627322009072.html}
}

@inproceedings{sorscherUnifiedTheoryOrigin2019,
  title = {A Unified Theory for the Origin of Grid Cells through the Lens of Pattern Formation},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sorscher, Ben and Mel, Gabriel and Ganguli, Surya and Ocko, Samuel},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-07-09},
  abstract = {Grid cells in the brain fire in strikingly regular hexagonal patterns across space. There are currently two seemingly unrelated frameworks for understanding these patterns. Mechanistic models account for hexagonal firing fields as the result of pattern-forming dynamics in a recurrent neural network with hand-tuned center-surround connectivity. Normative models specify a neural architecture, a learning rule, and a navigational task, and observe that grid-like firing fields emerge due to the constraints of solving this task. Here we provide an analytic theory that unifies the two perspectives by casting the learning dynamics of neural networks trained on navigational tasks as a pattern forming dynamical system. This theory provides insight into the optimal solutions of diverse formulations of the normative task, and shows that symmetries in the representation of space correctly predict the structure of learned firing fields in trained neural networks. Further, our theory proves that a nonnegativity constraint on firing rates induces a symmetry-breaking mechanism which favors hexagonal firing fields.  We extend this theory to the case of learning multiple grid maps and demonstrate that optimal solutions consist of a hierarchy of maps with increasing length scales. These results unify previous accounts of grid cell firing and provide a novel framework for predicting the learned representations of recurrent neural networks.},
  file = {/Users/daniekru/Zotero/storage/F7HIYX28/Sorscher et al. - 2019 - A unified theory for the origin of grid cells thro.pdf}
}

@article{sosaNavigatingReward2021,
  title = {Navigating for Reward},
  author = {Sosa, Marielena and Giocomo, Lisa M.},
  year = {2021},
  month = aug,
  journal = {Nature Reviews Neuroscience},
  volume = {22},
  number = {8},
  pages = {472--487},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/s41583-021-00479-z},
  urldate = {2024-03-16},
  abstract = {An organism's survival can depend on its ability to recall and navigate to spatial locations associated with rewards, such as food or a home. Accumulating research has revealed that computations of reward and its prediction occur on multiple levels across a complex set of interacting brain regions, including those that support memory and navigation. However, how the brain coordinates the encoding, recall and use of reward information to guide navigation remains incompletely understood. In this Review, we propose that the brain's classical navigation centres --- the hippocampus and the entorhinal cortex --- are ideally suited to coordinate this larger network by representing both physical and mental space as a series of states. These states may be linked to reward via neuromodulatory inputs to the hippocampus--entorhinal cortex system. Hippocampal outputs can then broadcast sequences of states to the rest of the brain to store reward associations or to facilitate decision-making, potentially engaging additional value signals downstream. This proposal is supported by recent advances in both experimental and theoretical neuroscience. By discussing the neural systems traditionally tied to navigation and reward at their intersection, we aim to offer an integrated framework for understanding navigation to reward as a fundamental feature of many cognitive processes.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Hippocampus,Reward,to study},
  file = {/Users/daniekru/Zotero/storage/7VQGDZYQ/Sosa and Giocomo - 2021 - Navigating for reward.pdf}
}

@article{sosaNavigatingReward2021a,
  title = {Navigating for Reward},
  author = {Sosa, Marielena and Giocomo, Lisa M},
  year = {2021},
  month = aug,
  journal = {Nature reviews. Neuroscience},
  volume = {22},
  number = {8},
  pages = {472--487},
  issn = {1471-003X},
  doi = {10.1038/s41583-021-00479-z},
  urldate = {2025-02-22},
  abstract = {An organism's survival can depend on its ability to recall and navigate to spatial locations associated with rewards, such as food or a home. Accumulating research has revealed that computations of reward and its prediction occur on multiple levels across a complex set of interacting brain regions, including those that support memory and navigation. Yet, how the brain coordinates the encoding, recall, and use of reward information to guide navigation remains incompletely understood. In this Review, we propose that the brain's classical navigation centres --- the hippocampus (HPC) and entorhinal cortex (EC) --- are ideally suited to coordinate this larger network, by representing both physical and mental space as a series of states. These states may be linked to reward via neuromodulatory inputs to the HPC--EC system. Hippocampal outputs can then broadcast sequences of states to the rest of the brain to store reward associations or to facilitate decision-making, potentially engaging additional value signals downstream. This proposal is supported by recent advances in both experimental and theoretical neuroscience. By discussing the neural systems traditionally tied to navigation and reward at their intersection, we aim to offer an integrated framework for understanding navigation to reward as a fundamental feature of many cognitive processes.},
  pmcid = {PMC9575993},
  pmid = {34230644},
  file = {/Users/daniekru/Zotero/storage/U38GA9JX/Sosa and Giocomo - 2021 - Navigating for reward.pdf}
}

@misc{SpontaneousEvolutionModularity,
  title = {Spontaneous Evolution of Modularity and Network Motifs},
  doi = {10.1073/pnas.0503610102},
  urldate = {2022-11-17},
  howpublished = {https://www.pnas.org/doi/10.1073/pnas.0503610102},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/KQVC25VW/Spontaneous evolution of modularity and network mo.pdf;/Users/daniekru/Zotero/storage/UKH9UK3Y/pnas.html}
}

@article{spratlingReviewPredictiveCoding2017,
  title = {A Review of Predictive Coding Algorithms},
  author = {Spratling, M. W.},
  year = {2017},
  month = mar,
  journal = {Brain and Cognition},
  series = {Perspectives on {{Human Probabilistic Inferences}} and the '{{Bayesian Brain}}'},
  volume = {112},
  pages = {92--97},
  issn = {0278-2626},
  doi = {10.1016/j.bandc.2015.11.003},
  urldate = {2025-01-31},
  abstract = {Predictive coding is a leading theory of how the brain performs probabilistic inference. However, there are a number of distinct algorithms which are described by the term ``predictive coding''. This article provides a concise review of these different predictive coding algorithms, highlighting their similarities and differences. Five algorithms are covered: linear predictive coding which has a long and influential history in the signal processing literature; the first neuroscience-related application of predictive coding to explaining the function of the retina; and three versions of predictive coding that have been proposed to model cortical function. While all these algorithms aim to fit a generative model to sensory data, they differ in the type of generative model they employ, in the process used to optimise the fit between the model and sensory data, and in the way that they are related to neurobiology.},
  keywords = {Cortex,Free energy,Neural networks,Predictive coding,Retina,Signal processing},
  file = {/Users/daniekru/Zotero/storage/TCCS9BNA/Spratling - 2017 - A review of predictive coding algorithms.pdf;/Users/daniekru/Zotero/storage/GPAUIQPV/S027826261530035X.html}
}

@inbook{stampRevealingIntroductionHidden2017,
  title = {A {{Revealing Introduction}} to {{Hidden Markov Models}}},
  booktitle = {Introduction to {{Machine Learning}} with {{Applications}} in {{Information Security}}},
  author = {Stamp, Mark},
  year = {2017},
  month = sep,
  edition = {1},
  pages = {7--35},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315213262-2},
  urldate = {2023-08-31},
  collaborator = {Stamp, Mark},
  isbn = {978-1-315-21326-2},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/3B7ZTUNF/Stamp - 2017 - A Revealing Introduction to Hidden Markov Models.pdf}
}

@article{stanleyEvolvingNeuralNetworks2002,
  title = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  year = {2002},
  month = jun,
  journal = {Evolutionary Computation},
  volume = {10},
  number = {2},
  pages = {99--127},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/106365602320169811},
  urldate = {2021-10-07},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/SQYJD9UV/Stanley and Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topolo.pdf}
}

@article{stanleyHypercubeBasedEncodingEvolving2009,
  title = {A {{Hypercube-Based Encoding}} for {{Evolving Large-Scale Neural Networks}}},
  author = {Stanley, Kenneth and D'Ambrosio, David and Gauci, Jason},
  year = {2009},
  month = jan,
  journal = {Artificial Life},
  volume = {15},
  number = {2},
  pages = {185--212},
  doi = {10.1162/artl.2009.15.2.15202},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/WJRVMKIG/Stanley et al. - 2009 - A Hypercube-Based Encoding for Evolving Large-Scal.pdf;/Users/daniekru/Zotero/storage/AB3HHSU5/2178.html}
}

@article{steyversBayesianAnalysisHuman2009a,
  title = {A {{Bayesian}} Analysis of Human Decision-Making on Bandit Problems},
  author = {Steyvers, Mark and Lee, Michael D. and Wagenmakers, Eric-Jan},
  year = {2009},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  volume = {53},
  number = {3},
  pages = {168--179},
  issn = {00222496},
  doi = {10.1016/j.jmp.2008.11.002},
  urldate = {2024-05-07},
  abstract = {The bandit problem is a dynamic decision-making task that is simply described, well-suited to controlled laboratory study, and representative of a broad class of real-world problems. In bandit problems, people must choose between a set of alternatives, each with different unknown reward rates, to maximize the total reward they receive over a fixed number of trials. A key feature of the task is that it challenges people to balance the exploration of unfamiliar choices with the exploitation of familiar ones. We use a Bayesian model of optimal decision-making on the task, in which how people balance exploration with exploitation depends on their assumptions about the distribution of reward rates. We also use Bayesian model selection measures that assess how well people adhere to an optimal decision process, compared to simpler heuristic decision strategies. Using these models, we make inferences about the decision-making of 451 participants who completed a set of bandit problems, and relate various measures of their performance to other psychological variables, including psychometric assessments of cognitive abilities and personality traits. We find clear evidence of individual differences in the way the participants made decisions on the bandit problems, and some interesting correlations with measures of general intelligence.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/RS9CLBYS/Steyvers et al. - 2009 - A Bayesian analysis of human decision-making on ba.pdf}
}

@article{stoewerNeuralNetworkBased2023,
  title = {Neural Network Based Formation of Cognitive Maps of Semantic Spaces and the Putative Emergence of Abstract Concepts},
  author = {Stoewer, Paul and Schilling, Achim and Maier, Andreas and Krauss, Patrick},
  year = {2023},
  month = mar,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {3644},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-30307-6},
  urldate = {2025-02-17},
  abstract = {How do we make sense of the input from our sensory organs, and put the perceived information into context of our past experiences? The hippocampal-entorhinal complex plays a major role in the organization of memory and thought. The formation of and navigation in cognitive maps of arbitrary mental spaces via place and grid cells can serve as a representation of memories and experiences and their relations to each other. The multi-scale successor representation is proposed to be the mathematical principle underlying place and grid cell computations. Here, we present a neural network, which learns a cognitive map of a semantic space based on 32 different animal species encoded as feature vectors. The neural network successfully learns the similarities between different animal species, and constructs a cognitive map of `animal space' based on the principle of successor representations with an accuracy of around 30\% which is near to the theoretical maximum regarding the fact that all animal species have more than one possible successor, i.e. nearest neighbor in feature space. Furthermore, a hierarchical structure, i.e. different scales of cognitive maps, can be modeled based on multi-scale successor representations. We find that, in fine-grained cognitive maps, the animal vectors are evenly distributed in feature space. In contrast, in coarse-grained maps, animal vectors are highly clustered according to their biological class, i.e. amphibians, mammals and insects. This could be a putative mechanism enabling the emergence of new, abstract semantic concepts. Finally, even completely new or incomplete input can be represented by interpolation of the representations from the cognitive map with remarkable high accuracy of up to 95\%. We conclude that the successor representation can serve as a weighted pointer to past memories and experiences, and may therefore be a crucial building block to include prior knowledge, and to derive context knowledge from novel input. Thus, our model provides a new tool to complement contemporary deep learning approaches on the road towards artificial general intelligence.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Language,Network models},
  file = {/Users/daniekru/Zotero/storage/J8FKY2KK/Stoewer et al. - 2023 - Neural network based formation of cognitive maps of semantic spaces and the putative emergence of ab.pdf}
}

@article{stoneNetworkMotifsTheir2019,
  title = {Network Motifs and Their Origins},
  author = {Stone, Lewi and Simberloff, Daniel and {Artzy-Randrup}, Yael},
  year = {2019},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {4},
  pages = {e1006749},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006749},
  urldate = {2022-11-17},
  abstract = {Author summary Modern network science is a new and exciting research field that has transformed the study of complex systems over the last 2 decades. Of particular interest is the identification of small ``network motifs'' that might be embedded in a larger network and that indicate the presence of evolutionary design principles or have an overly influential role on system-wide dynamics. Motifs are patterns of interconnections, or subgraphs, that appear in an observed network significantly more often than in compatible randomized networks. The concept of network motifs was introduced into Systems Biology by Milo, Alon and colleagues in 2002, quickly revolutionized the field, and it has had a huge impact in wider scientific domains ever since. Here, we argue that the same concept and tools for the detection of motifs were well known in the ecological literature decades into the last century, a fact that is generally not recognized. We review the early history of network motifs, their evolution in the mathematics literature, and their recent rediscoveries.},
  langid = {english},
  keywords = {Bibliometrics,Islands,Network motifs,Neural networks,Social networks,Species interactions,Systems biology,to study,Transcriptional control},
  file = {/Users/daniekru/Zotero/storage/69NP9D6H/Stone et al. - 2019 - Network motifs and their origins.pdf;/Users/daniekru/Zotero/storage/CGHI92A3/article.html}
}

@article{stuartDendriticIntegration602015,
  title = {Dendritic Integration: 60 Years of Progress},
  shorttitle = {Dendritic Integration},
  author = {Stuart, Greg J and Spruston, Nelson},
  year = {2015},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {18},
  number = {12},
  pages = {1713--1721},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4157},
  urldate = {2020-05-22},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/WQRS33CI/Stuart and Spruston - 2015 - Dendritic integration 60 years of progress.pdf}
}

@article{sugisakiCholinergicModulationSpike2011,
  title = {Cholinergic Modulation on Spike Timing-Dependent Plasticity in Hippocampal {{CA1}} Network},
  author = {Sugisaki, E. and Fukushima, Y. and Tsukada, M. and Aihara, T.},
  year = {2011},
  month = sep,
  journal = {Neuroscience},
  volume = {192},
  pages = {91--101},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2011.06.064},
  urldate = {2023-04-28},
  abstract = {Cholinergic inputs from the medial septum are projected to pyramidal neurons in the hippocampal CA1 region and release acetylcholine (ACh) from their terminals. The cholinergic inputs are considered to be integrated with sensory inputs and to play a crucial role in learning and memory. Meanwhile, it has been reported that the relative timing between pre- and post-synaptic spiking determines the direction and extent of synaptic changes in a critical temporal window, a process known as spike timing-dependent plasticity (STDP). Positive timing where excitatory postsynaptic potential (EPSP) precedes the postsynaptic action potential induces long-term potentiation (LTP) while negative timing where EPSP follows the action potential induces long-term depression (LTD). To investigate the influence of muscarinic activation by cholinergic inputs on synaptic plasticity, STDP-inducing stimuli were applied during the muscarinic induction of a slow EPSP followed by repetitive stimulation in the stratum oriens. As a result, LTP was facilitated and LTD was abolished by the muscarinic activation. Furthermore, interestingly, LTP was also facilitated and LTD was switched to LTP with an increase in ACh concentration following application of the cholinesterase inhibitor eserine. These results indicate that the orientation of plasticity was shifted for potentiation by muscarinic activation. On the other hand, the application of excess ACh concentration completely suppressed STDP, LTP and LTD. In addition, STDP was suppressed in the presence of atropine, a muscarinic ACh receptor antagonist. Taken together, the findings suggest that synaptic plasticity modulation depends on the amount of cholinergic inputs. The modulation of synaptic plasticity by muscarinic activation might be an important stage in the integration of top-down and bottom-up information in hippocampal CA1 neurons.},
  langid = {english},
  keywords = {acetylcholine,hippocampus,muscarinic receptor,slow EPSP,STDP},
  file = {/Users/daniekru/Zotero/storage/4JVPC6YW/Sugisaki et al. - 2011 - Cholinergic modulation on spike timing-dependent p.pdf;/Users/daniekru/Zotero/storage/73QZZFK6/S0306452211007743.html}
}

@misc{sunLearningProducesHippocampal2023,
  title = {Learning Produces a Hippocampal Cognitive Map in the Form of an Orthogonalized State Machine},
  author = {Sun, Weinan and Winnubst, Johan and Natrajan, Maanasa and Lai, Chongxi and Kajikawa, Koichiro and Michaelos, Michalis and Gattoni, Rachel and Fitzgerald, James E. and Spruston, Nelson},
  year = {2023},
  month = aug,
  primaryclass = {New Results},
  pages = {2023.08.03.551900},
  publisher = {bioRxiv},
  doi = {10.1101/2023.08.03.551900},
  urldate = {2023-08-23},
  abstract = {Cognitive maps confer animals with flexible intelligence by representing spatial, temporal, and abstract relationships that can be used to shape thought, planning, and behavior. Cognitive maps have been observed in the hippocampus, but their algorithmic form and the processes by which they are learned remain obscure. Here, we employed large-scale, longitudinal two-photon calcium imaging to record activity from thousands of neurons in the CA1 region of the hippocampus while mice learned to efficiently collect rewards from two subtly different versions of linear tracks in virtual reality. The results provide a detailed view of the formation of a cognitive map in the hippocampus. Throughout learning, both the animal behavior and hippocampal neural activity progressed through multiple intermediate stages, gradually revealing improved task understanding and behavioral efficiency. The learning process led to progressive decorrelations in initially similar hippocampal neural activity within and across tracks, ultimately resulting in orthogonalized representations resembling a state machine capturing the inherent structure of the task. We show that a Hidden Markov Model (HMM) and a biologically plausible recurrent neural network trained using Hebbian learning can both capture core aspects of the learning dynamics and the orthogonalized representational structure in neural activity. In contrast, we show that gradient-based learning of sequence models such as Long Short-Term Memory networks (LSTMs) and Transformers do not naturally produce such representations. We further demonstrate that mice exhibited adaptive behavior in novel task settings, with neural activity reflecting flexible deployment of the state machine. These findings shed light on the mathematical form of cognitive maps, the learning rules that sculpt them, and the algorithms that promote adaptive behavior in animals. The work thus charts a course toward a deeper understanding of biological intelligence and offers insights toward developing more robust learning algorithms in artificial intelligence.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/M7Y2TT3F/Sun et al. - 2023 - Learning produces a hippocampal cognitive map in t.pdf}
}

@article{sunLearningProducesOrthogonalized2025,
  title = {Learning Produces an Orthogonalized State Machine in the Hippocampus},
  author = {Sun, Weinan and Winnubst, Johan and Natrajan, Maanasa and Lai, Chongxi and Kajikawa, Koichiro and Bast, Arco and Michaelos, Michalis and Gattoni, Rachel and Stringer, Carsen and Flickinger, Daniel and Fitzgerald, James E. and Spruston, Nelson},
  year = {2025},
  month = feb,
  journal = {Nature},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-08548-w},
  urldate = {2025-02-17},
  abstract = {Cognitive maps confer animals with flexible intelligence by representing spatial, temporal and abstract relationships that can be used to shape thought, planning and behaviour. Cognitive maps have been observed in the hippocampus1, but their algorithmic form and learning mechanisms remain obscure. Here we used large-scale, longitudinal two-photon calcium imaging to record activity from thousands of neurons in the CA1 region of the hippocampus while mice learned to efficiently collect rewards from two subtly different linear tracks in virtual reality. Throughout learning, both animal behaviour and hippocampal neural activity progressed through multiple stages, gradually revealing improved task representation that mirrored improved behavioural efficiency. The learning process involved progressive decorrelations in initially similar hippocampal neural activity within and across tracks, ultimately resulting in orthogonalized representations resembling a state machine capturing the inherent structure of the task. This decorrelation process was driven by individual neurons acquiring task-state-specific responses (that is, `state cells'). Although various standard artificial neural networks did not naturally capture these dynamics, the clone-structured causal graph, a hidden Markov model variant, uniquely reproduced both the final orthogonalized states and the learning trajectory seen in animals. The observed cellular and population dynamics constrain the mechanisms underlying cognitive map formation in the hippocampus, pointing to hidden state inference as a fundamental computational principle, with implications for both biological and artificial intelligence.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Learning algorithms,Learning and memory},
  file = {/Users/daniekru/Zotero/storage/V5EUPYIT/Sun et al. - 2025 - Learning produces an orthogonalized state machine in the hippocampus.pdf}
}

@article{sunOpposingComplementaryTopographic2018,
  title = {Opposing and {{Complementary Topographic Connectivity Gradients Revealed}} by {{Quantitative Analysis}} of {{Canonical}} and {{Noncanonical Hippocampal CA1 Inputs}}},
  author = {Sun, Yanjun and Nitz, Douglas A. and Holmes, Todd C. and Xu, Xiangmin},
  year = {2018},
  month = jan,
  journal = {eNeuro},
  volume = {5},
  number = {1},
  pages = {ENEURO.0322-17.2018},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0322-17.2018},
  urldate = {2023-03-13},
  abstract = {Physiological studies suggest spatial representation gradients along the CA1 proximodistal axis. To determine the underlying anatomical basis, we quantitatively mapped canonical and noncanonical inputs to excitatory neurons in dorsal hippocampal CA1 along the proximal-distal axis in mice of both sexes using monosynaptic rabies tracing. Our quantitative analyses show comparable strength of subiculum complex and entorhinal cortex (EC) inputs to CA1, significant inputs from presubiculum and parasubiculum to CA1, and a threefold stronger input to proximal versus distal CA1 from CA3. Noncanonical subicular complex inputs exhibit opposing topographic connectivity gradients whereby the subiculum-CA1 input strength systematically increases but the presubiculum-CA1 input strength decreases along the proximal-distal axis. The subiculum input strength cotracks that of the lateral EC, known to be less spatially selective than the medial EC. The functional significance of this organization is verified physiologically for subiculum-to-CA1 inputs. These results reveal a novel anatomical framework by which to determine the circuit bases for CA1 representations.},
  pmcid = {PMC5790753},
  pmid = {29387780},
  file = {/Users/daniekru/Zotero/storage/X3L5PCGV/Sun et al. - 2018 - Opposing and Complementary Topographic Connectivit.pdf}
}

@article{sunVerbalCreativityCorrelates2019,
  title = {Verbal {{Creativity Correlates}} with the {{Temporal Variability}} of {{Brain Networks During}} the {{Resting State}}},
  author = {Sun, Jiangzhou and Liu, Zhaowen and Rolls, Edmund T. and Chen, Qunlin and Yao, Ye and Yang, Wenjing and Wei, Dongtao and Zhang, Qinglin and Zhang, Jie and Feng, Jianfeng and Qiu, Jiang},
  year = {2019},
  month = mar,
  journal = {Cerebral Cortex},
  volume = {29},
  number = {3},
  pages = {1047--1058},
  publisher = {Oxford Academic},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhy010},
  urldate = {2020-05-06},
  abstract = {Abstract.  Creativity is the ability to see the world in new ways. Creative individuals exhibit the ability to switch between different modes of thinking and sh},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/XESE4U7J/Sun et al. - 2019 - Verbal Creativity Correlates with the Temporal Var.pdf;/Users/daniekru/Zotero/storage/E6YPF3Q2/4836784.html}
}

@article{suttonReinforcementLearningProblem,
  title = {The {{Reinforcement Learning Problem}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FHVHMCSC/Sutton and Barto - The Reinforcement Learning Problem.pdf}
}

@incollection{suttonReinforcementLearningProblem1998,
  title = {The {{Reinforcement Learning Problem}}},
  booktitle = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {1998},
  pages = {51--85},
  publisher = {MIT Press},
  urldate = {2024-05-07},
  abstract = {This chapter contains sections titled: The Agent-Environment Interface, Goals and Rewards, Returns, Unified Notation for Episodic and Continuing Tasks, The Markov Property, Markov Decision Processes, Value Functions, Optimal Value Functions, Optimality and Approximation, Summary, Bibliographical and Historical Remarks},
  isbn = {978-0-262-25705-3},
  file = {/Users/daniekru/Zotero/storage/IWF6EIEU/6282968.html}
}

@article{swanBindingPoolModel2014,
  title = {The Binding Pool: {{A}} Model of Shared Neural Resources for Distinct Items in Visual Working Memory},
  shorttitle = {The Binding Pool},
  author = {Swan, Garrett and Wyble, Brad},
  year = {2014},
  month = oct,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {76},
  number = {7},
  pages = {2136--2157},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-014-0633-3},
  urldate = {2020-05-27},
  abstract = {Visual working memory (VWM) refers to the ability to encode, store, and retrieve visual information. The two prevailing theories that describe VWM assume that information is stored either in discrete slots or within a shared pool of resources. However, there is not yet a good understanding of the neural mechanisms that would underlie such theories. To address this gap, we provide a computationally realized neural account that uses a pool of shared neurons to store information about one or more distinct stimuli. The binding pool model is a neural network that is essentially a hybrid of the slot and resource theories. It describes how information can be stored and retrieved from a pool of shared resources using a type/token architecture (Bowman \& Wyble in Psychological Review 114(1), 38--70, 2007; Kanwisher in Cognition 27, 117--143, 1987; Mozer in Journal of Experimental Psychology: Human Perception and Performance 15(2), 287--303, 1989). The model can store multiple distinct objects, each containing binding links to one or more features. The binding links are stored in a pool of shared resources and, thus, produce mutual interference as memory load increases. Given a cue, the model retrieves a specific object and then reconstructs other features bound to that object, along with a confidence metric. The model can simulate data from continuous report and change detection paradigms and generates testable predictions about the interaction of report accuracy, confidence, and stimulus similarity. The testing of such predictions will help to identify the boundaries of shared resource theories, thereby providing insight into the roles of ensembles and context in explaining our ability to remember visual information.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/E3D4XIR2/Swan and Wyble - 2014 - The binding pool A model of shared neural resourc.pdf}
}

@article{tangGeometricTransformationCognitive2023,
  title = {Geometric Transformation of Cognitive Maps for Generalization across Hippocampal-Prefrontal Circuits},
  author = {Tang, Wenbo and Shin, Justin D. and Jadhav, Shantanu P.},
  year = {2023},
  month = mar,
  journal = {Cell Reports},
  volume = {42},
  number = {3},
  publisher = {Elsevier},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2023.112246},
  urldate = {2024-03-24},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}p{$>$}The ability to abstract information to guide decisions during navigation across changing environments is essential for adaptation and requires the integrity of the hippocampal-prefrontal circuitry. The hippocampus encodes navigational information in a cognitive map, but it remains unclear how cognitive maps are transformed across hippocampal-prefrontal circuits to support abstraction and generalization. Here, we simultaneously record hippocampal-prefrontal ensembles as rats generalize navigational rules across distinct environments. We find that, whereas hippocampal representational maps maintain specificity of separate environments, prefrontal maps generalize across environments. Furthermore, while both maps are structured within a neural manifold of population activity, they have distinct representational geometries. Prefrontal geometry enables abstraction of rule-informative variables, a representational format that generalizes to novel conditions of existing variable classes. Hippocampal geometry lacks such abstraction. Together, these findings elucidate how cognitive maps are structured into distinct geometric representations to support abstraction and generalization while maintaining memory specificity.{$<$}/p{$>$}},
  langid = {english},
  pmid = {36924498},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/L57UF725/Tang et al. - 2023 - Geometric transformation of cognitive maps for gen.pdf}
}

@misc{TargetSelectivitySeptal,
  title = {Target Selectivity of Septal Cholinergic Neurons in the Medial and Lateral Entorhinal Cortex {\textbar} {{PNAS}}},
  urldate = {2025-02-22},
  howpublished = {https://www.pnas.org/doi/abs/10.1073/pnas.1716531115},
  file = {/Users/daniekru/Zotero/storage/DUJUQP5Q/pnas.html}
}

@article{tejomurtulaInverseKinematicsRobotics1999,
  title = {Inverse Kinematics in Robotics Using Neural Networks},
  author = {Tejomurtula, Sreenivas and Kak, Subhash},
  year = {1999},
  month = jan,
  journal = {Information Sciences},
  volume = {116},
  number = {2-4},
  pages = {147--164},
  issn = {00200255},
  doi = {10.1016/S0020-0255(98)10098-1},
  urldate = {2020-05-24},
  langid = {english}
}

@article{thompsonLikelihoodThatOne1933,
  title = {On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples.},
  author = {Thompson, William R.},
  year = {1933},
  file = {/Users/daniekru/Zotero/storage/SWQQZ4P3/25-3-4-285.pdf}
}

@misc{ThusTheseDistinct,
  title = {Thus, through These Distinct Sets of Mechanisms, Acetylcholine and Noradrenaline Facilitate the Formation of Neuronal Ensembles in {{CA3}} That Encode Salient Episodic Memories in the Hippocampus but Acetylcholine Selectively Enhances the Density of Memory Storage. - {{Consensus}}},
  urldate = {2023-04-28},
  abstract = {Consensus used AI to extract this finding from 'Separable actions of acetylcholine and noradrenaline on neuronal ensemble formation in hippocampal CA3 circuits'},
  howpublished = {https://consensus.app/details/thus-sets-mechanisms-facilitate-formation-ensembles-prince/aa2bfbeb626a5f7aa7dcf8f88caa8beb/},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/8SC5M3V3/aa2bfbeb626a5f7aa7dcf8f88caa8beb.html}
}

@book{tiddiaModularTheoreticalFramework2023,
  title = {A Modular Theoretical Framework for Learning through Structural Plasticity},
  author = {Tiddia, Gianmarco and Sergi, Luca and Golosio, Bruno},
  year = {2023},
  month = jul,
  doi = {10.48550/arXiv.2307.11735},
  abstract = {It is known that, during learning, modifications in synaptic transmission and, eventually, structural changes of the connectivity take place in our brain. This can be achieved through a mechanism known as structural plasticity. In this work, starting from a simple phenomenological model, we exploit a mean-field approach to develop a modular theoretical framework of learning through this kind of plasticity, capable of taking into account several features of the connectivity and pattern of activity of biological neural networks, including probability distributions of neuron firing rates, selectivity of the responses of single neurons to multiple stimuli, probabilistic connection rules and noisy stimuli. More importantly, it describes the effects of consolidation, pruning and reorganization of synaptic connections. This framework will be used to compute the values of some relevant quantities used to characterize the learning and memory capabilities of the neuronal network in a training and validation procedure as the number of training patterns and other model parameters vary. The results will then be compared with those obtained through simulations with firing-rate-based neuronal network models.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/BKYQXW9Q/Tiddia et al. - 2023 - A modular theoretical framework for learning throu.pdf}
}

@article{toblerAdaptiveCodingReward2005,
  title = {Adaptive {{Coding}} of {{Reward Value}} by {{Dopamine Neurons}}},
  author = {Tobler, Philippe N. and Fiorillo, Christopher D. and Schultz, Wolfram},
  year = {2005},
  month = mar,
  journal = {Science},
  volume = {307},
  number = {5715},
  pages = {1642--1645},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1105370},
  urldate = {2024-05-02},
  abstract = {It is important for animals to estimate the value of rewards as accurately as possible. Because the number of potential reward values is very large, it is necessary that the brain's limited resources be allocated so as to discriminate better among more likely reward outcomes at the expense of less likely outcomes. We found that midbrain dopamine neurons rapidly adapted to the information provided by reward-predicting stimuli. Responses shifted relative to the expected reward value, and the gain adjusted to the variance of reward value. In this way, dopamine neurons maintained their reward sensitivity over a large range of reward values.},
  file = {/Users/daniekru/Zotero/storage/CBJ4X3LU/Tobler et al. - 2005 - Adaptive Coding of Reward Value by Dopamine Neuron.pdf}
}

@incollection{tokicAdaptiveEGreedyExploration2010,
  title = {Adaptive {$\varepsilon$}-{{Greedy Exploration}} in {{Reinforcement Learning Based}} on {{Value Differences}}},
  booktitle = {{{KI}} 2010: {{Advances}} in {{Artificial Intelligence}}},
  author = {Tokic, Michel},
  editor = {Dillmann, R{\"u}diger and Beyerer, J{\"u}rgen and Hanebeck, Uwe D. and Schultz, Tanja},
  year = {2010},
  volume = {6359},
  pages = {203--210},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-16111-7_23},
  urldate = {2024-03-23},
  abstract = {This paper presents ``Value-Difference Based Exploration'' (VDBE), a method for balancing the exploration/exploitation dilemma inherent to reinforcement learning. The proposed method adapts the exploration parameter of {$\varepsilon$}-greedy in dependence of the temporal-difference error observed from value-function backups, which is considered as a measure of the agent's uncertainty about the environment. VDBE is evaluated on a multi-armed bandit task, which allows for insight into the behavior of the method. Preliminary results indicate that VDBE seems to be more parameter robust than commonly used ad hoc approaches such as {$\varepsilon$}-greedy or softmax.},
  isbn = {978-3-642-16110-0 978-3-642-16111-7},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/CR7BH65S/Tokic - 2010 - Adaptive ε-Greedy Exploration in Reinforcement Lea.pdf}
}

@incollection{tokicValueDifferenceBasedExploration2011,
  title = {Value-{{Difference Based Exploration}}: {{Adaptive Control}} between {{Epsilon-Greedy}} and {{Softmax}}},
  shorttitle = {Value-{{Difference Based Exploration}}},
  booktitle = {{{KI}} 2011: {{Advances}} in {{Artificial Intelligence}}},
  author = {Tokic, Michel and Palm, G{\"u}nther},
  editor = {Bach, Joscha and Edelkamp, Stefan},
  year = {2011},
  volume = {7006},
  pages = {335--346},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-24455-1_33},
  urldate = {2024-03-23},
  abstract = {This paper proposes ``Value-Difference Based Exploration combined with Softmax action selection'' (VDBE-Softmax) as an adaptive exploration/exploitation policy for temporal-difference learning. The advantage of the proposed approach is that exploration actions are only selected in situations when the knowledge about the environment is uncertain, which is indicated by fluctuating values during learning. The method is evaluated in experiments having deterministic rewards and a mixture of both deterministic and stochastic rewards. The results show that a VDBE-Softmax policy can outperform {$\varepsilon$}-greedy, Softmax and VDBE policies in combination with on- and off-policy learning algorithms such as Q-learning and Sarsa. Furthermore, it is also shown that VDBE-Softmax is more reliable in case of value-function oscillations.},
  isbn = {978-3-642-24454-4 978-3-642-24455-1},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZM5YNNWB/Tokic and Palm - 2011 - Value-Difference Based Exploration Adaptive Contr.pdf}
}

@article{tolmanCOGNITIVEMAPSRATS1948,
  title = {{{COGNITIVE MAPS IN RATS AND MEN}}},
  author = {Tolman, Edward C},
  year = {1948},
  month = jul,
  journal = {Psychological Review},
  volume = {55},
  number = {4},
  langid = {english},
  annotation = {US: American Psychological Association},
  file = {/Users/daniekru/Zotero/storage/JT9NNG8M/Tolman - COGNITIVE MAPS IN RATS AND MEN.pdf}
}

@article{tremblayRelativeRewardPreference1999,
  title = {Relative Reward Preference in Primate Orbitofrontal Cortex},
  author = {Tremblay, L{\'e}on and Schultz, Wolfram},
  year = {1999},
  month = apr,
  journal = {Nature},
  volume = {398},
  number = {6729},
  pages = {704--708},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/19525},
  urldate = {2024-05-14},
  abstract = {The orbital part of prefrontal cortex appears to be crucially involved in the motivational control of goal-directed behaviour1,2. Patients with lesions of orbitofrontal cortex show impairments in making decisions about the expected outcome of actions3. Monkeys with orbitofrontal lesions respond abnormally to changes in reward expectations4,5 and show altered reward preferences6. As rewards constitute basic goals of behaviour7, we investigated here how neurons in the orbitofrontal cortex of monkeys process information about liquid and food rewards in a typical frontal task, spatial delayed responding8. The activity of orbitofrontal neurons increases in response to reward-predicting signals, during the expectation of rewards, and after the receipt of rewards. Neurons discriminate between different rewards, mainly irrespective of the spatial and visual features of reward-predicting stimuli and behavioural reactions. Most reward discriminations reflect the animals' relative preference among the available rewards, as expressed by their choice behaviour, rather than physical reward properties. Thus, neurons in the orbitofrontal cortex appear to process the motivational value of rewarding outcomes of voluntary action.},
  copyright = {1999 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/daniekru/Zotero/storage/PJ7CWN2D/Tremblay and Schultz - 1999 - Relative reward preference in primate orbitofronta.pdf}
}

@article{tsetsenisActivationLocusCoeruleus2022,
  title = {Activation of a {{Locus Coeruleus}} to {{Dorsal Hippocampus Noradrenergic Circuit Facilitates Associative Learning}}},
  author = {Tsetsenis, Theodoros and Badyna, Julia K. and Li, Rebecca and Dani, John A.},
  year = {2022},
  month = apr,
  journal = {Frontiers in Cellular Neuroscience},
  volume = {16},
  pages = {887679},
  issn = {1662-5102},
  doi = {10.3389/fncel.2022.887679},
  urldate = {2023-05-02},
  abstract = {Processing of contextual information during a new episodic event is crucial for learning and memory. Neuromodulation in the hippocampus and prefrontal cortex plays an important role in the formation of associations between environmental cues and an aversive experience. Noradrenergic neurons in the locus coeruleus send dense projections to both regions, but their contribution to contextual associative learning has not been established. Here, we utilize selective optogenetic and pharmacological manipulations to control noradrenergic transmission in the hippocampus during the encoding of a contextual fear memory. We find that boosting noradrenergic terminal release in the dorsal CA1 enhances the acquisition of contextual associative learning and that this effect requires local activation of {$\beta$}-adrenenergic receptors. Moreover, we show that increasing norepinephrine release can ameliorate contextual fear learning impairments caused by dopaminergic dysregulation in the hippocampus. Our data suggest that increasing of hippocampal noradrenergic activity can have important implications in the treatment of cognitive disorders that involve problems in contextual processing.},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/NVAUQ7R2/Tsetsenis et al. - 2022 - Activation of a Locus Coeruleus to Dorsal Hippocam.pdf}
}

@article{tsetsenisDopaminergicRegulationHippocampal2023,
  title = {Dopaminergic Regulation of Hippocampal Plasticity, Learning, and Memory},
  author = {Tsetsenis, Theodoros and Broussard, John I. and Dani, John A.},
  year = {2023},
  month = jan,
  journal = {Frontiers in Behavioral Neuroscience},
  volume = {16},
  pages = {1092420},
  issn = {1662-5153},
  doi = {10.3389/fnbeh.2022.1092420},
  urldate = {2023-05-02},
  abstract = {The hippocampus is responsible for encoding behavioral episodes into short-term and long-term memory. The circuits that mediate these processes are subject to neuromodulation, which involves regulation of synaptic plasticity and local neuronal excitability. In this review, we present evidence to demonstrate the influence of dopaminergic neuromodulation on hippocampus-dependent memory, and we address the controversy surrounding the source of dopamine innervation. First, we summarize historical and recent retrograde and anterograde anatomical tracing studies of direct dopaminergic projections from the ventral tegmental area and discuss dopamine release from the adrenergic               locus coeruleus               . Then, we present evidence of dopaminergic modulation of synaptic plasticity in the hippocampus. Plasticity mechanisms are examined in brain slices and in recordings from               in vivo               neuronal populations in freely moving rodents. Finally, we review pharmacological, genetic, and circuitry research that demonstrates the importance of dopamine release for learning and memory tasks while dissociating anatomically distinct populations of direct dopaminergic inputs.},
  file = {/Users/daniekru/Zotero/storage/2Y27VMNN/Tsetsenis et al. - 2023 - Dopaminergic regulation of hippocampal plasticity,.pdf}
}

@article{turnerSimplicityBiasMultiTask,
  title = {The {{Simplicity Bias}} in {{Multi-Task RNNs}}: {{Shared Attractors}}, {{Reuse}} of {{Dynamics}}, and {{Geometric Representation}}},
  author = {Turner, Elia and Barak, Omri},
  abstract = {How does a single interconnected neural population perform multiple tasks, each with its own dynamical requirements? The relation between task requirements and neural dynamics in Recurrent Neural Networks (RNNs) has been investigated for single tasks. The forces shaping joint dynamics of multiple tasks, however, are largely unexplored. In this work, we first construct a systematic framework to study multiple tasks in RNNs, minimizing interference from input and output correlations with the hidden representation. This allows us to reveal how RNNs tend to share attractors and reuse dynamics, a tendency we define as the "simplicity bias". We find that RNNs develop attractors sequentially during training, preferentially reusing existing dynamics and opting for simple solutions when possible. This sequenced emergence and preferential reuse encapsulate the simplicity bias. Through concrete examples, we demonstrate that new attractors primarily emerge due to task demands or architectural constraints, illustrating a balance between simplicity bias and external factors. We examine the geometry of joint representations within a single attractor, by constructing a family of tasks from a set of functions. We show that the steepness of the associated functions controls their alignment within the attractor. This arrangement again highlights the simplicity bias, as points with similar input spacings undergo comparable transformations to reach the shared attractor. Our findings propose compelling applications. The geometry of shared attractors might allow us to infer the nature of unknown tasks. Furthermore, the simplicity bias implies that without specific incentives, modularity in RNNs may not spontaneously emerge, providing insights into the conditions required for network specialization.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/QTGDZXFY/Turner and Barak - The Simplicity Bias in Multi-Task RNNs Shared Att.pdf}
}

@misc{UntitledIpynbJupyterLab,
  title = {Untitled.Ipynb (2) - {{JupyterLab}}},
  urldate = {2023-06-23},
  howpublished = {http://localhost:8888/lab/tree/Untitled.ipynb},
  file = {/Users/daniekru/Zotero/storage/MFSDQ4H8/Untitled.html}
}

@misc{varleyInformationTheoryComplex2023,
  title = {Information {{Theory}} for {{Complex Systems Scientists}}},
  author = {Varley, Thomas F.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.12482},
  eprint = {2304.12482},
  primaryclass = {physics, q-bio, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.12482},
  urldate = {2023-04-26},
  abstract = {In the 21st century, many of the crucial scientific and technical issues facing humanity can be understood as problems associated with understanding, modelling, and ultimately controlling complex systems: systems comprised of a large number of non-trivially interacting components whose collective behaviour can be difficult to predict. Information theory, a branch of mathematics historically associated with questions about encoding and decoding messages, has emerged as something of a lingua franca for those studying complex systems, far exceeding its original narrow domain of communication systems engineering. In the context of complexity science, information theory provides a set of tools which allow researchers to uncover the statistical and effective dependencies between interacting components; relationships between systems and their environment; mereological whole-part relationships; and is sensitive to non-linearities missed by commonly parametric statistical models. In this review, we aim to provide an accessible introduction to the core of modern information theory, aimed specifically at aspiring (and established) complex systems scientists. This includes standard measures, such as Shannon entropy, relative entropy, and mutual information, before building to more advanced topics, including: information dynamics, measures of statistical complexity, information decomposition, and effective network inference. In addition to detailing the formal definitions, in this review we make an effort to discuss how information theory can be interpreted and develop the intuition behind abstract concepts like "entropy," in the hope that this will enable interested readings to understand what information is, and how it is used, at a more fundamental level.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Physics - Data Analysis Statistics and Probability,Quantitative Biology - Quantitative Methods,Statistics - Other Statistics,to study},
  file = {/Users/daniekru/Zotero/storage/8ZM495U5/Varley - 2023 - Information Theory for Complex Systems Scientists.pdf;/Users/daniekru/Zotero/storage/CX2NEQXF/2304.html}
}

@article{vecovenIntroducingNeuromodulationDeep2020,
  title = {Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive Behaviours},
  author = {Vecoven, Nicolas and Ernst, Damien and Wehenkel, Antoine and Drion, Guillaume},
  year = {2020},
  month = jan,
  journal = {PLOS ONE},
  volume = {15},
  number = {1},
  pages = {e0227922},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0227922},
  urldate = {2022-09-08},
  abstract = {Animals excel at adapting their intentions, attention, and actions to the environment, making them remarkably efficient at interacting with a rich, unpredictable and ever-changing external world, a property that intelligent machines currently lack. Such an adaptation property relies heavily on cellular neuromodulation, the biological mechanism that dynamically controls intrinsic properties of neurons and their response to external stimuli in a context-dependent manner. In this paper, we take inspiration from cellular neuromodulation to construct a new deep neural network architecture that is specifically designed to learn adaptive behaviours. The network adaptation capabilities are tested on navigation benchmarks in a meta-reinforcement learning context and compared with state-of-the-art approaches. Results show that neuromodulation is capable of adapting an agent to different tasks and that neuromodulation-based approaches provide a promising way of improving adaptation of artificial systems.},
  langid = {english},
  keywords = {Behavior,Convergent evolution,Machine learning,Machine learning algorithms,Neural networks,Neuromodulation,Neuronal tuning,Neurons},
  file = {/Users/daniekru/Zotero/storage/P3T4CEN5/Vecoven et al. - 2020 - Introducing neuromodulation in deep neural network.pdf;/Users/daniekru/Zotero/storage/JW9FQ72B/article.html}
}

@article{vogelsGatingMultipleSignals2009,
  title = {Gating Multiple Signals through Detailed Balance of Excitation and Inhibition in Spiking Networks},
  author = {Vogels, Tim P. and Abbott, L. F.},
  year = {2009},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {12},
  number = {4},
  pages = {483--491},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn.2276},
  urldate = {2024-05-13},
  abstract = {The balance of excitation and inhibition across large populations of spiking neurons has been suggested to be important. Here the authors model the effects of a more detailed balance between incoming excitation and local inhibition on the transmission of signals through a neural network.},
  copyright = {2009 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/TXJZRQDQ/Vogels and Abbott - 2009 - Gating multiple signals through detailed balance o.pdf}
}

@article{wagatsumaLocusCoeruleusInput2018,
  title = {Locus Coeruleus Input to Hippocampal {{CA3}} Drives Single-Trial Learning of a Novel Context},
  author = {Wagatsuma, Akiko and Okuyama, Teruhiro and Sun, Chen and Smith, Lillian M. and Abe, Kuniya and Tonegawa, Susumu},
  year = {2018},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {2},
  pages = {E310-E316},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1714082115},
  urldate = {2023-04-12},
  abstract = {The memory for a new episode is formed immediately upon experience and can last up to a lifetime. It has been shown that the hippocampal network plays a fundamental role in the rapid acquisition of a memory of a one-time experience, in which the novelty component of the experience promotes the prompt formation of the memory. However, it remains unclear which neural circuits convey the novelty signal to the hippocampus for the single-trial learning. Here, we show that during encoding neuromodulatory input from locus coeruleus (LC) to CA3, but not CA1 or to the dentate gyrus, is necessary to facilitate novel contextual learning. Silencing LC activity during exposure to a novel context reduced subsequent reactivation of the engram cell ensembles in CA3 neurons and in downstream CA1 upon reexposure to the same context. Calcium imaging of the cells reactivated in both novel and familiar contexts revealed that suppression of LC inputs at the time of encoding resulted in more variable place fields in CA3 neurons. These results suggest that neuromodulatory input from LC to CA3 is crucial for the formation of a persistent memory in the hippocampus.},
  file = {/Users/daniekru/Zotero/storage/HXURYC7U/Wagatsuma et al. - 2018 - Locus coeruleus input to hippocampal CA3 drives si.pdf}
}

@article{wagnerComparingPsychophysicalGeometric2008,
  title = {Comparing the Psychophysical and Geometric Characteristics of Spatial Perception and Cognitive Maps},
  author = {Wagner, Mark},
  year = {2008},
  journal = {Cognitive Studies: Bulletin of the Japanese Cognitive Science Society},
  volume = {15},
  number = {1},
  pages = {6--21},
  publisher = {Japanese Cognitive Science Society (JCSS)},
  address = {Japan},
  issn = {1881-5995},
  abstract = {Spatial perception and cognitive mapping are two areas of research that invite comparison since they both involve fundamental geometric concepts, and researchers have applied psychophysical methods to examine spatial judgments in both domains. The first part of article discusses research that assesses the degree to which spatial judgments conform to various geometric axioms. In general, both spatial perception and cognitive maps violate many of the most fundamental geometric axioms, including metric axioms, compactness, the Desarguesian postulate, and free mobility. Of the two, cognitive maps show the most striking violations of even the most basic geometric concepts, including symmetry, transitivity, and Riemannian manifoldness. The second and larger part of this article performs a meta-analysis on psychophysical judgments for size and distance, comparing judgments made in perception research to those found with cognitive maps. The most striking results are that cognitive maps produce much lower power function exponents and much lower coefficients of determination for size and distance estimation than perception does. The meta-analysis presented here also shows that a number of the same factors influence both types of distance estimates including judgment method, whether judgments occur inside or outside, and stimulus range. Cognitive mapping exponents also rise with acquisition time and decline with increasing environment size. Several multivariate analyses are performed to yield a set of general psychophysical equations to predict size and distance estimates. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Maps,Geometry,Judgment,Psychophysical Measurement,Spatial Perception},
  file = {/Users/daniekru/Zotero/storage/ISNE9RTI/2009-25124-001.html}
}

@article{waltonADAPTIVEDECISIONMAKING2007,
  title = {{{ADAPTIVE DECISION MAKING AND VALUE IN THE ANTERIOR CINGULATE CORTEX}}},
  author = {Walton, Mark E. and Croxson, Paula L. and Behrens, Timothy E. J. and Kennerley, Steven W. and Rushworth, Matthew F. S.},
  year = {2007},
  journal = {NeuroImage},
  volume = {36},
  number = {Suppl 2},
  pages = {T142-T154},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2007.03.029},
  urldate = {2024-11-27},
  abstract = {Choosing an appropriate response in an uncertain and varying world is central to adaptive behaviour. The frequent activation of the anterior cingulate cortex (ACC) in a diverse range of tasks has lead to intense interest in and debate over its role in the guidance and control of performance. Here, we consider how this issue can be informed by a series of studies considering the ACC's role in more naturalistic situations where there is no single certain correct response and the relationships between choices and their consequences vary. A neuroimaging study of response switching demonstrates that dorsal ACC is not simply concerned with self-generated responses or error monitoring in isolation, but is instead involved in evaluating the outcome of choices, positive or negative, that have been voluntarily chosen. By contrast, an interconnected part of the orbitofrontal cortex is shown to be more active when attending to consequences of actions instructed by the experimenter. This dissociation is explained with reference to the anatomy of these regions in humans as demonstrated by diffusion weighted imaging. Lesions to a corresponding ACC region in monkeys has no effect on animals' ability to detect or immediately correct errors when response contingencies reverse, but renders them unable to sustain appropriate behaviour due to an impairment in the ability to integrate over time their recent history of choices and outcomes. Taken together, this implies a prominent role for the ACC within a distributed network of regions that determine the dynamic value of actions and guide decision making appropriately.},
  pmcid = {PMC2954047},
  pmid = {17499161},
  file = {/Users/daniekru/Zotero/storage/H6CXCMHN/Walton et al. - 2007 - ADAPTIVE DECISION MAKING AND VALUE IN THE ANTERIOR CINGULATE CORTEX.pdf}
}

@misc{wangComputationalModelsStudy2024,
  title = {Computational {{Models}} to {{Study Language Processing}} in the {{Human Brain}}: {{A Survey}}},
  shorttitle = {Computational {{Models}} to {{Study Language Processing}} in the {{Human Brain}}},
  author = {Wang, Shaonan and Sun, Jingyuan and Zhang, Yunhao and Lin, Nan and Moens, Marie-Francine and Zong, Chengqing},
  year = {2024},
  month = mar,
  number = {arXiv:2403.13368},
  eprint = {2403.13368},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-06},
  abstract = {Despite differing from the human language processing mechanism in implementation and algorithms, current language models demonstrate remarkable human-like or surpassing language capabilities. Should computational language models be employed in studying the brain, and if so, when and how? To delve into this topic, this paper reviews efforts in using computational models for brain research, highlighting emerging trends. To ensure a fair comparison, the paper evaluates various computational models using consistent metrics on the same dataset. Our analysis reveals that no single model outperforms others on all datasets, underscoring the need for rich testing datasets and rigid experimental control to draw robust conclusions in studies involving computational models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/daniekru/Zotero/storage/3KX8ZCGN/Wang et al. - 2024 - Computational Models to Study Language Processing .pdf}
}

@misc{wangLearningReinforcementLearn2017,
  title = {Learning to Reinforcement Learn},
  author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  year = {2017},
  month = jan,
  number = {arXiv:1611.05763},
  eprint = {1611.05763},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.05763},
  urldate = {2024-03-22},
  abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/9K3MISS7/Wang et al. - 2017 - Learning to reinforcement learn.pdf;/Users/daniekru/Zotero/storage/LNNXZZL6/1611.html}
}

@article{wangMetalearningNaturalArtificial2021,
  title = {Meta-Learning in Natural and Artificial Intelligence},
  author = {Wang, Jane X},
  year = {2021},
  month = apr,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Computational Cognitive Neuroscience},
  volume = {38},
  pages = {90--95},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2021.01.002},
  urldate = {2024-12-09},
  abstract = {Meta-learning, or learning to learn, has gained renewed interest in recent years within the artificial intelligence community. However, meta-learning is incredibly prevalent within nature, has deep roots in cognitive science and psychology, and is currently studied in various forms within neuroscience. The aim of this review is to recast previous lines of research in the study of biological intelligence within the lens of meta-learning, placing these works into a common framework. More recent points of interaction between AI and neuroscience will be discussed, as well as interesting new directions that arise under this perspective.},
  file = {/Users/daniekru/Zotero/storage/WL9WEEJ4/Wang - 2021 - Meta-learning in natural and artificial intelligence.pdf;/Users/daniekru/Zotero/storage/BAWSBHRE/S2352154621000024.html}
}

@article{wangProgressChallengesNeuroscience2020,
  title = {Progress and {{Challenges}} of {{Neuroscience}} and {{Brain-inspired Artificial Intelligence}}},
  author = {Wang, Lidong and Alexander, Cheryl Ann},
  year = {2020},
  month = jan,
  journal = {Neuroscience International},
  volume = {11},
  number = {1},
  pages = {13--21},
  issn = {2524-2237},
  doi = {10.3844/amjnsp.2020.1.9},
  urldate = {2025-01-16},
  abstract = {Neuroscience and brain-inspired artificial intelligence are significant research areas. Many countries have launched brain-related projects in which neuroscience and brain-inspired artificial intelligence are major targeted areas to increase national interests and enhance their strength in key areas such as military and homeland security in the competitive global world. Methods, emerging technologies, and progress in neuroscience and brain-inspired artificial intelligence are introduced in this paper that specifically include brain-inspired computing, brain association graph, brain networks, the connectome, brain reconstruction, imaging technologies used for the brain, chips and devices inspired by the human brain, brain-computer interface or brain-machine interfaces, cyborg, neuro-robotics, and quantum robotics. Challenges in some of the topics are also presented and discussed.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/BAA9Z3E6/Wang and Alexander - 2020 - Progress and Challenges of Neuroscience and Brain-inspired Artificial Intelligence.pdf}
}

@article{warrenNonEuclideanNavigation2019,
  title = {Non-{{Euclidean}} Navigation},
  author = {Warren, William H.},
  editor = {{el Jundi}, Basil and Kelber, Almut and Webb, Barbara},
  year = {2019},
  month = feb,
  journal = {Journal of Experimental Biology},
  volume = {222},
  number = {Suppl\_1},
  pages = {jeb187971},
  issn = {0022-0949},
  doi = {10.1242/jeb.187971},
  urldate = {2025-02-15},
  abstract = {A basic set of navigation strategies supports navigational tasks ranging from homing to novel detours and shortcuts. To perform these last two tasks, it is generally thought that humans, mammals and perhaps some insects possess Euclidean cognitive maps, constructed on the basis of input from the path integration system. In this article, I review the rationale and behavioral evidence for this metric cognitive map hypothesis, and find it unpersuasive: in practice, there is little evidence for truly novel shortcuts in animals, and human performance is highly unreliable and biased by environmental features. I develop the alternative hypothesis that spatial knowledge is better characterized as a labeled graph: a network of paths between places augmented with local metric information. What distinguishes such a cognitive graph from a metric cognitive map is that this local information is not embedded in a global coordinate system, so spatial knowledge is often geometrically inconsistent. Human path integration appears to be better suited to piecewise measurements of path lengths and turn angles than to building a consistent map. In a series of experiments in immersive virtual reality, we tested human navigation in non-Euclidean environments and found that shortcuts manifest large violations of the metric postulates. The results are contrary to the Euclidean map hypothesis and support the cognitive graph hypothesis. Apparently Euclidean behavior, such as taking novel detours and approximate shortcuts, can be explained by the adaptive use of non-Euclidean strategies.},
  file = {/Users/daniekru/Zotero/storage/K7DGF5WC/Warren - 2019 - Non-Euclidean navigation.pdf;/Users/daniekru/Zotero/storage/QE5GR9ZN/Non-Euclidean-navigation.html}
}

@misc{WaybackMachine2017,
  title = {Wayback {{Machine}}},
  year = {2017},
  month = jul,
  urldate = {2025-03-20},
  howpublished = {https://web.archive.org/web/20170718230207/http://bioinfo.ict.ac.cn/{\textasciitilde}dbu/AlgorithmCourses/Lectures/Prim1957.pdf},
  file = {/Users/daniekru/Zotero/storage/N7X3Y3IF/2017 - Wayback Machine.pdf}
}

@article{wehnerVisualNavigationInsects1996,
  title = {Visual {{Navigation}} in {{Insects}}: {{Coupling}} of {{Egocentric}} and {{Geocentric Information}}},
  shorttitle = {Visual {{Navigation}} in {{Insects}}},
  author = {Wehner, R{\"u}diger and Michel, Barbara and Antonsen, Per},
  year = {1996},
  month = jan,
  journal = {Journal of Experimental Biology},
  volume = {199},
  number = {1},
  pages = {129--140},
  issn = {0022-0949},
  doi = {10.1242/jeb.199.1.129},
  urldate = {2025-02-23},
  abstract = {Social hymenopterans such as bees and ants are central-place foragers; they regularly depart from and return to fixed positions in their environment. In returning to the starting point of their foraging excursion or to any other point, they could resort to two fundamentally different ways of navigation by using either egocentric or geocentric systems of reference. In the first case, they would rely on information continuously collected en route (path integration, dead reckoning), i.e. integrate all angles steered and all distances covered into a mean home vector. In the second case, they are expected, at least by some authors, to use a map-based system of navigation, i.e. to obtain positional information by virtue of the spatial position they occupy within a larger environmental framework.In bees and ants, path integration employing a skylight compass is the predominant mechanism of navigation, but geocentred landmark-based information is used as well. This information is obtained while the animal is dead-reckoning and, hence, added to the vector course. For example, the image of the horizon skyline surrounding the nest entrance is retinotopically stored while the animal approaches the goal along its vector course. As shown in desert ants (genus Cataglyphis), there is neither interocular nor intraocular transfer of landmark information. Furthermore, this retinotopically fixed, and hence egocentred, neural snapshot is linked to an external (geocentred) system of reference.In this way, geocentred information might more and more complement and potentially even supersede the egocentred information provided by the path-integration system. In competition experiments, however, Cataglyphis never frees itself of its homeward-bound vector -- its safety-line, so to speak -- by which it is always linked to home. Vector information can also be transferred to a longer-lasting (higher-order) memory. There is no need to invoke the concept of the mental analogue of a topographic map -- a metric map -- assembled by the insect navigator. The flexible use of vectors, snapshots and landmark-based routes suffices to interpret the insect's behaviour.The cognitive-map approach in particular, and the representational paradigm in general, are discussed.},
  file = {/Users/daniekru/Zotero/storage/U75JNLGD/Wehner et al. - 1996 - Visual Navigation in Insects Coupling of Egocentric and Geocentric Information.pdf;/Users/daniekru/Zotero/storage/WX6QHKRC/Visual-Navigation-in-Insects-Coupling-of.html}
}

@article{weisbergCognitiveMapsPeople2018,
  title = {Cognitive {{Maps}}: {{Some People Make Them}}, {{Some People Struggle}}},
  shorttitle = {Cognitive {{Maps}}},
  author = {Weisberg, Steven M. and Newcombe, Nora S.},
  year = {2018},
  month = aug,
  journal = {Current Directions in Psychological Science},
  volume = {27},
  number = {4},
  pages = {220--226},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1177/0963721417744521},
  urldate = {2025-02-15},
  abstract = {The proposal that humans can develop cognitive maps of their environment has a long and controversial history. We suggest an individual-differences approach to this question instead of a normative one. Specifically, there is evidence that some people derive flexible maplike representations from information acquired during navigation, whereas others store much less accurate information. Our research uses a virtual-reality paradigm in which two routes are learned and must be related to each other. It defines three groups: integrators, nonintegrators, and imprecise navigators. These groups show distinctive patterns of spatial skills and working memory, as well as personality. We contrast our approach with research challenging the cognitive-map hypothesis and offer directions for rapprochement between the two views.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/9P9H5QSG/Weisberg and Newcombe - 2018 - Cognitive Maps Some People Make Them, Some People Struggle.pdf}
}

@misc{weissThinkingTransformers2021,
  title = {Thinking {{Like Transformers}}},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  year = {2021},
  month = jul,
  number = {arXiv:2106.06981},
  eprint = {2106.06981},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.06981},
  urldate = {2023-01-14},
  abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,optional},
  file = {/Users/daniekru/Zotero/storage/B3Q8PIE7/Weiss et al. - 2021 - Thinking Like Transformers.pdf;/Users/daniekru/Zotero/storage/2NNFNKDK/2106.html}
}

@article{wenOneshotEntorhinalMaps2024,
  title = {One-Shot Entorhinal Maps Enable Flexible Navigation in Novel Environments},
  author = {Wen, John H. and Sorscher, Ben and Aery Jones, Emily A. and Ganguli, Surya and Giocomo, Lisa M.},
  year = {2024},
  month = nov,
  journal = {Nature},
  volume = {635},
  number = {8040},
  pages = {943--950},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-08034-3},
  urldate = {2024-12-01},
  abstract = {Animals must navigate changing environments to find food, shelter or mates. In mammals, grid cells in the medial entorhinal cortex construct a neural spatial map of the external environment1--5. However, how grid cell firing patterns rapidly adapt to novel or changing environmental features on a timescale relevant to behaviour remains unknown. Here, by recording over 15,000 grid cells in mice navigating virtual environments, we tracked the real-time state of the grid cell network. This allowed us to observe and predict how altering environmental features influenced grid cell firing patterns on a nearly instantaneous timescale. We found evidence that visual landmarks provide inputs to fixed points in the grid cell network. This resulted in stable grid cell firing patterns in novel and altered environments after a single exposure. Fixed visual landmark inputs also influenced the grid cell network such that altering landmarks induced distortions in grid cell firing patterns. Such distortions could be predicted by a computational model with a fixed landmark to grid cell network architecture. Finally, a medial entorhinal cortex-dependent task revealed that although grid cell firing patterns are distorted by landmark changes, behaviour can adapt via a downstream region implementing behavioural timescale synaptic plasticity6. Overall, our findings reveal how the navigational system of the brain constructs spatial maps that balance rapidity and accuracy. Fixed connections between landmarks and grid cells enable the brain to quickly generate stable spatial maps, essential for navigation in novel or changing environments. Conversely, plasticity in regions downstream from grid cells allows the spatial maps of the brain to more accurately mirror the external spatial environment. More generally, these findings raise the possibility of a broader neural principle: by allocating fixed and plastic connectivity across different networks, the brain can solve problems requiring both rapidity and representational accuracy.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Neural circuits,to study},
  file = {/Users/daniekru/Zotero/storage/FTJPE6IL/Wen et al. - 2024 - One-shot entorhinal maps enable flexible navigation in novel environments.pdf}
}

@incollection{wernerModellingNavigationalKnowledge2000,
  title = {Modelling {{Navigational Knowledge}} by {{Route Graphs}}},
  booktitle = {Spatial {{Cognition II}}: {{Integrating Abstract Theories}}, {{Empirical Studies}}, {{Formal Methods}}, and {{Practical Applications}}},
  author = {Werner, Steffen and {Krieg-Br{\"u}ckner}, Bernd and Herrmann, Theo},
  editor = {Freksa, Christian and Habel, Christopher and Brauer, Wilfried and Wender, Karl F.},
  year = {2000},
  pages = {295--316},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45460-8_22},
  urldate = {2025-02-22},
  abstract = {Navigation has always been an interdisciplinary topic of research, because mobile agents of different types are inevitably faced with similar navigational problems. Therefore, human navigation can readily be compared to navigation in other biological organisms or in artificial mobile agents like autonomous robots. One such navigational strategy, route-based navigation, in which an agent moves from one location to another by following a particular route, is the focus of this paper. Drawing on the research from cognitive psychology and linguistics, biology, and robotics, we present a simple, abstract formalism to express the key concepts of route-based navigation in a common scientific language. Starting with the distinction of places and route segments, we develop the notion of a route graph, which can serve as the basis for complex navigational knowledge. Implications and constraints of the model are discussed along the way, together with examples of different instantiations of parts of the model in different mobile agents. By providing this common conceptual framework, we hope to advance the interdisciplinary discussion of spatial navigation.},
  isbn = {978-3-540-45460-1},
  langid = {english},
  keywords = {Mobile Agent,Nest Entrance,Spatial Knowledge,Spatial Navigation,Survey Knowledge},
  file = {/Users/daniekru/Zotero/storage/6DG6UTUK/Werner et al. - 2000 - Modelling Navigational Knowledge by Route Graphs.pdf}
}

@article{westbrookDopamineDoesDouble2016,
  title = {Dopamine Does Double Duty in Motivating Cognitive Effort},
  author = {Westbrook, Andrew and Braver, Todd S.},
  year = {2016},
  month = feb,
  journal = {Neuron},
  volume = {89},
  number = {4},
  pages = {695--710},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2015.12.029},
  urldate = {2025-02-17},
  abstract = {Cognitive control is subjectively costly, suggesting that engagement is modulated in relationship to incentive state. Dopamine appears to play key roles. In particular, dopamine may mediate cognitive effort by two broad classes of functions: 1) modulating the functional parameters of working memory circuits subserving effortful cognition, and 2) mediating value-learning and decision-making about effortful cognitive action. Here we tie together these two lines of research, proposing how dopamine serves ``double duty'', translating incentive information into cognitive motivation.},
  pmcid = {PMC4759499},
  pmid = {26889810},
  file = {/Users/daniekru/Zotero/storage/682BRTFZ/Westbrook and Braver - 2016 - Dopamine does double duty in motivating cognitive effort.pdf}
}

@article{whishawCalibratingSpaceExploration1999a,
  title = {Calibrating Space: {{Exploration}} Is Important for Allothetic and Idiothetic Navigation},
  shorttitle = {Calibrating Space},
  author = {Whishaw, Ian Q. and Brooks, Brian L.},
  year = {1999},
  journal = {Hippocampus},
  volume = {9},
  number = {6},
  pages = {659--667},
  issn = {1050-9631, 1098-1063},
  doi = {10.1002/(SICI)1098-1063(1999)9:6<659::AID-HIPO7>3.0.CO;2-E},
  urldate = {2025-02-18},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  langid = {english}
}

@misc{whittingtonDisentanglementBiologicalConstraints2023,
  title = {Disentanglement with {{Biological Constraints}}: {{A Theory}} of {{Functional Cell Types}}},
  shorttitle = {Disentanglement with {{Biological Constraints}}},
  author = {Whittington, James C. R. and Dorrell, Will and Ganguli, Surya and Behrens, Timothy E. J.},
  year = {2023},
  month = mar,
  number = {arXiv:2210.01768},
  eprint = {2210.01768},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2023-09-24},
  abstract = {Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentanglement in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why single neurons in the brain often represent single human-interpretable factors, and steps towards an understanding task structure shapes the structure of brain representation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,to study},
  file = {/Users/daniekru/Zotero/storage/V5V52PQ3/Whittington et al. - 2023 - Disentanglement with Biological Constraints A The.pdf;/Users/daniekru/Zotero/storage/U2EFRAX7/2210.html}
}

@article{whittingtonTolmanEichenbaumMachineUnifying2020,
  title = {The {{Tolman-Eichenbaum Machine}}: {{Unifying Space}} and {{Relational Memory}} through {{Generalization}} in the {{Hippocampal Formation}}},
  shorttitle = {The {{Tolman-Eichenbaum Machine}}},
  author = {Whittington, James C. R. and Muller, Timothy H. and Mark, Shirley and Chen, Guifen and Barry, Caswell and Burgess, Neil and Behrens, Timothy E. J.},
  year = {2020},
  month = nov,
  journal = {Cell},
  volume = {183},
  number = {5},
  pages = {1249-1263.e23},
  publisher = {Elsevier},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2020.10.024},
  urldate = {2023-03-31},
  langid = {english},
  pmid = {33181068},
  keywords = {entorhinal cortex,generalization,grid cells,hippocampus,neural networks,non-spatial reasoning,place cells,representation learning},
  file = {/Users/daniekru/Zotero/storage/R53HGP7Q/Whittington et al. - 2020 - The Tolman-Eichenbaum Machine Unifying Space and .pdf}
}

@article{widloskiModelGridCell2014,
  title = {A {{Model}} of {{Grid Cell Development}} through {{Spatial Exploration}} and {{Spike Time-Dependent Plasticity}}},
  author = {Widloski, John and Fiete, Ila~R.},
  year = {2014},
  month = jul,
  journal = {Neuron},
  volume = {83},
  number = {2},
  pages = {481--495},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.06.018},
  urldate = {2023-10-24},
  abstract = {Grid cell responses develop gradually after eye opening, but little is known about the rules that govern this process. We present a biologically plausible model for the formation of a grid cell network. An asymmetric spike time-dependent plasticity rule acts upon an initially unstructured network of spiking neurons that receive inputs encoding animal velocity and location. Neurons develop an organized recurrent architecture based on the similarity of their inputs, interacting through inhibitory interneurons. The mature network can convert velocity inputs into estimates of animal location, showing that spatially periodic responses and the capacity of path integration can arise through synaptic plasticity, acting on inputs that display neither. The model provides numerous predictions about the necessity of spatial exploration for grid cell development, network topography, the maturation of velocity tuning and neural correlations, the abrupt transition to stable patterned responses, and possible mechanisms to set grid period across grid modules.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/VADYHBQ6/Widloski and Fiete - 2014 - A Model of Grid Cell Development through Spatial E.pdf}
}

@misc{wijmansEmergenceMapsMemories2023,
  title = {Emergence of {{Maps}} in the {{Memories}} of {{Blind Navigation Agents}}},
  author = {Wijmans, Erik and Savva, Manolis and Essa, Irfan and Lee, Stefan and Morcos, Ari S. and Batra, Dhruv},
  year = {2023},
  month = jan,
  number = {arXiv:2301.13261},
  eprint = {2301.13261},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.13261},
  urldate = {2023-10-27},
  abstract = {Animal navigation research posits that organisms build and maintain internal spatial representations, or maps, of their environment. We ask if machines -- specifically, artificial intelligence (AI) navigation agents -- also build implicit (or 'mental') maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent's perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train 'blind' agents -- with sensing limited to only egomotion and no other sensing of any kind -- to perform PointGoal navigation ('go to \${\textbackslash}Delta\$ x, \${\textbackslash}Delta\$ y') via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments ({\textasciitilde}95\% success); (2) they utilize memory over long horizons (remembering {\textasciitilde}1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent 'forgets' exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,insightful},
  file = {/Users/daniekru/Zotero/storage/XKMVUGXJ/Wijmans et al. - 2023 - Emergence of Maps in the Memories of Blind Navigat.pdf;/Users/daniekru/Zotero/storage/LXAYG77J/2301.html}
}

@misc{wilsonNeuromodulatedLearningDeep2018,
  title = {Neuromodulated {{Learning}} in {{Deep Neural Networks}}},
  author = {Wilson, Dennis G. and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e} and Harrington, Kyle},
  year = {2018},
  month = dec,
  number = {arXiv:1812.03365},
  eprint = {1812.03365},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.03365},
  urldate = {2022-09-05},
  abstract = {In the brain, learning signals change over time and synaptic location, and are applied based on the learning history at the synapse, in the complex process of neuromodulation. Learning in artificial neural networks, on the other hand, is shaped by hyper-parameters set before learning starts, which remain static throughout learning, and which are uniform for the entire network. In this work, we propose a method of deep artificial neuromodulation which applies the concepts of biological neuromodulation to stochastic gradient descent. Evolved neuromodulatory dynamics modify learning parameters at each layer in a deep neural network over the course of the network's training. We show that the same neuromodulatory dynamics can be applied to different models and can scale to new problems not encountered during evolution. Finally, we examine the evolved neuromodulation, showing that evolution found dynamic, location-specific learning strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/ETCT83UQ/Wilson et al. - 2018 - Neuromodulated Learning in Deep Neural Networks.pdf;/Users/daniekru/Zotero/storage/9QKKTWKW/1812.html}
}

@article{wiseBrainDopamineReward1989,
  title = {Brain Dopamine and Reward},
  author = {Wise, Roy A. and Rompr{\'e}, Pierre-Paul},
  year = {1989},
  journal = {Annual Review of Psychology},
  volume = {40},
  pages = {191--225},
  publisher = {Annual Reviews},
  address = {US},
  issn = {1545-2085},
  doi = {10.1146/annurev.ps.40.020189.001203},
  abstract = {Reviews the evidence implicating dopaminergic neurons in reward and discusses major findings that have established the hypothesis that dopamine plays a critical role in the rewarding effects of brain stimulation, psychomotor stimulants, opiates, and food. The complexity of brain circuitry within which dopamine serves as a single, though very important link, is discussed, and a broader motivational perspective for what was initially termed a "pleasure center" in the brain is suggested. It is concluded that dopamine systems play a very general role in mood and movement, a role that is essential to reward function as well as to other aspects of motivated behavior. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Brain,Dopamine,Literature Review,Motivation,Neurochemistry,Reinforcement},
  file = {/Users/daniekru/Zotero/storage/JND6WAVW/1989-25079-001.html}
}

@article{wurbelEthologyAppliedAnimal2009,
  title = {Ethology Applied to Animal Ethics},
  author = {W{\"u}rbel, Hanno},
  year = {2009},
  month = may,
  journal = {Applied Animal Behaviour Science},
  series = {Special {{Issue}}: {{Animal Suffering}} and {{Welfare}}},
  volume = {118},
  number = {3},
  pages = {118--127},
  issn = {0168-1591},
  doi = {10.1016/j.applanim.2009.02.019},
  urldate = {2023-05-29},
  abstract = {According to modern animal welfare legislation, animals should be protected from suffering and lasting harm not for the benefit of us humans as in earlier anthropocentric conceptions, but in their own interest. The driving force behind animal protection is our empathy with animals which triggers feelings of compassion. Empathy with animals most likely is a psychological side-effect of adaptive empathy among humans, and its expression is largely determined by the degree of similarity between animals and us in morphology and behaviour. As a result, compassion with animals is vulnerable to anthropocentric bias, prejudice, and deception, and animal protection based on compassion is likely to be unfair towards animals. Moreover, from an ethological perspective, protecting animals in their own interest represents true altruism which places considerable ethical demand on us. However, there may be hidden selfish intentions that question the altruistic nature of animal protection, while at the same time facilitating its implementation. For example, animal protection could help to avoid unpleasant feelings induced by witnessing cruel actions towards animals. Alternatively, exhibiting a caring personality towards animals could represent human social behaviour that pays off indirectly through building up a caring reputation. It is, therefore, important to distinguish between our intention to protect animals (which may be partly selfish) and true animal protection that needs to be justified biologically by values that apply to the animals. Based on the sentientist nature of animal welfare legislation, the greatest challenges to applied ethologists, and important ones as testified by this special issue, are (i) to determine sentience in animals and (ii) to establish valid and reliable measures of affective states such as suffering and well-being. However, there are promising measures of animal welfare beyond measures of affective states. In particular, biologically meaningful measures of `integrity of form and function' may provide powerful indicators of animal welfare. The integrity concept originates in biocentric ethics and goes beyond sentientism, as it can also be applied to non-sentient animals and even plants. However, when applied to (potentially) sentient animals, it appears to be consistent with our common sense notion of animal welfare which also respects the animals' `nature' or `telos'. Moreover, the integrity concept would relieve scientists from solving the `hard problem' of animal consciousness first, or from establishing valid measures of demand or aversion that are notoriously difficult to establish. In particular, measures of behavioural integrity could offer an opportunity for applied ethology to strengthen its impact on ethical and legal decision-taking, thereby advancing animal welfare without compromising scientific credibility.},
  langid = {english},
  keywords = {Animal rights,Animal welfare,Applied ethology,Behaviour,Compassion,Empathy,Ethics,Integrity,Legislation,Sentience,Suffering,Utilitarianism,Well-being},
  file = {/Users/daniekru/Zotero/storage/A8RM2HDW/Würbel - 2009 - Ethology applied to animal ethics.pdf;/Users/daniekru/Zotero/storage/U8TDXJJ2/S0168159109000392.html}
}

@article{wuSimpleModelBehavioral2025,
  title = {A Simple Model for {{Behavioral Time Scale Synaptic Plasticity}} ({{BTSP}}) Provides Content Addressable Memory with Binary Synapses and One-Shot Learning},
  author = {Wu, Yujie and Maass, Wolfgang},
  year = {2025},
  month = jan,
  journal = {Nature Communications},
  volume = {16},
  number = {1},
  pages = {342},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-55563-6},
  urldate = {2025-01-20},
  abstract = {Recent experimental studies in the awake brain have identified a rule for synaptic plasticity that is instrumental for the instantaneous creation of memory traces in area CA1 of the mammalian brain: Behavioral Time scale Synaptic Plasticity. This one-shot learning rule differs in five essential aspects from previously considered plasticity mechanisms. We introduce a transparent model for the core function of this learning rule and establish a theory that enables a principled understanding of the system of memory traces that it creates. Theoretical predictions and numerical simulations show that our model is able to create a functionally powerful content-addressable memory without the need for high-resolution synaptic weights. Furthermore, it reproduces the repulsion effect of human memory, whereby traces for similar memory items are pulled apart to enable differential downstream processing. Altogether, our results create a link between synaptic plasticity in area CA1 of the hippocampus and its network function. They also provide a promising approach for implementing content-addressable memory with on-chip learning capability in highly energy-efficient crossbar arrays of memristors.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Computational science},
  file = {/Users/daniekru/Zotero/storage/Z8WVF8UE/Wu and Maass - 2025 - A simple model for Behavioral Time Scale Synaptic Plasticity (BTSP) provides content addressable mem.pdf}
}

@article{xiaSurveyMeasuresNetwork2019,
  title = {A {{Survey}} of {{Measures}} for {{Network Motifs}}},
  author = {Xia, Feng and Wei, Haoran and Yu, Shuo and Zhang, Da and Xu, Bo},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {106576--106587},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2926752},
  abstract = {Network motifs provide an enlightening insight into uncovering the structural design principles of complex networks across multifarious disciplines, such as physics, biology, social science, engineering, and military science. Measures for network motifs play an indispensable role in the procedures of motif measurement and evaluation which are crucial steps in motif detection, counting, and clustering. However, there is a relatively small body of literature concerned with measures for network motifs. In this paper, we review the measures for network motifs in two categories: structural measures and statistical measures. The application scenarios for each measure and the distinctions of measures in similar scenarios are also summarized. We also conclude the challenges for using these measures and put forward some future directions on this topic. Overall, the objective of this survey is to provide an overview of motif measures, which is anticipated to shed light on the theory and practice of complex networks.},
  keywords = {Complex networks,Frequency measurement,Military communication,motif measure,Network motif,network science and motif definition,Software,Systematics,Volume measurement},
  file = {/Users/daniekru/Zotero/storage/T2HLRGRI/Xia et al. - 2019 - A Survey of Measures for Network Motifs.pdf;/Users/daniekru/Zotero/storage/WWCDSN7G/8755292.html}
}

@misc{XMOL,
  title = {X-{{MOL}}},
  journal = {x-mol.net},
  urldate = {2023-05-12},
  howpublished = {https://www.x-mol.net/paper/article/1492204724588675072},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/26F8ALGS/1492204724588675072.html}
}

@article{yoderAcetylcholineContributesIntegration2017,
  title = {Acetylcholine Contributes to the Integration of Self-Movement Cues in Head Direction Cells.},
  author = {Yoder, Ryan M. and Chan, Jeremy H. M. and Taube, Jeffrey S.},
  year = {2017},
  month = aug,
  journal = {Behavioral Neuroscience},
  volume = {131},
  number = {4},
  pages = {312--324},
  issn = {1939-0084, 0735-7044},
  doi = {10.1037/bne0000205},
  urldate = {2024-03-16},
  abstract = {Acetylcholine contributes to accurate performance of some navigation, but details of its contribution to the underlying brain signals are not fully understood. Selective damage to medial septal cholinergic neurons generally has little effect on landmark-based navigation, or the underlying neural representations of location and directional heading in visual environments. In contrast, the loss of cholinergic neurons disrupts navigation based on path integration, but no studies have tested whether these path integration deficits are associated with disrupted head direction (HD) cell activity. We therefore evaluated HD cell responses to visual cue rotations in a familiar arena, and during navigation between familiar and novel arenas, following muscarinic receptor blockade with systemic atropine. Atropine treatment reduced the peak firing rate of HD cells, but failed to significantly affect other HD cell firing properties. Atropine also failed to significantly disrupt the dominant landmark control of the HD signal, even though we used a procedure that challenged this landmark control. In contrast, atropine disrupted HD cell stability during navigation between familiar and novel arenas, where path integration normally maintains a consistent HD cell signal across arenas. These results suggest that acetylcholine contributes to path integration, in part, by facilitating the use of idiothetic cues to maintain a consistent representation of directional heading.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/I24SVVN5/Yoder et al. - 2017 - Acetylcholine contributes to the integration of se.pdf}
}

@misc{zadorNextGenerationArtificialIntelligence2022,
  title = {Toward {{Next-Generation Artificial Intelligence}}: {{Catalyzing}} the {{NeuroAI Revolution}}},
  shorttitle = {Toward {{Next-Generation Artificial Intelligence}}},
  author = {Zador, Anthony and Richards, Blake and {\"O}lveczky, Bence and Escola, Sean and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and DiCarlo, James and Ganguli, Surya and Hawkins, Jeff and Koerding, Konrad and Koulakov, Alexei and LeCun, Yann and Lillicrap, Timothy and Marblestone, Adam and Olshausen, Bruno and Pouget, Alexandre and Savin, Cristina and Sejnowski, Terrence and Simoncelli, Eero and Solla, Sara and Sussillo, David and Tolias, Andreas S. and Tsao, Doris},
  year = {2022},
  month = oct,
  number = {arXiv:2210.08340},
  eprint = {2210.08340},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.08340},
  urldate = {2022-10-26},
  abstract = {Neuroscience has long been an important driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,insightful,optional,Quantitative Biology - Neurons and Cognition},
  file = {/Users/daniekru/Zotero/storage/Z33HNRUQ/Zador et al. - 2022 - Toward Next-Generation Artificial Intelligence Ca.pdf;/Users/daniekru/Zotero/storage/UPVVNPGI/2210.html}
}

@article{zannoneAcetylcholinemodulatedPlasticityRewarddriven2018,
  title = {Acetylcholine-Modulated Plasticity in Reward-Driven Navigation: A Computational Study},
  shorttitle = {Acetylcholine-Modulated Plasticity in Reward-Driven Navigation},
  author = {Zannone, Sara and Brzosko, Zuzanna and Paulsen, Ole and Clopath, Claudia},
  year = {2018},
  month = jun,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {9486},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-27393-2},
  urldate = {2023-06-19},
  abstract = {Neuromodulation plays a fundamental role in the acquisition of new behaviours. In previous experimental work, we showed that acetylcholine biases hippocampal synaptic plasticity towards depression, and the subsequent application of dopamine can retroactively convert depression into potentiation. We also demonstrated that incorporating this sequentially neuromodulated Spike-Timing-Dependent Plasticity (STDP) rule in a network model of navigation yields effective learning of changing reward locations. Here, we employ computational modelling to further characterize the effects of cholinergic depression on behaviour. We find that acetylcholine, by allowing learning from negative outcomes, enhances exploration over the action space. We show that this results in a variety of effects, depending on the structure of the model, the environment and the task. Interestingly, sequentially neuromodulated STDP also yields flexible learning, surpassing the performance of other reward-modulated plasticity rules.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Learning algorithms,Network models,to study},
  file = {/Users/daniekru/Zotero/storage/WYFLIHTL/Zannone et al. - 2018 - Acetylcholine-modulated plasticity in reward-drive.pdf}
}

@misc{zannoneSynapticPlasticityModels2019,
  title = {Synaptic Plasticity Models for Reinforcement Learning},
  author = {Zannone, Sara},
  year = {2019},
  month = oct,
  publisher = {Imperial College London},
  abstract = {Animal learning is based on a process of trial and error. This is a fundamental observation in behavioural psychology, but also the core idea of Reinforcement Learning (RL). As such, Reinforcement Learning provides a suitable theoretical framework to study animal learning, but little is known about how Reinforcement Learning is achieved at the level of the neural networks in our brain. In general, it is believed that salient information is stored in neural network by altering the connections between neurons, a process called synaptic plasticity. Here, we study how known plasticity models can give rise to Reinforcement Learning algorithms in simple models of hippocampal networks. {\textbackslash}{\textbackslash} First, we present experimental results from synaptic plasticity experiments under the control of neuromulation: specifically dopamine and acetylcholine. Then, we incorporate these observations in a network model of a navigation task. We find that dopamine is responsible for reward-modulated learning, whereas acetylcholine affects exploration patterns and allows flexible learning in changing environments. Separately, we propose a biologically plausible plasticity rule that can learn a predictive representation of the environment, a concept borrowed from RL theory which is called the Successor Representation. We then analyse the learning dynamics of our network from a normative perspective and relate it to Temporal Difference (TD) methods, well known RL algorithms. This relationship with TD learning provides us with a powerful tool to study the role of various biological parameters in relationship to the RL algorithm. It allows us to propose a new mechanism for learning on behavioural timescales through STDP, and to postulate novel functional roles for neuronal activity patterns called replays. We also find that altering the timescale of the neuronal activity adds flexibility to the representation of the state space, which is useful for subgoal discovery and hierarchical reasoning.},
  keywords = {computational model,to study},
  file = {/Users/daniekru/Zotero/storage/8G3EQ5C5/Zannone-S-2019-PhD-Thesis.pdf}
}

@article{zenkeDiverseSynapticPlasticity2015,
  title = {Diverse Synaptic Plasticity Mechanisms Orchestrated to Form and Retrieve Memories in Spiking Neural Networks},
  author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
  year = {2015},
  month = apr,
  journal = {Nature Communications},
  volume = {6},
  number = {1},
  pages = {6922},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/ncomms7922},
  urldate = {2022-10-27},
  abstract = {Synaptic plasticity, the putative basis of learning and memory formation, manifests in various forms and across different timescales. Here we show that the interaction of Hebbian homosynaptic plasticity with rapid non-Hebbian heterosynaptic plasticity is, when complemented with slower homeostatic changes and consolidation, sufficient for assembly formation and memory recall in a spiking recurrent network model of excitatory and inhibitory neurons. In the model, assemblies were formed during repeated sensory stimulation and characterized by strong recurrent excitatory connections. Even days after formation, and despite ongoing network activity and synaptic plasticity, memories could be recalled through selective delay activity following the brief stimulation of a subset of assembly neurons. Blocking any component of plasticity prevented stable functioning as a memory network. Our modelling results suggest that the diversity of plasticity phenomena in the brain is orchestrated towards achieving common functional goals.},
  copyright = {2015 The Author(s)},
  langid = {english},
  keywords = {Learning and memory,Neural circuits,Synaptic plasticity},
  file = {/Users/daniekru/Zotero/storage/JUMUG2IZ/Zenke et al. - 2015 - Diverse synaptic plasticity mechanisms orchestrate.pdf;/Users/daniekru/Zotero/storage/AYI2LPRX/ncomms7922.html}
}

@article{zhangCheapCleverHuman,
  title = {Cheap but {{Clever}}: {{Human Active Learning}} in a {{Bandit Setting}}},
  author = {Zhang, Shunan and Yu, Angela J},
  abstract = {How people achieve long-term goals in an imperfectly known environment, via repeated tries and noisy outcomes, is an important problem in cognitive science. There are two interrelated questions: how humans represent information, both what has been learned and what can still be learned, and how they choose actions, in particular how they negotiate the tension between exploration and exploitation. In this work, we examine human behavioral data in a multi-armed bandit setting, in which the subject choose one of four ``arms'' to pull on each trial and receives a binary outcome (win/lose). We implement both the Bayes-optimal policy, which maximizes the expected cumulative reward in this finite-horizon bandit environment, as well as a variety of heuristic policies that vary in their complexity of information representation and decision policy. We find that the knowledge gradient algorithm, which combines exact Bayesian learning with a decision policy that maximizes a combination of immediate reward gain and longterm knowledge gain, captures subjects' trial-by-trial choice best among all the models considered; it also provides the best approximation to the computationally intense optimal policy among all the heuristic policies.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/I5NDI744/Zhang and Yu - Cheap but Clever Human Active Learning in a Bandi.pdf}
}

@techreport{zhangComplexityRestingBrain2020,
  type = {Preprint},
  title = {Complexity of Resting Brain Dynamics Shaped by Multiscale Structural Constraints},
  author = {Zhang, Mengsen and Saggar, Manish},
  year = {2020},
  month = may,
  institution = {Neuroscience},
  doi = {10.1101/2020.05.14.097196},
  urldate = {2020-06-18},
  abstract = {The brain is a complex, nonlinear system, exhibiting ever-evolving patterns of activities even without external inputs or tasks. Such intrinsic or resting neural dynamics has been found to play critical roles in the normal functioning of the brain and psychiatric disorders. It remains a challenge, however, to link the intrinsic dynamics to the underlying structure, in part, due to the nonlinearity. Here we use a nonlinear-dynamical model to examine how the complexity of intrinsic neural dynamics, in terms of multistability and temporal diversity, is sculpted by structural properties across scales. Our model combines a population-level model (WilsonCowan) with additional biophysical constraints (from the reduced WongWang model). We show that multistability can emerge at the whole-brain level even when individual brain regions are by themselves monostable. The multi-functionality and memory capacity associated with multistability are thus synergistic properties of the whole-brain, irreducible to properties of its parts. The exact size of the functional repertoire and memory capacity is a joint product of the nonlinearity in the local dynamics and the topology of the large-scale network. Similarly, temporal diversity of the brain is determined by both local structural differences and the topology of the global network. Together, this work unravels an intertwined and circular relationship between local and global properties in defining the intrinsic dynamic organization of the brain. Looking forward, the model can be used to probe the multiscale mechanisms underlying psychiatric disorders and the effective scales for treatment.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/CFUHZZT4/Zhang and Saggar - 2020 - Complexity of resting brain dynamics shaped by mul.pdf}
}

@article{zhangExpectancyrelatedChangesFiring2024,
  title = {Expectancy-Related Changes in Firing of Dopamine Neurons Depend on Hippocampus},
  author = {Zhang, Zhewei and Takahashi, Yuji K. and {Montesinos-Cartegena}, Marlian and Kahnt, Thorsten and Langdon, Angela J. and Schoenbaum, Geoffrey},
  year = {2024},
  month = oct,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {8911},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-53308-z},
  urldate = {2025-02-17},
  abstract = {The orbitofrontal cortex (OFC) and hippocampus (HC) both contribute to the cognitive maps that support flexible behavior. Previously, we used the dopamine neurons to measure the functional role of OFC. We recorded midbrain dopamine neurons as rats performed an odor-based choice task, in which expected rewards were manipulated across blocks. We found that ipsilateral OFC lesions degraded dopaminergic prediction errors, consistent with reduced resolution of the task states. Here we have repeated this experiment in male rats with ipsilateral HC lesions. The results show HC also shapes the task states, however unlike OFC, which provides information local to the trial, the HC is necessary for estimating upper-level hidden states that distinguish blocks. The results contrast the roles of the OFC and HC in cognitive mapping and suggest that the dopamine neurons access rich information from distributed regions regarding the environment's structure, potentially enabling this teaching signal to support complex behaviors.},
  copyright = {2024 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
  langid = {english},
  keywords = {Learning and memory,Neural circuits},
  file = {/Users/daniekru/Zotero/storage/RI3MGB76/Zhang et al. - 2024 - Expectancy-related changes in firing of dopamine neurons depend on hippocampus.pdf}
}

@inproceedings{zhangForgetfulBayesMyopic2013,
  title = {Forgetful {{Bayes}} and Myopic Planning: {{Human}} Learning and Decision-Making in a Bandit Setting},
  shorttitle = {Forgetful {{Bayes}} and Myopic Planning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Shunan and Yu, Angela J},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-02},
  abstract = {How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects' choices, on a trial-to-trial basis, are best captured by a forgetful" Bayesian iterative learning model in combination with a partially myopic decision policy known as Knowledge Gradient. This model accounts for subjects' trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, epsilon-greedy and win-stay-lose-shift. It has the added benefit of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are significantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment."},
  file = {/Users/daniekru/Zotero/storage/V336BASA/Zhang and Yu - 2013 - Forgetful Bayes and myopic planning Human learnin.pdf}
}

@article{zhangHippocampalSpatialRepresentations2023,
  title = {Hippocampal Spatial Representations Exhibit a Hyperbolic Geometry That Expands with Experience},
  author = {Zhang, Huanqiu and Rich, P. Dylan and Lee, Albert K. and Sharpee, Tatyana O.},
  year = {2023},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {26},
  number = {1},
  pages = {131--139},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01212-4},
  urldate = {2024-06-06},
  abstract = {Daily experience suggests that we perceive distances near us linearly. However, the actual geometry of spatial representation in the brain is unknown. Here we report that neurons in the CA1 region of rat hippocampus that mediate spatial perception represent space according to a non-linear hyperbolic geometry. This geometry uses an exponential scale and yields greater positional information than a linear scale. We found that the size of the representation matches the optimal predictions for the number of CA1 neurons. The representations also dynamically expanded proportional to the logarithm of time that the animal spent exploring the environment, in correspondence with the maximal mutual information that can be received. The dynamic changes tracked even small variations due to changes in the running speed of the animal. These results demonstrate how neural circuits achieve efficient representations using dynamic hyperbolic geometry.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {insightful,Learning and memory,Neural encoding,to study},
  file = {/Users/daniekru/Zotero/storage/VR3WIJLY/Zhang et al. - 2023 - Hippocampal spatial representations exhibit a hype.pdf}
}

@article{zhaoBrainInspiredDecisionMakingSpiking2018,
  title = {A {{Brain-Inspired Decision-Making Spiking Neural Network}} and {{Its Application}} in {{Unmanned Aerial Vehicle}}},
  author = {Zhao, Feifei and Zeng, Yi and Xu, Bo},
  year = {2018},
  month = sep,
  journal = {Frontiers in Neurorobotics},
  volume = {12},
  publisher = {Frontiers},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2018.00056},
  urldate = {2024-05-03},
  abstract = {Decision-making is a crucial cognitive function for various animal species surviving in nature, and it is also a fundamental ability for intelligent agents. To make a step forward in the understanding of the computational mechanism of human-like decision-making, this paper proposes a brain-inspired decision-making spiking neural network (BDM-SNN) and applies it to decision-making tasks on intelligent agents. This paper makes the following contributions: (1) A spiking neural network (SNN) is used to model human decision-making neural circuit from both connectome and functional perspectives. (2) The proposed model combines dopamine and spike-timing-dependent plasticity (STDP) mechanisms to modulate the network learning process, which indicates more biological inspiration. (3) The model considers the effects of interactions among sub-areas in PFC on accelerating the learning process. (4) The proposed model can be easily applied to decision-making tasks in intelligent agents, such as an unmanned aerial vehicle (UAV) flying through a window and a UAV avoiding an obstacle. The experimental results support the effectiveness of the model. Compared with traditional reinforcement learning and existing biologically inspired methods, our method contains more biologically-inspired mechanistic principles, has greater accuracy and is faster.},
  langid = {english},
  keywords = {brain-inspired decision-making,Dopamine Regulation,multiple brain areas coordination,reinforcement learning model,Spiking Neural network,UAV autonomous learning},
  file = {/Users/daniekru/Zotero/storage/NYWWHQ9Q/Zhao et al. - 2018 - A Brain-Inspired Decision-Making Spiking Neural Ne.pdf}
}

@article{zhouCorticalMechanismsMultisensory2022,
  title = {Cortical {{Mechanisms}} of {{Multisensory Linear Self-motion Perception}}},
  author = {Zhou, Luxin and Gu, Yong},
  year = {2022},
  month = jul,
  journal = {Neuroscience Bulletin},
  volume = {39},
  number = {1},
  pages = {125--137},
  issn = {1673-7067},
  doi = {10.1007/s12264-022-00916-8},
  urldate = {2025-02-18},
  abstract = {Accurate self-motion perception, which is critical for organisms to survive, is a process involving multiple sensory cues. The two most powerful cues are visual (optic flow) and vestibular (inertial motion). Psychophysical studies have indicated that humans and nonhuman primates integrate the two cues to improve the estimation of self-motion direction, often in a statistically Bayesian-optimal way. In the last decade, single-unit recordings in awake, behaving animals have provided valuable neurophysiological data with a high spatial and temporal resolution, giving insight into possible neural mechanisms underlying multisensory self-motion perception. Here, we review these findings, along with new evidence from the most recent studies focusing on the temporal dynamics of signals in different modalities. We show that, in light of new data, conventional thoughts about the cortical mechanisms underlying visuo-vestibular integration for linear self-motion are challenged. We propose that different temporal component signals may mediate different functions, a possibility that requires future studies.},
  pmcid = {PMC9849545},
  pmid = {35821337},
  file = {/Users/daniekru/Zotero/storage/SAWDJIVH/Zhou and Gu - 2022 - Cortical Mechanisms of Multisensory Linear Self-motion Perception.pdf}
}

@article{zylberbergMechanismsPersistentActivity2017,
  title = {Mechanisms of {{Persistent Activity}} in {{Cortical Circuits}}: {{Possible Neural Substrates}} for {{Working Memory}}},
  shorttitle = {Mechanisms of {{Persistent Activity}} in {{Cortical Circuits}}},
  author = {Zylberberg, Joel and Strowbridge, Ben W.},
  year = {2017},
  month = jul,
  journal = {Annual Review of Neuroscience},
  volume = {40},
  number = {Volume 40, 2017},
  pages = {603--627},
  publisher = {Annual Reviews},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-070815-014006},
  urldate = {2024-05-13},
  abstract = {A commonly observed neural correlate of working memory is firing that persists after the triggering stimulus disappears. Substantial effort has been devoted to understanding the many potential mechanisms that may underlie memory-associated persistent activity. These rely either on the intrinsic properties of individual neurons or on the connectivity within neural circuits to maintain the persistent activity. Nevertheless, it remains unclear which mechanisms are at play in the many brain areas involved in working memory. Herein, we first summarize the palette of different mechanisms that can generate persistent activity. We then discuss recent work that asks which mechanisms underlie persistent activity in different brain areas. Finally, we discuss future studies that might tackle this question further. Our goal is to bridge between the communities of researchers who study either single-neuron biophysical, or neural circuit, mechanisms that can generate the persistent activity that underlies working memory.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/MKX56IQL/Zylberberg and Strowbridge - 2017 - Mechanisms of Persistent Activity in Cortical Circ.pdf;/Users/daniekru/Zotero/storage/GDPP3LU5/annurev-neuro-070815-014006.html}
}
