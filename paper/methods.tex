\section{Methods}

Our model focuses on cognitive map formation through an agent's experiences within a closed environment, as illustrated in Figure \ref{fig:model}\textbf{a}..

The architecture operates with minimal external information—just binary reward and collision signals.
Instead, spatial representation emerges primarily from idiothetic information: the agent's perception of self-motion \cite{zhouCorticalMechanismsMultisensory2022}.
Specifically, we use the factual velocity vector (the agent's actual environmental displacement) as the primary navigational cue, which in biological systems emerges from integrated inertial and relative motion signals \cite{jerjianSelfmotionPerceptionSequential2023, whishawCalibratingSpaceExploration1999a}.

\paragraph{Place Cell Formation}
Spatial representation begins with a stack of grid cell modules, each encoding a periodic tiling of space over a 2D torus $\mathbf{T}^2$ (Fig.\ref{fig:model}\textbf{b,c}). Grid cell activity vectors $\mathbf{u}^{\text{GC}}$ project to two place cell layers with initially zeroed weights.
When no place cells activate for the current input, a silent neuron is randomly selected and imprinted with the current activity.
To ensure selectivity, imprinting is aborted if the new weight vector overlaps too strongly (via cosine similarity) with existing ones.

Each place cell’s activation is given by a bounded activation function based on cosine similarity.
Details of lateral inhibition and recurrent connectivity are given in Appendix~A.1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/figure_model_v2.png}
    \caption{\textsc{Model layout and spatial representations} -
    \textbf{a}: \textit{the full architecture of the model, consisting of three main sensory input, targeting the two modulators and the cognitive map module, and the executive components, represented by a policy module, two behavioral programs
    and a reward receiver}.
    \textbf{b}: \textit{the cognitive map component, organized with a stack of grid cell modules receiving the velocity input and projecting to the layer of place cells}.
    \textbf{c}: \textit{the neural activity of a grid cell module from
a random trajectory; in blue the repeating activity of all cell, while in green the activity of only one, highlighting the periodicity in space}.
    \textbf{d}: \textit{the distribution in space of the place cells centers, together with the activity of two cells showing the size of their place field}.
    \textbf{e}: \textit{neuromodulation activity over the place cells map, with in blue the cells tagged by the collision modulation, and in green the ones targeted by reward modulation.}}
    \label{fig:model}
\end{figure}

\paragraph{Neuromodulation}
Neuromodulators encode environmental salience. Two scalar modulators, one for reward (DA) and one for boundary collisions (BND), are driven by binary inputs $I$ and accumulate over time via exponential decay.

Each modulator $k$ updates synaptic weights to place cells through Hebbian plasticity:
\[
\Delta \mathbf{W}^k = \eta^k v^k \mathbf{u}^{\text{PC}}
\]
Weight vectors are constrained to remain non-negative.
Reward modulation tags cells near rewarded locations, while boundary modulation builds a representation of environmental edges. These scalar fields form the core of the cognitive map (Fig.~\ref{fig:model}\textbf{e}). See Appendix~A.2 for full learning rules and parameter settings.

\paragraph{Online Adaptation}
To remain resilient to environmental changes (e.g., moving rewards), the model uses a predictive mechanism to correct outdated internal representations.
Before executing a movement toward position $\mathbf{x}_{t+1}$, the system predicts the expected value $\hat{v}^{k}$ of neuromodulator $k$ at that location.
Then, a prediction error is computed and used to adjust the weights:
\[
\Delta \mathbf{W}^{k} \leftarrow \Delta \mathbf{W}^{k} + \hat{\eta}^{k} (\hat{v}^k - v^k) \mathbf{u}^{\text{PC}}
\]
The prediction learning rate $\hat{\eta}^{k} < 1$ ensures that small errors are corrected gradually, while repeated mismatches lead to weight depression. This mechanism implements a simple form of predictive coding and is inspired by temporal-difference learning \cite{suttonReinforcementLearningProblem}, aligning with evidence that neuromodulatory systems signal prediction errors and update beliefs \cite{montagueFrameworkMesencephalicDopamine1996, decothiPredictiveMapsRats2022, krishnanRewardExpectationExtinction2022}.

\paragraph{Modulation of Place Fields}
\hfill \break
We further tested whether neuromodulators could directly alter spatial tuning. Place fields were dynamically shifted and resized based on recent salience signals.

Following a salient event (reward or collision), place field centers were displaced in grid cell space, with magnitude scaled by the neuromodulator $v^k$ and proximity to the event:
\[
\Delta \mathbf{W}^{\text{GC}, \text{PC}}_i = c^{k} v^{k} \varphi_{\sigma^{k}}(\mathbf{u}^{\text{GC}} - \mathbf{W}^{\text{GC}, \text{PC}}_i)
\]
Here, $\varphi_{\sigma}$ is a Gaussian function, and $c^{k}$ a scaling factor. This rule is inspired by BTSP plasticity \cite{bittnerBehavioralTimeScale2017, milsteinBidirectionalSynapticPlasticity2021}, which shifts CA1 place fields following salient experiences. Lateral inhibition prevents field overlap during remapping.

In addition to dislocation, field size was modulated by scaling the gain of recently active neurons. The gain $\beta_i$ of each cell was adjusted via a trace variable $m_i$.
This provides a mechanism for neuromodulators to transiently enhance or suppress spatial sensitivity.

\paragraph{Policy and Behavior}
To evaluate the model's utility in navigation, we implemented a simple policy toggling between exploration and goal-seeking behavior, depending on an external goal flag and the internal map.

Exploration consisted of a stochastic walk with persistence, plus periodic plans to visit random known locations to avoid stagnation. Goal-directed navigation was triggered when a reward representation was present and executed via shortest-path planning on the place cell graph.

The graph was defined by place cells as nodes and synaptic links as edges. The agent selected targets either randomly or based on cells with high dopaminergic weight, echoing hippocampal replay and value-based navigation \cite{mcnamaraDopaminergicNeuronsPromote2014, michonSingletrialDynamicsHippocampal2021, shamashMiceIdentifySubgoal2021}. Planning was achieved via Dijkstra's algorithm with a cost function that penalized proximity to boundaries.

% In the process of behavior, learning does not occur explicitly, but is instead accounted for in the online formation of the cognitive map.

\paragraph{Comparison with previous architectures}

\begin{table}[H]
\centering
\caption{Comparison of neural network models for spatial navigation and representation}
\label{tab:models_comparison}
\small
\begin{tabular}{l p{2.5cm} p{3.cm} c}
\toprule
\textbf{Model} & \textbf{Architecture} & \textbf{Training Method} & \textbf{Ext. C.} \\
\midrule
Banino et al. \cite{baninoVectorbasedNavigationUsing2018} & LSTM + linear layers + CNN & BPTT and deep RL, supervised & Yes \\
\midrule
Cueva et al. \cite{cuevaEmergenceGridlikeRepresentations2018} & RNN + linear layers & Hessian-free algorithm with regularization & Yes \\
\midrule
Sorcher et al. \cite{sorscherUnifiedTheoryOrigin2019} & RNN + linear layers & Backpropagation with regularization & Yes \\
\midrule
Whittington et al. \cite{whittingtonTolmanEichenbaumMachineUnifying2020} & Attractor network and deep networks & Backpropagation and Hebbian learning & No \\
\midrule
de-Cothi et al. \cite{decothiPredictiveMapsRats2022} & Successor representation & TD-learning + eligibility traces & Yes \\
\midrule
Brozsko et al. \cite{brzoskoSequentialNeuromodulationHebbian2017} & Spike Response Model & Online modulated Hebbian plasticity & Yes \\
\midrule
\textbf{Ours} & Rate layers & Online neuro-modulated plasticity & No \\
\bottomrule
\end{tabular}

\vspace{0.3cm}

\begin{tabular}{l p{2.8cm} p{2.8cm} p{2.cm}}
\toprule
\textbf{Model} & \textbf{Task} & \textbf{Input} & \textbf{Output} \\
\midrule
Banino et al. & Path integration, goal navigation & Velocity, visual input, reward & PC, HDC \\
\midrule
Cueva et al. & Path integration & Velocity & Position \\
\midrule
Sorcher et al. & Path integration & Velocity & PC \\
\midrule
Whittington et al. & Relational graph knowledge & Observation and action & Observation \\
\midrule
de-Cothi et al. & Planned navigation & Observation & -- \\
\midrule
Brozsko et al. & Goal navigation & Position, reward & Action \\
\midrule
\textbf{Ours} & Goal navigation & Velocity, reward, collision & Action \\
\bottomrule
\end{tabular}
\caption*{Note: PC = Place Cells, HDC = Head Direction Cells, Ext. C. = External Spatial Coordinates}
\end{table}

Several previous computational models show structural and conceptual similarities with the present work. A prominent category among them employs deep neural networks—often with recurrent components—and relies on gradient-based learning strategies such as backpropagation through time. These models typically require multiple training episodes or large datasets for convergence. In contrast, our model adopts biologically inspired, synaptically local plasticity rules, and requires only a single training episode for adaptation.

Other models utilize spiking neurons \cite{brzoskoSequentialNeuromodulationHebbian2017} or explicit neural representations \cite{decothiPredictiveMapsRats2022}, and incorporate online learning rules more closely aligned with ours. These models also focus more directly on goal-directed navigation, in contrast to purely path integration tasks. However, both of these rely on external spatial coordinates to represent current position. Our model instead constructs an internal coordinate system by integrating its own velocity output, enabling endogenous spatial tracking.

% benchmarks used
\paragraph{Naturalistic task}
The model was evaluated on a biologically inspired navigation benchmark involving exploration and goal-seeking behavior in closed environments. Performance was measured as the total number of rewards collected over multiple trials.


