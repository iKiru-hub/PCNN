\section{Methods}

% brief introduction of the previous models
% \subsection{Related work}

% mathematical formulation of the k-armed bandit problem.
\subsection{Architecture}
The formation of a spatial representation is achieved by tuning new neurons to a representation of the current position. The model is constructed with a feedforward architecture, where an input position vector $\mathbf{x}=(x,y)$ is passed through a spatial layer $M_{\text{sp}}$ and then to the main
network $M_{\text{pc}}$ of un-tuned cells. In this work, the intermediate spatial layer serves the purpose of increasing the dimensionality of the input for robustness reasons, since our synaptic plasticity works better with more discrete vectors. Ideally, position-related information is extracted from more or less
raw sensory data, but for simplicity here it is chosen a network of place cells with hard-coded Gaussian fields tiling the entire environment.\\ The activity of neurons in $M_{\text{pc}}$ is defined by the sum of the feedfoward input $\textbf{x}_{\text{sp}}$ and recurrent activity:
\begin{equation}
    \textbf{u}=\textbf{W}_{\text{ff}}\textbf{x}_{\text{sp}}+\textbf{W}_{\text{rec}}\sigma(\textbf{u})
\end{equation}

\noindent where $\sigma$ is a generalized sigmoid used as activation function.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/minimal_architecture.png}
    \caption{Architecture of the model}
    \label{fig:architecture}
\end{figure}

\noindent The feedfoward connections $W_{\text{ff}}$ are plastic and their weight update is dependent on the concentration of acetilcholine (ACh) and dopamine (DA). \\ The formation of new cells is mediated by ACh, while DA medites the remapping on cells.
The recurrent connections $W_{\text{rec}}$ are updated everytime a new neuron gets tuned, and are calculated with a
K-Nearest-Neighbor algorithm (KNN) based on the cosine distance between weight vectors ($W_{\text{ff};1,}, W_{\text{ff};2,}$) and a maximum distance threshold.

\subsection{Plasticity}
The generation of new place cells follows a heterosynaptic plasticity rule, where the weight update is proportional to the current acetylcholine level ($c_{\text{ACh}}$) and the difference between the input vector and the current weights. Importantly, the neuron to be plastic ($i$) is randomly selected from the pool of un-tuned neurons. This approach rapidly forms new tuning while relying on competition to ensure sparse encoding.

The weight update for new place cell generation is given by:

\begin{equation}
    \Delta W_{i} = \text{ReLU}(c_{\text{ACh}} - \theta_{\text{ACh}}) (x_{i} - W_{i}) \phi_{-}
\end{equation}

\noindent where $\phi_{-}$ implements lateral inhibition, a homeostatic mechanism preventing the overlap of multiple place fields, and $\text{ReLU}$ is a rectified linear function with threshold $\theta_{\text{ACh}}$.

The remapping of existing place cells is mediated by dopamine and implemented similarly, but with an additional term $\phi_{+}$ that accounts for the effect of distance between the place field and the current location:

\begin{equation}
    \Delta W_{i} = \eta \; \text{ReLU}(c_{\text{DA}} - \theta_{\text{DA}}) (W_{i} - x_{i}) \; \phi_{-} \; \phi_{+}
\end{equation}

\noindent where $\eta$ is the learning rate, $c_{\text{DA}}$ is the dopamine concentration, $\phi_{-}$ is lateral inhibition as before, and $\text{ReLU}$ is a rectified linear function with threshold $\theta_{\text{DA}}$.

For more details see the Appendix \ref{sec:appendix}.

\subsubsection{Neuromodulation}

The dynamics of both neuromodulators, acetylcholine (ACh) and dopamine (DA), follow leaky integrator models and are constrained within the range $[0, 1]$.
\paragraph{Acetylcholine} It is present by default with an equilibrium concentration of 1. However, its concentration can be reduced to zero under two conditions: (1) when an active neuron exceeds a threshold $\theta$, or (2) during an ACh-dependent weight update. The dynamics of ACh concentration are described by:
\begin{equation}
    \tau\dot{c}_{\text{ACh}} = 1 - c_{\text{ACh}} - \mathbf{1}_{\max(u) > \theta} - \mathbf{1}_{\sum_{i,j}\Delta W_{\text{ff},ij} > 0}
\end{equation}
\noindent where $\tau$ is the time constant, $c_{\text{ACh}}$ is the ACh concentration, $u$ represents neuronal activities, and $W_{\text{ff},ij}$ are the feedforward weights.

\paragraph{Dopamine} In contrast to ACh, dopamine has a default equilibrium concentration of zero, reflecting the occasional nature of remapping events. The DA dynamics are similar to those of ACh but are additionally modulated by reward. The equilibrium value $E_{\text{DA}}$ increases to 1 in the presence of a reward. The DA concentration is governed by:

\begin{equation}
    \tau\dot{c}_{\text{DA}} = E_{\text{DA}} - c_{\text{DA}} - \mathbf{1}_{\sum_{i,j}\Delta W_{\text{ff},ij} > 0}
\end{equation}

\noindent where $c_{\text{DA}}$ is the DA concentration, and $E_{\text{DA}}$ is the equilibrium value (0 by default, 1 in the presence of reward).
In both equations, $\mathbf{1}_{\{\cdot\}}$ denotes the indicator function, which equals 1 when the condition in the subscript is true and 0 otherwise.

% \subsubsection{Neuromodulation}
% Both modulators follow leaky dynamics and are clipped in $[0, 1]$.
% \hfill \break
% On one hand, acetilcholine is present by default and has an equilibrium concentration at $1$. However, its value can be to zero if an active neuron is above a threshold $\theta$ or an ACh-dependent weight update:
% \begin{equation}
%     \tau\dot{c}_{\text{ACh}} = 1 -c_{\text{ACh}} - \mathbf{1}_{\max (u)>\theta} - \mathbf{1}_{\sum_{i,j}\Delta W_{\text{ff},ij}>0}
% \end{equation}
% On the other hand, dopamine has an equilibrium set to zero, as remapping is occasional rather then the default. Its dynamics follows that of ACh  and decreased by a non-zero DA-dependent weight update. In the presence of reward, the equilibrium value $E_{\text{DA}}$ is increased to $1$: 
% \begin{equation}
%     \tau\dot{c}_{\text{DA}} = E_{\text{DA}} -c_{\text{DA}} - \mathbf{1}_{\sum_{i,j}\Delta W_{\text{ff},ij}>0}
% \end{equation}

\subsection{Reinforcement Learning}
The model described above is incorporated into an agent trained to solve a simple goal-directed navigation task. The learning process is divided into two phases: exploratory behavior and goal-directed behavior.
\subsubsection{Exploratory Behavior}
In the initial phase, the agent is programmed to explore the environment extensively, forming a comprehensive place cell representation.
The exploration policy is based on the current position representation $\mathbf{u}_{t}$, the proximal positions representation $\mathbf{u}_{\text{prox}} = \sigma(W{\text{rec}} \cdot \mathbf{u}_{t})$, and a modulation function: $\varphi(\mathbf{u}_{\text{prox}}, \lambda)$

The function $\varphi$ modulates the representation of proximal positions, potentially biasing nearby or distant place cells. The parameter $\lambda$ is computed by an external policy (either a reinforcement learning algorithm or a hard-coded heuristic) that takes as input the distance between current and proximal representations.
This external policy also computes $\Theta$, a set of crucial hyperparameters for the place cell network, such as the acetylcholine time constant.
The velocity vector $\mathbf{v}_{t}$ is then calculated using a function $\phi$:
$\mathbf{v}_{t} = \phi(\mathbf{u}_{t}, \varphi(\mathbf{u}_{\text{prox}}, \lambda))$.
\subsubsection{Goal-Directed Behavior}\label{sec:policy}
Once a robust spatial representation is established, a goal is randomly placed within the explored area. The agent's objective is to reach this goal using only locally available information.
We designed a decision-making algorithm based on the spatial representation of the current and target positions, assuming the target lies within the explored area (i.e., the agent "knows" where it is). Figure \ref{fig:poldiag} illustrates the policy diagram:
\newpage

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/policy_diagram.png}
    \caption{\textsc{Policy diagram}}
    \label{fig:poldiag}
\end{figure}

\noindent The functions $\varphi$ and $\phi$ operate as described earlier. The parameter $\epsilon$ is a scalar that determines the weight of the target position in calculating the next representation.
The external policy now receives the distance between current and target position representations as input and returns the current values of parameters $\epsilon$, $\lambda$, and $\Theta$. This approach serves two primary purposes.
First, it generates behavior solely based on the population vector of the formed place cells. Second,
it allows for flexibility in accounting for the drive towards particular proximal positions (given by $\lambda$), and the direction of the target according to the agent's state (represented by the distance between the two representations); the parameter $\epsilon$ provides the ability to balance
these factors in the decision-making process.


