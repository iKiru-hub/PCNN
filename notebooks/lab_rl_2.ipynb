{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "from gym import spaces\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, PPO\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.expanduser('~/Research/lab/PCNN/src/rl/smoothworld'))\n",
    "import envs as se\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.expanduser('~/Research/lab/PCNN'))\n",
    "import src.utils as utils\n",
    "import src.visualizations as vis\n",
    "\n",
    "logger = utils.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" train PPO agent \"\"\"\n",
    "\n",
    "T_TIMEOUT = 1_000\n",
    "EPOCHS = 100_000\n",
    "\n",
    "GOAL_POS = np.array([0.2, 0.2])\n",
    "# GOAL_POS = np.array([[0.7, 0.7],\n",
    "#                      [0.2, 0.7]])\n",
    "\n",
    "GOAL_RADIUS = 0.05\n",
    "\n",
    "INIT_POS = np.array([0.3, 0.7])\n",
    "INIT_POS_RADIUS = 0.075\n",
    "# INIT_POS = None\n",
    "# INIT_POS_RADIUS = None\n",
    "\n",
    "MAX_WALL_HITS = 10_000\n",
    "\n",
    "# pcnn params\n",
    "pcnn_params = {\n",
    "                \"N\": 100,\n",
    "                \"Nj\": 20**2,\n",
    "                \"alpha\": 0.10, # 0.1\n",
    "                \"beta\": 20.0, # 20.0\n",
    "                \"lr\": 0.00,\n",
    "                \"threshold\": 0.4,\n",
    "                \"da_threshold\": 0.2,\n",
    "                \"tau_da\": 75,\n",
    "                \"eq_da\": 1.\n",
    "}\n",
    "\n",
    "N_PCNN = pcnn_params[\"N\"]\n",
    "\n",
    "logger(\"<settings>\")\n",
    "\n",
    "\n",
    "# Optionally check the environment (useful during\n",
    "# development\n",
    "env_params = {\n",
    "    \"IS_PCNN_flag\": False,\n",
    "    \"flat\": \"one\",\n",
    "    \"GOAL_POS\": GOAL_POS,\n",
    "    \"GOAL_RADIUS\": GOAL_RADIUS,\n",
    "    \"REWARD_DURATION\": 10,\n",
    "    \"nb_experiences\": 2,\n",
    "    \"experience_duration\": T_TIMEOUT,\n",
    "}\n",
    "cell_types = [\"PCNN\", \"FOV\"]\n",
    "env, agent = se.generate_navigation_task_env(IS_PCNN_flag=True,\n",
    "                                             flat=\"two\",\n",
    "                                             GOAL_POS=GOAL_POS,\n",
    "                                             GOAL_RADIUS=GOAL_RADIUS,\n",
    "                                             nb_experiences=2,\n",
    "                                             experience_duration=T_TIMEOUT,\n",
    "                                             max_experiences=1,\n",
    "                                             init_pos=INIT_POS,\n",
    "                                             init_pos_radius=INIT_POS_RADIUS,\n",
    "                                             max_wall_hits=MAX_WALL_HITS,\n",
    "                                             pcnn_params=pcnn_params,\n",
    "                                             cell_types=cell_types)\n",
    "\n",
    "logger(f\"%{env=}\")\n",
    "\n",
    "GOAL_POS, GOAL_RADIUS = env.GOAL_POS, env.GOAL_RADIUS\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "env._env.plot_environment(autosave=False, fig=fig, ax=ax)\n",
    "\n",
    "if len(GOAL_POS.shape) > 1:\n",
    "    for goal_pos in GOAL_POS:\n",
    "        fig, ax = se.display_reward_patch(fig, ax, reward_pos=goal_pos,\n",
    "                                          reward_radius=GOAL_RADIUS)\n",
    "else:\n",
    "    fig, ax = se.display_reward_patch(fig, ax, reward_pos=GOAL_POS,\n",
    "                                      reward_radius=GOAL_RADIUS)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Train RL\n",
    "# model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "logger(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "model = model.learn(total_timesteps=EPOCHS,\n",
    "            log_interval=1_000,\n",
    "            tb_log_name=\"ppo_smoothworld\",\n",
    "            progress_bar=True)\n",
    "\n",
    "logger(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._cells[0].__repr__().split(\".\")[-1].split(\" \")[0]\n",
    "\n",
    "envrec = {\n",
    "    \"cells\": [cell.__repr__().split(\".\")[-1].split(\" \")[0] for cell in env._cells],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#### test : run the model over multiple trajectories\n",
    "logger.info(\"test\")\n",
    "\n",
    "# figure\n",
    "if is_plotting:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cells = env._cells\n",
    "dir(cells[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells[0].plot_place_cell_locations()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "-------------------------\n",
    "STUDY OF THE PCNN LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnn = env._cells[0]._pcnn._pcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "place_cell_centres = env._cells[0]._pcnn.make_pc_centers(\n",
    "    trajectory=env.whole_track,\n",
    "    bounds=tuple(env._env.extent),\n",
    "    knn=5,\n",
    "    max_dist=0.13\n",
    ")\n",
    "\n",
    "place_cell_connections = env._cells[0]._pcnn.connections.copy()\n",
    "\n",
    "place_cell_centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# plot graph\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "env._env.plot_environment(autosave=False, fig=fig, ax=ax)\n",
    "\n",
    "# ax.scatter(place_cell_centres[:, 0], place_cell_centres[:, 1])\n",
    "vis.plot_graph(centers=place_cell_centres,\n",
    "               connectivity=place_cell_connections,\n",
    "               bounds=env._env.extent,\n",
    "               alpha=0.5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pcnn._Wff)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pcnn = env._cells[0]._pcnn._pcnn\n",
    "\n",
    "plt.plot(pcnn.tau_record)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %\n",
    "pcnn = env._cells[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, size=(10, 2))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnn = env._cells[0]\n",
    "out = pcnn.get_state(evaluate_at=\"all\").reshape(100, -1)\n",
    "\n",
    "z = out.sum(axis=0)\n",
    "nb_tests = 4\n",
    "fig, axs = plt.subplots(nrows=2, ncols=nb_tests//2,\n",
    "                        figsize=(3*nb_tests//2, 5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "logger.debug(f\"{GOAL_POS=}\")\n",
    "\n",
    "for i in range(nb_tests):\n",
    "\n",
    "    logger.info(f\"run {i}\")\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    rewards = 0\n",
    "\n",
    "    t_start = env.t\n",
    "    positions = []\n",
    "\n",
    "    # for t in tqdm(np.arange(t_start, 100+t_start, env._env.dt)):\n",
    "    for t in tqdm(np.arange(0, T_TIMEOUT+t_start, env._env.dt)):\n",
    "\n",
    "        try:\n",
    "            action, _state = model.predict(obs, deterministic=True)\n",
    "            obs, reward_rate, done, _ , info =  env.step(\n",
    "                action=action)\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "        positions += [env._env.agents_dict['agent_0'].pos.tolist()]\n",
    "\n",
    "\n",
    "        # if t < 2:\n",
    "        #     print(positions)\n",
    "\n",
    "        # exit\n",
    "        # if done or reward_rate:\n",
    "        #     logger.debug(f\"{done=} {reward_rate=} {_=}\")\n",
    "        #     break\n",
    "\n",
    "    # display the trajectory\n",
    "    env._env.plot_environment(autosave=False, fig=fig, ax=axs[i])\n",
    "    fig, xax = se.display_reward_patch(fig, axs[i],\n",
    "                                      reward_pos=env.GOAL_POS,\n",
    "                                      reward_radius=env.GOAL_RADIUS)\n",
    "    # axs[i] = xax\n",
    "\n",
    "    positions = np.array(positions)\n",
    "\n",
    "    axs[i].plot(positions[:, 0], positions[:, 1], '-')\n",
    "    axs[i].set_xlabel(f\"pos={positions[-1, :]}\")\n",
    "\n",
    "    logger.debug(f\"{positions=}\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#### test 2 : plot all trajectories experienced by the agent\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig, ax = agent.plot_trajectory(fig=fig, ax=ax, t_start=env.t-1000,\n",
    "                                t_end=env.t,\n",
    "                                framerate=30,\n",
    "                                color=\"changing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "cell_names = [\n",
    "    cell.__repr__().split(\".\")[-1].split(\" \")[0] for cell in env._cells\n",
    "]\n",
    "cell_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#### test 3 : online\n",
    "\n",
    "obs, _ = env.reset()\n",
    "cell = env._cells[0]\n",
    "pcnn = env._cells[0]._pcnn._pcnn\n",
    "agent = env._env.agents_dict['agent_0']\n",
    "rewards =  0\n",
    "\n",
    "pcnn._knn = 15\n",
    "pcnn._max_dist = 0.3\n",
    "\n",
    "t_start = env.t\n",
    "positions = []\n",
    "actions = []\n",
    "DA = []\n",
    "nb_collisions = 0\n",
    "\n",
    "is_plotting = False\n",
    "\n",
    "cell.flag_make_pf()\n",
    "\n",
    "for tc, t in enumerate(np.arange(0, T_TIMEOUT+t_start, env._env.dt)):\n",
    "\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward_rate, done, truncated , info =  env.step(\n",
    "        action=action)\n",
    "\n",
    "    positions += [env._env.agents_dict['agent_0'].pos.tolist()]\n",
    "    posr = np.array(positions)\n",
    "    actions += [action.tolist()]\n",
    "    DA += [env._cells[0]._pcnn._pcnn._DA]\n",
    "    nb_collisions += 1*(agent.is_wall_hit)\n",
    "\n",
    "    # exit\n",
    "    # if reward_rate:\n",
    "    #     logger.debug(f\"{done=} {reward_rate=}\")\n",
    "    #     break\n",
    "\n",
    "    if tc > 10 and not is_plotting:\n",
    "        is_plotting = True\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,\n",
    "                                       figsize=(15, 5))\n",
    "\n",
    "    # display the trajectory\n",
    "    if tc % 200 == 0 and is_plotting:\n",
    "\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "\n",
    "        tpl = 1000\n",
    "        env._env.plot_environment(autosave=False, fig=fig, ax=ax1)\n",
    "        fig, ax1 = se.display_reward_patch(fig, ax1,\n",
    "                                          reward_pos=env.GOAL_POS,\n",
    "                                          reward_radius=env.GOAL_RADIUS)\n",
    "        ax1.plot(posr[-min(tc,tpl):, 0], posr[-min(tc,tpl):, 1], 'b-')\n",
    "        # print(f\"position={tuple(np.around(positions[-1], 2))}\")\n",
    "        ax1.set_title(f\"$a=${np.around(action, 2)} | {t=:.1f}s \" + \\\n",
    "                      f\"| collision={nb_collisions}\")\n",
    "\n",
    "        if env._cells[0]._is_making_pf:\n",
    "            ax2.imshow(cell._place_field.reshape(100, 100),\n",
    "                       vmin=0.03, vmax=0.5, cmap=\"Greens\")\n",
    "            vis.plot_graph(centers=cell._pcnn_centers,\n",
    "                           connectivity=cell._pcnn_connections,\n",
    "                           bounds=env._env.extent,\n",
    "                           alpha=0.5, ax=ax1)\n",
    "        else:\n",
    "            ax2.imshow(pcnn._Wff)\n",
    "            # ax2.imshow(env._cells[0]._pcnn._pcnn.u.reshape(10, 10),\n",
    "            #            vmin=0, vmax=1, cmap=\"Greys\")\n",
    "\n",
    "        ax2.set_title(\"$E_{DA}=$\" + f\"{pcnn._eq_da}, \\n\" + \\\n",
    "            f\"DA={pcnn._DA:.3f}, \" + \\\n",
    "            f\"$\\langle\\Delta W\\\\rangle$={pcnn._dw_rec:.3f}\\n\" + \\\n",
    "            \"last $t_{dt}=$\" + f\"{cell._last_makepf}, tot={pcnn._umask.sum()}\")\n",
    "\n",
    "        ax2.axis(\"off\")\n",
    "\n",
    "        plt.pause(0.0001)\n",
    "\n",
    "if is_plotting:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test 4 : online w/ pcnn\n",
    "obs, _ = env.reset()\n",
    "agent = env._env.agents_dict['agent_0']\n",
    "rewards =  0\n",
    "\n",
    "t_start = env.t\n",
    "positions = []\n",
    "actions = []\n",
    "nb_collisions = 0\n",
    "\n",
    "is_plotting = False\n",
    "\n",
    "for tc, t in enumerate(np.arange(0, T_TIMEOUT+t_start, env._env.dt)):\n",
    "\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward_rate, done, truncated , info =  env.step(\n",
    "        action=action)\n",
    "\n",
    "    positions += [env._env.agents_dict['agent_0'].pos.tolist()]\n",
    "    posr = np.array(positions)\n",
    "    actions += [action.tolist()]\n",
    "    nb_collisions += 1*(agent.is_wall_hit)\n",
    "\n",
    "    # exit\n",
    "    if done or reward_rate:\n",
    "        logger.debug(f\"{done=} {reward_rate=}\")\n",
    "        # break\n",
    "\n",
    "    if tc > 0 and not is_plotting:\n",
    "        is_plotting = True\n",
    "        fig, ax1 = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "    # display the trajectory\n",
    "    if tc % 2 == 0 and is_plotting:\n",
    "\n",
    "        env._env.plot_environment(autosave=False, fig=fig, ax=ax1)\n",
    "        fig, ax1 = se.display_reward_patch(fig, ax1,\n",
    "                                          reward_pos=env.GOAL_POS,\n",
    "                                          reward_radius=env.GOAL_RADIUS)\n",
    "        ax1.plot(posr[-min(tc,20):, 0], posr[-min(tc,20):, 1], 'b-')\n",
    "        # print(f\"position={tuple(np.around(positions[-1], 2))}\")\n",
    "        ax1.set_title(f\"$a=${np.around(action, 2)} | {t=:.1f}s \" + \\\n",
    "                      f\"| collision={nb_collisions}\")\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        plt.pause(0.2)\n",
    "\n",
    "# z -= z.mean(axis=0)\n",
    "# z = 1/(1 + np.exp(-20*(z-0.1)))\n",
    "\n",
    "print(z.max(),z.mean())\n",
    "plt.imshow(z.reshape(100, 100), vmin=0, vmax=1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=10, ncols=10)\n",
    "\n",
    "for i, ax, in enumerate(axs.flatten()):\n",
    "    ax.imshow(out[i, :].reshape(100, 100))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
